---
title: "COLA_results"
author: "DVM Bishop"
date: "20/01/2022"
output: word_document
---

<!--- Github project COLA_analysis_2022 initiated on 4 Feb 2022. Working version currently on Dropbox; this is a clone to allow version control --->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# If the package is not installed, install it. If it is installed, load it.
usePackage <- function(p) {
    if (!is.element(p, installed.packages()[,1]))
        install.packages(p, dep = TRUE)
    require(p, character.only = TRUE)
}
usePackage('dplyr')
usePackage('tidyr')
usePackage('ggplot2')
usePackage('yarrr') #for pirate plots
usePackage("osfr") #for reading files from OSF
usePackage("stringr")
usePackage("table1") #useful for making simple tables of demographic etc
usePackage("ggExtra") #for marginal plots
usePackage("ggpubr")
usePackage("patchwork") #for combining plots in ggplot
usePackage("flextable")

usePackage("tidyverse")
usePackage("here") #to find filepaths
usePackage("kableExtra")

usePackage("ggstatsplot")
usePackage("MASS") #includes boxcox function
usePackage("MBESS")
usePackage("nlme")
usePackage("semPower")
usePackage("semTools")
usePackage("bookdown")
usePackage("lavaan")
usePackage("semPlot")

usePackage("officer")
usePackage("corrr") #added by DB for easy correlations
usePackage("plyr")
usePackage("qpcR") #used in Kievit script
usePackage("ggpubr")
usePackage("reshape2")
usePackage("mice")
usePackage("MVN") #multivariate normality



options(scipen=999)
```

<!--- GENERIC FUNCTIONS START -->
```{r pformat, echo=F}
#function to format p-values
pformat=function(myp){
  pout <- paste('p =',round(myp,3))
  if(myp<.001){pout = 'p < .001'}
  return(pout)
}
```

```{r pformat2, echo=F}
#function to format p-values, without the 'p = ' bit
pformat2=function(myp){
  pout <- round(myp,3)
  if(myp<.001){pout ='< .001'}
  return(pout)
}
```

```{r LIdensityplot,echo=F}
#generic density plot, subsetted by handedness or another categorical variable
#includes line showing zero point.
#takes as input allsum; temp and cattemp are dummy columns that are assigned prior to call to the function
doLIplot <- function(myfile,temp,cattemp,mysubsetname,mysubsetlabels,xlabel,xrange){
LI.plot <- ggplot(myfile, aes(x=temp, color=as.factor(cattemp))) +
  geom_density()+
  xlab(xlabel)+
  geom_vline(xintercept = 0,lty=3)+
  xlim(xrange)+
  scale_color_manual(name=mysubsetname,
                       labels=mysubsetlabels,
                       values=c("blue","red"))


return(LI.plot)
}
```


```{r trianglefunction,echo=F}
#Gets upper triangle of a correlation matrix
 get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat,diag=T)]<- NA
    return(cormat)
  }
```



```{r heatmapfunction}
#Make a heatmap
# http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization
makeheatmap <- function(mydf,mycols){
cormat <- cor(mydf[,mycols],use="complete.obs")

melted_cormat <- melt(cormat)
head(melted_cormat)

upper_tri <- get_upper_tri(cormat)
# Melt the correlation matrix

melted_cormat <- melt(upper_tri, na.rm = TRUE)
# Heatmap

ggheatmap <- ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()

myheatmap <- ggheatmap + 
geom_text(aes(Var2, Var1, label = round(value,3)), color = "black", size = 4) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.5, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))

return(myheatmap)}
```
<!--- GENERIC FUNCTIONS END -->

# Results

## Descriptives  
```{r readcombined,echo=F}
mydir <- "~/Dropbox/COLA_RR_Analysis"
readfile<-paste0(mydir,"/02-data/combined_data.csv")
combdat <- read.csv(readfile)
#We will create a code that reflects whether strict exclusion criteria are met for language. This is 1 for lextale < 80, 10 for gram12err >1, and 11 if both are met
combdat$lang2exclude <- 0
w<- which(combdat$gramerr12>1)
combdat$lang2exclude[w]<-10
w<- which(combdat$lexTALE<80)
combdat$lang2exclude[w]<-combdat$lang2exclude[w]+1
#For now we set 'excluded' for any who were below 80 on LexTale or more than 1 error on short grammar AND are not native English speakers. I.e. all native English are included, regardless of language tests
combdat$excluded <- 0
w<-intersect(which(combdat$lang2exclude>0),which(combdat$nativeEnglish==0))
combdat$excluded[w] <-1

testtab <- table(combdat$Rhanded,combdat$ftcd)
ftcd.dat <- filter(combdat,ftcd==1) #has demog, behav and ftcd data for those who did ftcd
langtab <- table(ftcd.dat$Rhanded, ftcd.dat$lang2exclude,ftcd.dat$nativeEnglish)
#Langtab cells can be accessed with 3 indices corresponding to Rhanded,langexclude and native English status).

myftcd <- filter(ftcd.dat,excluded==0) #we'll do analysis on myftcd
```

_Departures from pre-registration plan_  
Our plan had been to recruit 300 left-handers and 150 right-handers for the online behavioural battery, and from these to select 112 left-handers and 112 right-handers for in-person testing. Because of disruption to in-person testing caused by pandemic restrictions, we did not meet our target numbers for in-person testing, despite over-recruiting for online testing. In total we tested `r testtab[1,2] + testtab[1,1]` left-handers, `r testtab[1,2]` of whom were tested with fTCD, and `r testtab[2,2] + testtab[2,1]` right-handers, `r testtab[2,2]` of whom were tested with fTCD. However, `r langtab[1,2,1]+langtab[1,3,1]+langtab[1,4,1]` left-handers and `r langtab[2,2,1]+langtab[2,3,1]+langtab[2,4,1]` right-handers tested with fTCD were non-native English speakers who met our criteria for excluding participants on the basis of either the lexTALE or Games with Words measures of English language competence, giving final samples of `r langtab[1,1,1]+langtab[1,1,2]+langtab[1,2,2]+langtab[1,3,2]+langtab[1,4,2]` left-handers and `r langtab[2,1,1]+langtab[2,1,2]+langtab[2,2,2]+langtab[2,3,2]+langtab[2,4,2]` right-handers. An unexpected issue was that a few native English speakers failed the language screen (`r langtab[1,2,2]+langtab[1,3,2]+langtab[1,4,2]` left-handers and `r langtab[2,2,2]+langtab[2,3,2]+langtab[2,4,2]` right-handers): however, the purpose of these tests had been to exclude non-native speakers with inadequate language skills, so we retained those participants in the study.

<!--- We may not stick to this; I will press on for now assuming this. I suspect we'll need to analyse with them excluded just to check if it makes a difference -->

### Subsample used for test-retest study
[to be added]


### Demographic data 

```{r demog.table,echo=F}
#We'll use the table1 package to create a nice-looking summary table for demographics; this excludes those excluded on language tests, but otherwise includes everyone given the online testing.

mycomb <- filter(combdat,excluded==0) #create a version of the main data file that excludes anyone excluded on Lextale or Grammar task

mycomb$Gender <- factor(mycomb$male,levels=c(0,1),labels=c("Female","Male"))
label(mycomb$age) <- "Age (yr)"
mycomb$Native <- factor(mycomb$nativeEnglish,levels=c(0,1),labels=c("No","Yes"))
label(mycomb$Native)<-"Native English speaker"
mycomb$Bilingual <- factor(mycomb$bilingual,levels=c("No","Yes"),labels=c("No","Yes"))
mycomb$Handedness <- factor(mycomb$Rhanded,levels=c(0,1),labels=c("Left-Handed","Right-Handed"))
mycomb$ftcd <- factor(mycomb$ftcd,levels=c(0,1),labels=c("No FTCD data","With FTCD data"))

demog.table <- table1(~ Gender + age + Native+ Bilingual+EHI.LI|(factor(ftcd)+Handedness) , data=mycomb,overall=F)
demog.table

```
Table x shows demographic data for the subset of individiuals tested on the online battery only, and the subset who also completed the session with FTCD. It is evident from inspection that there are no systematic differences between the two subgroups. 

### Lateralised responses on the online battery


__1. Dichotic listening__ 

```{r dichotic.plot,echo=F}

#NB the odd/even values were not computed for zlat so we do it here.

myR <-mycomb$DL.R.odd
myL <-mycomb$DL.L.odd
n <- myR + myL
mycomb$DL.odd.zlat <- (myR/n-.5)/sqrt(myR/n*myL/n/n)
myR <-mycomb$DL.R.even
myL <-mycomb$DL.L.even
n <- myR + myL
mycomb$DL.even.zlat <- (myR/n-.5)/sqrt(myR/n*myL/n/n)


mycomb$DLsig <-0
w<-which(abs(mycomb$DL.zlat)>1.96)
mycomb$DLsig[w]<-1


DLdat <- mycomb[mycomb$excludeDL==0,]
DLsides <- ggplot(DLdat, aes(x=DL.L, y=DL.R, color=Handedness,shape=as.factor(DLsig))) +
  xlab("N correct L ear")+
   ylab("N correct R ear")+
  geom_point()+
  ggtitle("Dichotic Listening") +
  scale_shape_manual(name="Significantly lateralised",
                     labels=c("No","Yes"),
                     values=c(4,16))+
  scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
  geom_abline(intercept = 0, slope = 1)
ggsave(paste0(mydir,"/03-graphic-outputs/DLsides.png"),width = 6, height = 4)

DLsides


```

Figure x shows the distribution of total correct for L and R sides subdivided by handedness. Note that on this task there is inevitably a negative correlation between these totals, because on each trial the response is either left or right: hence, the more responses are 'left' the fewer are 'right', and vice versa. The black line shows the point of equality, where the number correct is the same for left and right. The points are distinguished for those where the proportion on one side is significantly different from .5 (filled circles), versus those who are equally likely to respond to L or R (crosses, which are bunched around the black line). It is evident from this plot that a high proportion of participants are significantly lateralised, and that overall the sample shows a right ear advantage, i.e. there are more points above the black line than below it. Distributions of the laterality indices will be analysed in the next section.   


__2. Rhyme Decision__ 

```{r RDT,echo=F}
mycomb$RDTsig <-0
w<-which(abs(mycomb$RDT.zlat)>1.96)
mycomb$RDTsig[w]<-1


RDTdat <- mycomb[mycomb$excludeRDT==0,]
RDTsides <- ggplot(RDTdat, aes(x=RDT.Lmean, y=RDT.Rmean, color=Handedness,shape=as.factor(RDTsig))) +
  xlab("Mean correct RT (ms) left VHF")+
   ylab("Mean correct RT (ms) right VHF")+
  geom_point()+
  ggtitle("Rhyme Decision") +
  scale_shape_manual(name="Significantly lateralised",
                     labels=c("No","Yes"),
                     values=c(4,16))+
  scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
  geom_abline(intercept = 0, slope = 1)
ggsave(paste0(mydir,"/03-graphic-outputs/RDTsides.png"),width = 6, height = 4)

RDTsides

```
In Rhyme Decision, the task was to judge which of two pictured items had a name that rhymed with a centrally-presented written word. The pictures were presented in the left or right periphery. This was an easy task, as the words used were common, and the participants were given a practice session where they were introduced to all the pictures and their names. We could therefore use accuracy as an index of engagement with the task, and we excluded `r length(intersect(which(combdat$excludeRDT==1),which(combdat$lang2exclude==0)))` participants with accuracy of less than 75% correct. 

The dependent variable of interest was Response Time (RT), which was computed for correct responses only in each half-field, after excluding outliers using a participant-specific algorithm. This involved taking the complete set of correct RTs for a participant, and applying the Hoaglin-Iglewicz (1987) criterion of outlier detection with cutoff set to 1.65 to remove unusually long RTs. In addition, any RTs less than 200 ms were excluded. A z-LI was then computed by computing a t-test for each participant, to compare the mean correct RT to left- and right-sided stimuli. Absolute values of z-LI greater than 1.96 can be regarded as evidence that an individual is significantly faster to one side than the other. Because RT is scaled so that high values correspond to poor performance, the z-LI was computed so that it was positive when left-sided RT was greater than right-sided RT. Thus those with a left-hemisphere advantage should cluster below the black line in figure X2. 


__3. Word Comprehension__    
As with Rhyme Decision, this task involved responding to laterally presented visual stimuli. The stimuli are pictures of semantically-related words, and the task is simply to respond to the picture whose name is spoken. Again, this is an easy task, where the focus is on RT of correct responses. Participants who made more than 75% errors on the task (N = `r length(intersect(which(combdat$excludeWC==1),which(combdat$lang2exclude==0)))`) were excluded from analysis.  The same method as for Rhyme Decision was used to remove outlier RTs participant by participant before computing a z-LI based on comparison of mean RT to left and right sides, where a positive value indicated faster RTs to stimuli presented in the right visual field. 

```{r WC,echo=F}
mycomb$WCsig <-0
w<-which(abs(mycomb$WC.zlat)>1.96)
mycomb$WCsig[w]<-1


WCdat <- mycomb[mycomb$excludeWC==0,]
WCsides <- ggplot(WCdat, aes(x=WC.Lmean, y=WC.Rmean, color=Handedness,shape=as.factor(WCsig))) +
  xlab("Mean correct RT (ms) left VHF")+
   ylab("Mean correct RT (ms) right VHF")+
  geom_point()+
  ggtitle("Word Comprehension") +
  scale_shape_manual(name="Significantly lateralised",
                     labels=c("No","Yes"),
                     values=c(4,16))+
  scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
  geom_abline(intercept = 0, slope = 1)
ggsave(paste0(mydir,"/03-graphic-outputs/WCsides.png"),width = 6, height = 4)

WCsides

```

### Laterality indices

_1. Dichotic Listening__  
In our pre-registration we stated that we would compute a conventional laterality index, 100* (Left-Right)/(Left+Right), as well as a laterality z-score:
(pL-.5)/sqrt(pL*pR/n).  We made one small modification, which was to flip the sign of these quotients, so that for all our laterality measures, left-hemisphere superiority is reflected in a positive score. This has no material effect on any computations, but gives better consistency with other research. For the laterality z-score, scores were censored at +/- 10.

```{r DLdensities,echo=F}

#censor scores to restrict range that is displayed
DLdat$DL.zlat[DLdat$DL.zlat>10]<-10
DLdat$DL.zlat[DLdat$DL.zlat< -10]<-(-10)


plot1 <- ggplot(DLdat, aes(x = DL.LI, y = DL.zlat)) + 
  geom_point(shape = 4,  size = 1)+
  ggtitle("Dichotic Listening") 

  

dens1 <- ggplot(DLdat, aes(x = DL.LI, fill = Handedness)) + 
  geom_density(alpha = 0.4) + 
  geom_vline(xintercept = 0, linetype="dotted")+
  theme_void() + 
  theme(legend.position = "none")

dens2 <- ggplot(DLdat, aes(x = DL.zlat, fill = Handedness)) + 
  geom_density(alpha = 0.4) + 
  geom_vline(xintercept = 0, linetype="dotted")+
  theme_void() + 
  theme(legend.position = "none") + 
  coord_flip()

dens1 + plot_spacer() + plot1 + dens2 + 
  plot_layout(ncol = 2, nrow = 2, widths = c(4, 1), heights = c(1, 4))

DLdens <- ggsave(paste0(mydir,"/03-graphic-outputs/DLdens.png"),width = 5, height = 4)

```




As is evident from figure x2, the z-LI score is very highly correlated with the conventional LI, the principal difference being that the z-LI follows the normal distribution, with a sigmoid shape at the extremes (truncated here because of the censored scale). For subsequent analyses, we use z-LI, as this allows us to compare different tasks on a common scale.  

__2-3. Rhyme Decision and Word Comprehension__  
These two tasks are shown together, as both have z-LIs computed on the basis of correct RTs to visual stimuli presented to the left and right.  

```{r RD-WC-densities,echo=F}
#We will plot RD and WC together as they both involve z-LI from RTs

RDWCdat<-filter(mycomb,excludeRDT==0,excludeWC==0)

RDWCdens <- ggplot(RDWCdat, aes(x = RDT.zlat, y = WC.zlat,col=Handedness)) + 
  geom_point(shape = 4,  size = 1)+
  scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
  ggtitle("Rhyme Decision vs Word Comprehension") +
   geom_abline(intercept = 0, slope = 1)

  

dens1 <- ggplot(RDWCdat, aes(x = RDT.zlat, fill = Handedness)) + 
  geom_density(alpha = 0.4) + 
  geom_vline(xintercept = 0, linetype="dotted")+
  theme_void() + 
  theme(legend.position = "none")

dens2 <- ggplot(RDWCdat, aes(x = WC.zlat, fill = Handedness)) + 
  geom_density(alpha = 0.4) + 
  geom_vline(xintercept = 0, linetype="dotted")+
  theme_void() + 
  theme(legend.position = "none") + 
  coord_flip()

dens1 + plot_spacer() + RDWCdens + dens2 + 
  plot_layout(ncol = 2, nrow = 2, widths = c(4, 1), heights = c(1, 4))

ggsave(paste0(mydir,"/03-graphic-outputs/RDWCdens.png"),width = 5, height = 4)


```

Figure X3 shows a suggestive trend for a bias to positive LI for the Rhyme Decision task, but for Word Comprehension, there is a more obvious bias in the opposite direction, with faster responses to stimuli presented in the left than the right visual half-field, i.e. points cluster below the line, indicating a right hemisphere advantage.  

### Preliminary analyses on online laterality measures  

Before testing specific predictions about interrelationships between measures, we conducted preliminary analysis on z-LI values for all three online tasks, to check for significant lateralisation in left- and right-handers, to compare laterality between handedness groups, and to compute split-half and test-retest reliability for laterality indices.  


```{r beh_normal}

mycols <- c("ID","Rhanded","DL.zlat","RDT.zlat","WC.zlat")
thisdat <- combdat[,mycols]

  for (t in 3:5){ #each test
    s<-shapiro.test(thisdat[,t])$p.value
    print(paste0(colnames(thisdat)[t],' Normality test, p-value: ',round(s,5)))
    }

```

```{r DL-ttests,echo=F}
#Make a table to show characteristics of different tests

onlinesummary <- data.frame(matrix(NA,nrow=8,ncol=4))
colnames(onlinesummary)<-c('Statistic','Dichotic','Rhyme','Comprehension')
onlinesummary[,1]<-c('N','Mean (SD) L-hander','Mean (SD) R-hander','one-group t L-hander','one-group t R-hander','R-hander vs L-hander t','Split half r','Test-retest r')
```

```{r filltable,echo=F}
#Generate function to populate summary data frame - same steps for all online tasks
#This function also repurposed for ftcd
populate <- function(mytask,mysummary,writecolnum,tasktype){
  if(tasktype=='beh'){
w<-which(colnames(mycomb)==paste0(mytask,'.zlat'))
x<-which(colnames(mycomb)==paste0('exclude',mytask))
odds<-which(colnames(mycomb)==paste0(mytask,'.odd.zlat'))
evens<-which(colnames(mycomb)==paste0(mytask,'.even.zlat'))
  }
    if(tasktype=='ftcd'){
w<-which(colnames(mycomb)==paste0(mytask,'_mean_LI'))
x<-which(colnames(mycomb)==paste0(mytask,'_exclude'))
odds<-which(colnames(mycomb)==paste0(mytask,'_mean_odd'))
evens<-which(colnames(mycomb)==paste0(mytask,'_mean_even'))
  }
  
thisdat<-mycomb[mycomb[,x]==0,]
thisdat$thiscol<-thisdat[,w]
nL <- nrow(filter(thisdat,Rhanded==0))
nR <- nrow(filter(thisdat,Rhanded==1))
tL<- t.test(thisdat$thiscol[thisdat$Rhanded==0])
tR<- t.test(thisdat$thiscol[thisdat$Rhanded==1])
sdL<-round(sd(thisdat$thiscol[thisdat$Rhanded==0],na.rm=T),2)
sdR<-round(sd(thisdat$thiscol[thisdat$Rhanded==1],na.rm=T),2)
tcompare <- t.test(thisdat$thiscol~thisdat$Rhanded,alternative='less')
#write N for L and R in row 1 of online summary
mysummary[1,writecolnum]<-paste0(nL,' LH + ',nR,' RH')
mysummary[2,writecolnum]<-paste0(round(tL$estimate,2)," (",sdL,")")
mysummary[3,writecolnum]<-paste0(round(tR$estimate,2)," (",sdR,")")
mysummary[4,writecolnum]<- paste0('t = ',round(tL$statistic,1),'; ', pformat(tL$p.value))
mysummary[5,writecolnum]<- paste0('t = ',round(tR$statistic,1),'; ', pformat(tR$p.value))
mysummary[6,writecolnum]<- paste0('t = ',-round(tcompare$statistic,1),'; ', pformat(tcompare$p.value))
mysummary[7,writecolnum]<- round(cor(thisdat[,odds],thisdat[,evens],use='complete.obs'),3)



return(mysummary)
}
```


```{r fillonlinesummary,echo=F}

#need to censor odd and even zlat values
w<-which(mycomb$DL.odd.zlat< (-10))
mycomb$DL.odd.zlat[w] <- -10
w<-which(mycomb$DL.odd.zlat> 10)
mycomb$DL.odd.zlat[w] <- 10
w<-which(mycomb$DL.even.zlat< (-10))
mycomb$DL.even.zlat[w] <- -10
w<-which(mycomb$DL.even.zlat> 10)
mycomb$DL.even.zlat[w] <- 10

#also censor overall DL
w<-which(mycomb$DL.zlat< (-10))
mycomb$DL.zlat[w] <- -10
w<-which(mycomb$DL.zlat> 10)
mycomb$DL.zlat[w] <- 10


#also need to read in sess1 and sess3 for the test-retest data

sess1 <- read.csv(paste0(mydir,'/02-data/02.1_gorilla/sess1.csv'))
sess3 <- read.csv(paste0(mydir,'/02-data/02.1_gorilla/sess3.csv'))

mytask<-'DL'
writecolnum <- 2 #column of online summary to write to for this task
onlinesummary <- populate(mytask,onlinesummary,writecolnum,'beh')

mytask<-'RDT'
writecolnum <- 3 #column of online summary to write to for this task
onlinesummary <- populate(mytask,onlinesummary,writecolnum,'beh')
onlinesummary[8,3]<-round(cor(sess1$RDT.zlat,sess3$RDT.zlat,use='complete.obs'),3)

mytask<-'WC'
writecolnum <- 4 #column of online summary to write to for this task
onlinesummary <- populate(mytask,onlinesummary,writecolnum,'beh')
onlinesummary[8,4]<-round(cor(sess1$WC.zlat,sess3$WC.zlat,use='complete.obs'),3)

ft<-flextable(onlinesummary)
ft<-autofit(ft)
ft
```
```{r behpirates, echo=FALSE, warning=FALSE}
#NB Formatting of this figure needs to be tweaked to achieve good resolution and legibility, but we can do that when we know what format figures need to be created in. 


 mypath<-paste0(mydir,"/03-graphic-outputs")
  plotname<-paste0(mypath,'/beh_pirates.jpg')
jpeg(plotname, width = 800, height = 500)

bLIdat <- combdat %>% 
  dplyr::select(ID, Rhanded, DL.zlat,RDT.zlat,WC.zlat)
colnames(bLIdat) <- c('ID','Handed','Dichotic','Rhyme','Comprehension')
bLIdat$Dichotic[bLIdat$Dichotic>10]<-10
bLIdat$Dichotic[bLIdat$Dichotic<(-10)]<- -10
bLIdat$Handed <- as.factor(bLIdat$Handed)
levels(bLIdat$Handed)<-c("L","R")
longdata.b <- pivot_longer(data = bLIdat, cols = c(3:5), names_to = 'Task', values_to = 'LI')
longdata.b$Task<-as.factor(longdata.b$Task)
longdata.b$Task <- factor(longdata.b$Task, levels = c('Dichotic', 'Rhyme', 'Comprehension'))
pirateplot(data = longdata.b, LI ~ Handed * Task)
abline(h=0)

title(main=paste0('Distributions of z-LI \non behavioural tasks; N = ',length(bLIdat$ID)))

dev.off()

```
<!--- Need to describe retest sample somewhere -->

As shown in Table x, the three tasks showed very different patterns of laterality. As expected from previous studies, on dichotic listening there was a clear right ear advantage in both left- and right-handers. In addition, there was a small but statistically reliable difference between handedness groups, with stronger laterality in the R handers.  We did not assess test-retest reliability for this task, as we had done this in our previous study with this task and found it to be high. Here we confirm excellent split-half reliability for this task. 

The Rhyme Decision task was far less reliable, with split half reliability of .432 and test-retest reliability of .539.  These figures indicate that laterality on this test is far from being at chance, but there is a great deal of random variation. In addition, although the task showed statistically reliable laterality in both left- and right-handers in this large sample, the effect size was small, and most individuals were not significantly lateralised. Furthermore, there was no effect of handedness on laterality on this task.

The Word Comprehension task did rather better in terms of reliability, with split half reliability of .664, though test-retest reliability was lower at .555. The striking observation about this task was that it showed a laterality bias in the opposite direction to what is usually seen in language tasks, with faster responses to pictures viewed in the left visual half-field, which projects directly to the right hemisphere. Furthermore, there was a significant effect of handedness, with the laterality index being more negative in left-handers than in right-handers.  



## Functional Transcranial Doppler measures

### Data Quality and Outliers for fTCD
``` {r dataqual, echo=FALSE, warning=FALSE}

# Identify outliers
for (t in 1:6){
  SEcol <- which(colnames(combdat) == paste0(LETTERS[t], '_mean_se'))
  Q3<-quantile(combdat[ , SEcol],.75,na.rm=TRUE)
  Q1<-quantile(combdat[ , SEcol],.25,na.rm=TRUE)
  Qlimit<-Q3+2.2*(Q3-Q1)
  
# If there are at least 10 trials, include the data
  excludecol = which(colnames(combdat) == paste0(LETTERS[t], '_exclude'))
  trialscol = which(colnames(combdat) == paste0(LETTERS[t], '_N'))
  combdat[which(combdat[ , trialscol] > 9), excludecol] <- 0
  combdat[which(combdat[ , trialscol] < 10), excludecol] <- 1
  
  # If the SE is too high, exclude the datatrials
  combdat[which(combdat[ , SEcol] > Qlimit) , excludecol] <- 1
}

# Count number of missing or excluded datapoints per task. ZW: These numbers include people without fTCD, is this really what we want?
n_excludeLI = matrix(data=NA, nrow=1, ncol=6)
for (t in 1:6){
  excludecol = which(colnames(combdat) == paste0(LETTERS[t], '_exclude'))
  n_excludeLI[t] = length(which(combdat[ , excludecol] > 0)) + length(which(is.na(combdat[ , excludecol])))
}

# Should any participants be excluded? If a participant has more than one excluded task, the participant is excluded entirely
combdat$DopExclude <- 0
tmp <- combdat$A_exclude + combdat$B_exclude + combdat$C_exclude + combdat$D_exclude + combdat$E_exclude + combdat$F_exclude
combdat$DopExclude[which(tmp > 1)] = 1
n_excluded = length(which(combdat$DopExclude == 1))

ddat<- filter(combdat,DopExclude==0,excluded==0,ftcd==1) #not really necessary to create this, but done for consistency with previous script : this has just those who did ftcd and who were not excluded on demographics or ftcd.

```

### Normality
```{r fTCD_normal}

mycols <- c("ID","Rhanded","A_mean_LI","B_mean_LI","C_mean_LI","D_mean_LI","E_mean_LI","F_mean_LI")
thisdat <- ddat[,mycols]

  for (t in 3:8){ #each test
    s<-shapiro.test(thisdat[,t])$p.value
    print(paste0(colnames(thisdat)[t],' Normality test, p-value: ',round(s,5)))
    }

```

```{r scatterplus,echo=F}
#Function for doing scatterplot with marginal density distributions
doscatterplus <- function(myfile, taskname,mycolnames,myrange){ #myfile contains group in col 1 and the 2 cols to plot in cols 2-3
colnames(myfile)[2:4]<- mycolnames

r <- round(cor(myfile$Odd,myfile$Even,use="complete.obs",method="spearman"),3)
myt<-t.test(myfile$All~myfile$Handed)
myt2 <- t.test(myfile$All) #single group t-test
text1 <- paste0('Mean diff from zero: \nt = ',round(myt2$statistic,2),"; ", pformat(myt2$p.value))
text2 <- paste0('L vs R handers \n(all trials):\n t = ',round(myt$statistic,2),"; ", pformat(myt$p.value))
text3 <-paste0('Spearman rho: ',r)
p <- ggplot(myfile, aes_string('Odd','Even')) +
  aes_string(colour = 'Handed') +
  geom_point() + theme_bw(15)+
  annotate("text", x = 3.2, y = -3,label=text3,size=3)+
  annotate("text", x = -2.5, y = 5.5,label=text1,size=3)+
  annotate("text", x = -2.5, y = 3.5,label=text2,size=3)+
  geom_hline(yintercept=0,linetype="dashed",colour="grey")+
  geom_vline(xintercept=0,linetype="dashed",colour="grey")+
  xlim(myrange)+
  ylim(myrange)+
  ggtitle(taskname)

p2 <- ggExtra::ggMarginal(
  p,
  type = 'density',
  margins = 'both',
  size = 5,
  groupColour = TRUE,
  groupFill = TRUE
)
return(p2)

}

```

<!--- Need to decide on colour scheme and stick to it-->
```{r selectplots,echo=F}
tasknames <- c('Word generation','Sentence generation','Phonological decision','Word comprehension','Sentence comprehension','Syntactic decision')
ddat$Handed<-as.factor(ddat$Rhanded)
levels(ddat$Handed)<-c("Left","Right")
for (i in 1:length(tasknames)){
  col1<- paste0(LETTERS[i],"_mean_odd")
  col2<- paste0(LETTERS[i],"_mean_even")
  col3 <-paste0(LETTERS[i],"_mean_LI")
  c1<-which(colnames(ddat)==col1)
  c2<-which(colnames(ddat)==col2)
  c3<-which(colnames(ddat)==col3)
  h <- which(colnames(ddat)=='Handed')
  
  myfile <- ddat[,c(h,c1,c2,c3)]
  mycolnames <- c('Odd','Even','All')
  myrange=c(-5,6)
  p2<-doscatterplus(myfile,tasknames[i],mycolnames,myrange)
  p2
   mypath<-paste0(mydir,"/03-graphic-outputs")
  plotname<-paste0(mypath,'/OddEven_',LETTERS[i],'.png')
  ggsave(plotname,p2,width = 5, height = 4)

  
}

```





## LI Summary Statistics

The pirate plot shows LI values for the six tasks (A = Word Generation, B = Sentence Generation, C = Phonological Decision, D = Word Comprehension, E = Sentence Comprehension and F = Syntactic Decision) for left and right handed participants. One sample t-tests were computed to test whether the group LI values differed significantly from zero, i.e. showed significant lateralisation. In left handers, tasks A, B and C were left lateralised; task D was right lateralised; and tasks E and F were not significantly lateralised. In right handers, all six tasks differed significantly for zero. (NB - no correction for multiple comparisons). Between-group t-tests were also computed to test whether lateralisation differed between left and right handers. In all cases, LI values were significantly stronger in the right handers than the left handers.

```{r LIpirates, echo=FALSE, warning=FALSE}
#NB Formatting of this figure needs to be tweaked to achieve good resolution and legibility, but we can do that when we know what format figures need to be created in. 

#Make task names that will print on 2 lines for compactness
tasknames2 <- c("Word\ngeneration","Sentence\ngeneration","Phonological\ndecision","Word\ncomprehension","Sentence\ncomprehension","Syntactic\ndecision")
#Now make text locations for these on pirate plot: we'll place A-C below plot and D-F above it
horizpts<-rep(1,6)
for (i in 1:6){
  horizpts[i] <- 1+(i-1)*3
}
vertpts <- rep(-5.5,6)
vertpts[4:6]<-6

 mypath<-paste0(mydir,"/03-graphic-outputs")
  plotname<-paste0(mypath,'/ftcd_pirates.jpg')
jpeg(plotname, width = 800, height = 500)

LIdata <- ddat %>% 
  dplyr::select(ID, Rhanded, A_mean_LI, B_mean_LI, C_mean_LI, D_mean_LI, E_mean_LI, F_mean_LI)
colnames(LIdata) <- c('ID','Handed','A','B','C','D','E','F')
LIdata$Handed <- as.factor(LIdata$Handed)
levels(LIdata$Handed)<-c("L","R")

#only one person had missing data on task A, and they had NA for all tasks.
#We will delete this case when moving to long form to avoid later problems.
w<- which(is.na(LIdata$A))

longdata.d <- pivot_longer(data = LIdata[-w,], cols = c(3:8), names_to = 'Task', values_to = 'LI')
pirateplot(data = longdata.d, LI ~ Handed * Task,ylim=c(-6,8))
abline(h=0)

for (i in 1:6){
  text(horizpts[i], vertpts[i], tasknames2[i], #add label for task
     cex = .8)
}
title(main=paste0('Distributions of LI Data \nN = ',length(LIdata$ID)))

dev.off()

```


```{r doppler-ttests,echo=F}
#Make a table to show characteristics of different tests


ftcdsummary <- data.frame(matrix(NA,nrow=8,ncol=7))
colnames(ftcdsummary)<-c('Statistic',LETTERS[1:6])
ftcdsummary[,1]<-c('N','Mean (SD) L-hander','Mean (SD) R-hander','one-group t L-hander','one-group t R-hander','R-hander vs L-hander t','Split half r','Test-retest r')
```

```{r fill-ftcdsummary,echo=F}
#We use the same function 'populate' to populate the data frame as we had for behavioural tasks - the 'ftcd' term at end of function call ensures correct columns are found
for (t in 1:6){
mytask<-LETTERS[t]
writecolnum <- 1+t #column of online summary to write to for this task

ftcdsummary <- populate(mytask,ftcdsummary,writecolnum,'ftcd')
}
ft<-flextable(ftcdsummary)
ft<-autofit(ft)
ft
```
### Checking normality of fTCD data

The planned approach to analysis, structural equation modeling (SEM), assumes multivariate normality. A preliminary check using the Shapiro-Wilk test found that the distribution of LIs on most of the measures departed significantly from normal, typically because of skew, with a long tail on one side. To make data suitable for entry into SEM, the Box-Cox transformation was applied to normalise the distributions as far as possible. Results are shown in Table x.

```{r optimalBoxCoxFunction,echo=F}
#Where a measure is non-normal on Shapiro-Wilk test, the BoxCox transformation is applied.  Because the LIs include negative values, we first add a constant so all values are positive. After applying the BoxCox, we add another constant to ensure that the mean is unchanged by the transformation.

#The BoxCox function is similar to applying a transformation such as log or square root, but rather than pre-specifying the function, we hunt for the optimal value of lambda to give normal data, where newdata <- olddata^lambda-1/lambda

optimalBoxCox <- function(myvector){

#find optimal lambda for Box-Cox transformation 

offset<-min(myvector,na.rm=T)
origmean <- mean(myvector,na.rm=T)
respvar <- -offset+.1+myvector
bc <- boxcox(respvar ~ 1)
lambda <- bc$x[which.max(bc$y)]
newresp <- respvar^lambda-1/lambda
numean<-mean(newresp,na.rm=T)
newoffset <- numean-origmean
nuvector<-newresp-newoffset
return(nuvector)

}


```



```{r do-boxcox,echo=F}
#This function identifies columns with nonnormal data and applies optimal BoxCox function, saving the new version of the variable with suffix _bc

#NB I have set this to only do transform if nonnormality gives p < pcut.
#To ignore normalisation, just use extreme pcut 
#Concern that otherwise transform  might distort relation between variables?
normalisedf <- function(thisdat,pcut){ #need a data frame with just the variables to be normalised
  mymvn <- mvn(thisdat,mvnTest = c("mardia", "hz", "royston", "dh",
  "energy")) #useful for getting normality check on all vars
  print(mymvn)
swlist <-mymvn$univariateNormality
collist<-vector() #initialise a vector that will hold list of cols to be normalised
for (i in 1:nrow(swlist)){
  if (as.numeric(swlist[i,4])>pcut){collist<-c(collist,i)}
  if (as.numeric(swlist[i,4])<pcut){ #pvalue
    thisdat$temp<-optimalBoxCox(thisdat[,i])
    newmean<-mean(thisdat$temp,na.rm=T)
    colnames(thisdat)[ncol(thisdat)]<-paste0(colnames(thisdat)[i],'_bc')
    collist<-c(collist,ncol(thisdat))
  }
}
return(list(thisdat,collist))
}
```

```{r checkMVN,echo=F}
#check multivariate normality
thisdat <- ddat[,c('A_mean_LI','B_mean_LI','C_mean_LI','D_mean_LI','E_mean_LI','F_mean_LI')]
pcut <- .05
normed.dat<-normalisedf(thisdat,pcut)
#now select variables to be used in SEM analysis - will use the _bc versions if they exist. The collist in normed.dat gives the col numbers in correct order


thisdat <- normed.dat[[1]]
nunames<- c('A_P1','B_P2','C_P3','D_R1','E_R2','F_R3') #cols for SEM; will use _bc if it exists
thisdat[,nunames]<-thisdat[,normed.dat[[2]]]

#some checks on how the normalisation has worked.
nc <- ncol(thisdat)
for (i in 1:6){
  plot(thisdat[,i],thisdat[,(nc-6+i)],xlab=colnames(thisdat)[i],ylab=colnames(thisdat)[(nc-6+i)])
  m1<-round(mean(thisdat[,i],na.rm=T),2)
  m2<-round(mean(thisdat[,(nc-6+i)],na.rm=T),2)
  text(-2,4,paste0(m1,': ',m2))
  abline(0,1,lty=2)
}

#remove case if no data on A_P1 - need to do this for both ddat and thisdat
w<-which(is.na(thisdat$A_P1))
if(length(w)>0){
thisdat<-thisdat[-w,]
ddat<-ddat[-w,]
}

#Interpolate missing values using mice package
thisdat.i <- mice(thisdat[,nunames], m=1, maxit = 50, method = 'pmm', seed = 500)
ddati <-cbind(ddat[,c('ID','male','Handed')],complete(thisdat.i,1))


```





## Hypothesis 1: Testing a two-factor model using behavioural data

__The pattern of correlation between laterality indices from online measures will reflect the extent to which they involve implicit speech production, rather than whether they involve spoken or written language. Thus we anticipate dissociation between the rhyme judgement task and the other two measures (dichotic listening and OVP task), which is not accountable for in terms of low reliability of measures.__ 

For this analysis we focused on predicting patterns of covariance between the three online tasks, as we do not have sufficient indicators for modeling with latent variables. We prespecified four possible covariance structures, which were then compared using AIC weights. This is a subset of SEM that does not include any latent variables or directional paths. It allows us to constrain particular covariance patterns and report the 'best' model according to AIC weights.

The four models are:  
Model A, where all LIs are intercorrelated to a similar degree
Model B1,  where LIs for the two receptive language measures (dichotic listening and word comprehension) are intercorrelated, but independent of rhyme detection
Model B2, where LIs for the two tasks involving visual presentation and written language (word comprehension and rhyme detection) are intercorrelated, and independent of the auditory task, dichotic listening.
Model C, where all LIs are independent of one another.

Models B1 and B2 are mathematically equivalent, differing only in the specific variables that are correlated. On a priori grounds, we favour model B1, which is compatible with our overall 2-factor model of language lateralisation. We tested B2, however, as we predicted that this pattern of correlation might occur if the laterality index was dependent more on mode of presentation (visual vs auditory) than on task demands.  Note that where we specified tests as correlated in a model, the extent of correlation will be constrained by test reliability. 



```{r bivplotfunction,echo=F}
#Function to make a bivariate plot subdivided by group, with spearman correlation in plot

#When we call this function we have already created temporary x and y cols (tempx and tempy) to be used in this function
bivplot<-function(bivdat,name1,name2){
#correlations for each group
cor1 <- cor.test(bivdat$tempx[bivdat$Group==1],bivdat$tempy[bivdat$Group==1],method="spearman")
cor2 <- cor.test(bivdat$tempx[bivdat$Group==2],bivdat$tempy[bivdat$Group==2],method="spearman")
lab1<- paste0("Group 1: rs = ",round(cor1$estimate,3))
lab2<- paste0("Group 2: rs = ",round(cor2$estimate,3))   

myplot <- ggplot(bivdat, aes(x=tempx, y=tempy, color=Handedness,shape=as.factor(Group))) +
  xlab(name1)+
   ylab(name2)+
  xlim(-10,10)+
  ylim(-10,10)+
  geom_point()+
  scale_shape_manual(name="Group (random)",
                     labels=c(1,2),
                     values=c(1,16))+
  scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
  geom_hline(yintercept=0,linetype="dashed")+
   geom_vline(xintercept=0,linetype="dashed")+
  annotate("text", x=-5, y=9.5, label= lab1,size=3) +
  annotate("text", x=-5, y=8, label= lab2,size=3)   
  return(myplot)
}


```

```{r behavcorrs,echo=F}


bivdat <- filter(mycomb,excludeRDT==0,excludeDL==0,excludeWC==0)
#Assign Group at random
set.seed(50) #make reproducible
bivdat$Group<-1+rbinom(nrow(bivdat),1,.5)
#Check handedness distribution
handgrouptab <- table(bivdat$Handed,bivdat$Group) #This is just to confirm roughly equal distribution of L and R handers in the 2 random groups.


#tempx and tempy are reassigned to variables of interest before calling generic function that will base plot on these two variables
bivdat$tempx <- bivdat$DL.zlat
bivdat$tempy <- bivdat$RDT.zlat
name1 <-"Dichotic Listening z-lat"
name2 <- "Rhyme decision z-lat"
DL_RD_plot <- bivplot(bivdat,name1,name2) #this is our specially created function
#I've commented out saving the individual plots, just because I've used ggarange to make a composite plot with all 3 pairings
#ggsave(paste0(mydir,"/03-graphic-outputs/DL-RDT.png"),width = 6, height = 4)

bivdat$tempy <- bivdat$WC.zlat
name2 <- "Word Comprehension z-lat"
DL_WC_plot <- bivplot(bivdat,name1,name2)
#ggsave(paste0(mydir,"/03-graphic-outputs/DL-WC.png"),width = 6, height = 4)

bivdat$tempx <- bivdat$RDT.zlat
name1 <- "Rhyme Decision z-lat"
RD_WC_plot <- bivplot(bivdat,name1,name2)
#ggsave(paste0(mydir,"/03-graphic-outputs/RD-WC.png"),width = 6, height = 4)

#For now am making a plot with all 3 scatterplots together in a row. Easy to change layout if needed.
allplot <- ggarrange(DL_RD_plot, DL_WC_plot, RD_WC_plot, ncol = 3, nrow = 1,common.legend=TRUE)
ggsave(paste0(mydir,"/03-graphic-outputs/allbiv.png"),width = 9, height = 3)
```

Our power analysis showed that the best way to demonstrate superiority of a 2-factor model is by replicating model comparison on two random halves of the sample. Accordingly, the sample was divided into two random halves: Group 1 contained `r handgrouptab[1,1]` left-handers and `r handgrouptab[2,1]` right-handers; Group 2 contained `r handgrouptab[1,2]` left-handers and `r handgrouptab[2,2]` right-handers.  

Figure x shows scatterplots of the bivariate relationships between the three variables, coded by Group (i.e. the random subset 1 or 2). It is evident from inspection that we can reject model C, in which all three LIs are independent, and model B1, where only Dichotic Listening and Rhyme Decision are correlated. The strongest correlation is between the two visual tasks, Rhyme Decision and Word Comprehension, as predicted by model B2. However, on formal test, this model is rejected, because it predicts no association between Dichotic Listening and the other variables, whereas in practice a weak association is found between Dichotic Listening and Rhyme Judgement. 

FOR DISCUSSION:  
NEED TO SAY SOMETHING ABOUT RELIABILITY -  COULD WE REPEAT WITH ODD/EVEN INCORPORATED?
ALSO, THIS ANAYSIS IS PARAMETRIC WHEREAS DATA IS NON_NORMAL_ NEED TO CONSIDER IF WE SHOULD DO SOMETHING ABOUT THIS.
NOTE THAT THE CORRELATION BETWEEN WC AND RDT IS ACTUALLY QUITE CLOSE TO THE RELIABILITY OF WC.





```{r aicmodels}
#Specify models

  #Model A all correlated
    modelA<-"
    Rhyme~~Dichotic
    Rhyme~~Comprehension
    Dichotic~~Comprehension
    "
    #Model B 2 correlated (Comprehension and dichotic)
    modelB<-"
    Rhyme~~0*Dichotic
    Rhyme~~0*Comprehension
    Dichotic~~Comprehension"
    
    #model C all independent
    modelC<-"
    Rhyme~~0*Dichotic
    Rhyme~~0*Comprehension
    Dichotic~~0*Comprehension"

    #Model BB 2 correlated (Comprehension and rhyme)
    modelBB<-"
    Rhyme~~0*Dichotic
    Rhyme~~Comprehension
    Dichotic~~0*Comprehension"
    
```    

```{r runAIC}
set.seed(50) #make reproducible
bLIdat$Group<-1+rbinom(nrow(bLIdat),1,.5)
table(bLIdat$Handed,bLIdat$Group)


AICdf <- data.frame(matrix(NA,nrow=2,ncol=7))
colnames(AICdf)<-c('Group','modelA','modelB1','modelB2','modelC','bestfit','chi.p.vs.C')
group <- c('Group.1','Group.2')
latcols <- c('Dichotic','Rhyme','Comprehension')

thisrow<-0

  for (g in 1:2){ # grouping - 1 or 2}
    
    aicdat<-bLIdat
    if(g==1){aicdat<-bLIdat[bLIdat$Group==1,]}
    if(g==2){aicdat<-bLIdat[bLIdat$Group==2,]}  

    thisrow<-thisrow+1
 
    fitA <- cfa(modelA, data=aicdat) #
    fitB1 <- cfa(modelB, data=aicdat)
    fitC <- cfa(modelC, data=aicdat)
    fitB2 <- cfa(modelBB,data=aicdat)
    
   aic_vector1<- c(fitMeasures(fitA, "aic"),fitMeasures(fitB1, "aic"),fitMeasures(fitB2, "aic"),fitMeasures(fitC,"aic"))
   
    AICdf$Group[thisrow]<-g
    AICdf[thisrow,2:5]<-round(akaike.weights(aic_vector1)$weights,3)
   bestfit <- which(AICdf[thisrow,2:5]==max(AICdf[thisrow,2:5]))
     AICdf$bestfit[thisrow] <- LETTERS[bestfit]
     

    
 if(bestfit<3)
 {
   compfit<-fitA
   if(bestfit==2){compfit<-fitB1}
#If true model is not C, can it be distinguished from model C (all independent)
    AICdf$chi.p.vs.C[thisrow] <- round(anova(compfit,fitC)$`Pr(>Chisq)`[2],3)
 }
 }#end of g loop


#Need to add appropriate label to table - the entries in body of table are Aikake weights
flextable(AICdf)
```
Summary from the Aikake weights analysis is shown in Table x. In both Groups, the best fitting model is Model A. Nevertheless, consideration of the results of the two subgroups indicates how labile results from such an analysis can be, even with over 200 participants per group. In Group 1, the difference between Models A and B2 is small, whereas in Group 2, Model A is the clear winner.

NB - this may need reconsidering depending on how we handle issues of reliability/non-normality.

## Hypothesis 2: Testing a two-factor model using fTCD data

Our second preregistered prediction was: __The data will fit a model where 'language generation' tasks cluster together on one factor, and 'receptive languageâ€™ tasks on a second factor.__  

It was further predicted that factors will be correlated, but the fit of a 2-factor model will be superior to a single-factor model where all LIs load on a common factor.
 
The analysis conducted by Woodhead et al (2019, 2020) used an exploratory bifactor model in which each task could load on each of two factors. Because we had two measures for each task (from test and retest sessions), this exploratory approach was adequately powered. In the current study, we changed some of the tasks, and we only had one measurement occasion for each of the six measures. Accordingly we used confirmatory factor analysis, using a prespecified two-factor model which constrains which indicators can load on two factors. This is compared to a unitary model, in which all tasks load on a single factor.

```{r modelfit}

#nb we will use the variables from ddati


ddati$randgroup<-1+rbinom(nrow(ddati),1,.5) #create random group 1 or 2 for later split

#Add correlation matrix

LIcols <- c("A_P1","B_P2","C_P3","D_R1","E_R2","F_R3")

myheatmap <- makeheatmap(ddati,LIcols)
myheatmap
#Saved heatmap needs a bit of tweaking! Size of axis labels and grey background need fixing
ggsave(paste0(mydir,"/03-graphic-outputs/ftcd-heatmap.png"),width = 6, height = 6)

```
```{r makeSEMtab,echo=F}
#Function to make tidy table for SEM output for model with one group
#This is now largely superseded by bigsummary, though SEMtab has more complete information
makeSEMtab <- function(myfit){
ss<- summary(myfit)$PE
 srow<-nrow(ss)
 scol<-ncol(ss)
mySEMout <-ss[,-4]
mySEMout[,4:7]<-round(mySEMout[,4:7],3)

fm<-fitmeasures(myfit)
mySEMout[(1+srow),1]<-'CFI'
mySEMout[(1+srow),4]<-round(fm[9],3)
mySEMout[(2+srow),1]<-'rmsea'
mySEMout[(2+srow),4]<-round(fm[24],3)
return(mySEMout)
}
```


```{r initialisebigsummary,echo=F}
#Initialise a table to show factors loadings and some other stuff (CFA, rmsea) for each model in a column, so we can compare them
bigsummary <- data.frame(matrix(NA,nrow=19,ncol=8))
colnames(bigsummary)<-c('Estimate','Model.1F','Model.2F','Model2Fn.1','Model2Fn.2','Model2Fn','Model2Fn.L','Model2Fn.R')
bigsummary[,1]<-c('NObs','A -> Fac1','B -> Fac1','C -> Fac1','D -> Fac1','E -> Fac1','F -> Fac1','A -> Fac2','B -> Fac2','C -> Fac2','D -> Fac2','E -> Fac2','F -> Fac2','Fac1~~Fac2','CFI','rmsea','chisq','DF','p vs Model.1F')
```



```{r makebigSEMtab,echo=F}
#This function adds to the bigsummary data frame - it takes the paths and a few diagnostic stats from myfit and writes to writecol in bigsummary. If comparisonfit is specified, it will also do a chi square comparison with that model and put p value in final row.
addmodel <- function(bigsummary,myfit,comparisonfit,myfitname,writecol){
  colnames(bigsummary)[writecol]<-myfitname #name of the model as column name
ss<- summary(myfit)$PE #get coefficients from current model
bigsummary[1,writecol]<-lavInspect(myfit,'nobs') #sample size goes in 1st row

thisrow<-1 #initialise counter for factor loadings
#if a relevant path exists, put its estimate in correct row; first Factor1, then Factor2
#If  no path in the model, it just skips it
for (f in 1:2){
for (m in 1:6){
  thisrow<-thisrow+1
  w<-which(ss$lhs==paste0('f',f))
  x<-which(substr(ss$rhs,1,1) == LETTERS[m])
  myrow<-intersect(w,x)[1]
  bigsummary[thisrow,writecol]<-round(ss$est[myrow],3)
}
}
#Correlation between factors is added (if it exists)
myrow<-intersect(which(ss$lhs=='f1'),which(ss$rhs=='f2'))[1]
if(length(myrow)>0){
  bigsummary[14,writecol]<-round(ss$est[myrow],3)
}

wantfits <- c('CFI','rmsea','chisq','df') #fit indices to include
#NB there are many other fit indices we could add, but if we do, would need to modify bigsummary.
#Currently script assumes we will have these 4 fit indices 

fm<-fitmeasures(myfit,wantfits)
#find first row to write to
r<-which(bigsummary[,1]==wantfits[1]) #find row corresponding to first fit index
bigsummary[r:(r+length(wantfits)-1),writecol]<-fm #write the fit indices in successive rows, starting with r

bigsummary[2:17,writecol]<-round(bigsummary[2:17,writecol],3) #round all the numeric values


#Add chi square comparison with comparisonfit
if(!is.na(comparisonfit)){
chicomp <- fitmeasures(comparisonfit,wantfits[3:4]) #get chisq and DF for comparison model
chidiff<-chicomp[1]-bigsummary[17,writecol]
dfdiff<-chicomp[2]-bigsummary[18,writecol]
pval<- 1-pchisq(chidiff,dfdiff) #p-value for chi sq difference
bigsummary[19,writecol]<-pformat2(pval) #p-value written to row 19
}

#because this is data.frame, all values in a column must be same format.
#Starts numeric, but that means Nobs and DF is shown to 3 decimal places. 
#Can remove decimal places with line below, but then all values become characters
#In practice, I've found trying to format creates problems, so better to format bigsummary outside this function
#bigsummary[1,mycol]<-as.character(round(bigsummary[1,mycol],0))

return(bigsummary)
}
```


```{r factormodels,echo=F}
#In lavaan, we first define the factor model between quotes
#So this step doesn't do anything - just sets up the model to be run later

#Single factor model : I've kept here the original way we defined this, but it's confusing becausae it does refer to f1 and f2, though they are then designed to have correlation of 1, so are in effect identical.  
#But further confusion because the first variable on the right hand side defaults to being the index variable, with path = 1. So if set up with 2 factors, both A and D are set to one.
#So next bit of code is just preserved in case we do need it, but seems preferable to define more simply
# model.1F <- '
# f1 =~   A_P1 + B_P2 + C_P3
# f2 =~   D_R1 + E_R2 + F_R3
# 
# f1 ~~ 1*f2  #single factor model, f1 and f2 constrained to covariance of one
# 
# ' 

#This is definition of single factor model we will use here. A_P1 will be index variable with path of 1. 
model.1F <- 'f1 =~  A_P1 + B_P2  + C_P3 +D_R1 + E_R2 + F_R3' 


#2 factor production/reception model
model.2F <- '
f1 =~  A_P1 + B_P2+C_P3
f2 =~ F_R3 + D_R1 + E_R2  #2 factor model: 
#covariance unspecified, which means there is no constraint on covariance

'


fit1 <- cfa(model.1F, data=ddati) #runs the model and saves results in fit1
sfit1 <- makeSEMtab(fit1) #saves the results from the model in a neat format
bigsummary<- addmodel(bigsummary,fit1,NA,'Model.1F',writecol=2) #summary from this model written to col 2 of bigsummary. For single factor model we don't specify a comparison model, hence NA.

#lavResiduals(fit1) #if we want to understand reasons for poor fit, we can look at residuals - shows size of covariances that aren't explained by model.

#Creates a structural diagram for single factor model: nb does NOT include path estimates (these can be shown if we put 'par' rather than 'diagram', but it gets messy, and the parameters are shown instead in a table)
#lots of details of this here
#https://www.rdocumentation.org/packages/semPlot/versions/1.1.2/topics/semPaths

pathfigname<-paste0(mydir,"/03-graphic-outputs/pathfigF1")
semPaths(fit1, "diagram", weighted = FALSE,  shapeMan = "rectangle", sizeMan = 8, 
    sizeMan2 = 5,filetype='jpg',filename=pathfigname,width=3,height=2) #draws a path diagram

fit2 <- cfa(model.2F, data=ddati)
sfit2 <- makeSEMtab(fit2)
bigsummary<- addmodel(bigsummary,fit2,fit1,'Model.2F',writecol=3) #summary from this model written to col 3 of bigsummary

#lavResiduals(fit2)
pathfigname<-paste0(mydir,"/03-graphic-outputs/pathfigF2")
semPaths(fit2, "diagram", weighted = FALSE,  shapeMan = "rectangle", sizeMan = 8, 
    sizeMan2 = 5,filetype='jpg',filename=pathfigname,width=3,height=2) #draws a path diagram
#anova(fit1,fit2) #compares model fit -if significant, means 2nd model is better fit than 1st. This information is now included in bigsummary.





```
The fit of both the one-factor and the two-factor model is poor. Therefore, as planned we divided the sample into two random subsamples, 1 and 2. The first subsample is used in an exploratory analysis. We start with tasks A-E linked to Factor 1 and tasks B-F linked to Factor 2, and then drop non-significant paths. This gives a revised model with considerably improved fit, as shown in Figure x, where tasks A, B, C, and E load on Factor 1, and tasks C, D, E and F load on Factor 2. 

This model is then run using data from subsample 2. Table x shows that the fit is actually better for subsample 2 than subsample 1, but it also indicates how model fit can vary with samples of this size. Table x also shows the fit of the revised 2-factor model for the full sample. The model fit falls short of meeting criteria for a good fit, though it is much better than for the previous model. Given the limitations of the dataset, we concluded this is likely to be the optimal model that can be achieved, and so we proceeded to test whether the same model was appropriate for left- and right-handers.  






```{r case175remove}
removebit <-0 #just keeping this here but want to avoid triggering it!

if(removebit==1){
#case 175 looks v odd - Left hander who is bivariate outlier - +ve for A and F, -ve for others
  #Try removing to see if this case is causing negative variances in variable B
  #Nope - doesn't fix the problem. Keep this one in
ddati<-ddati[-175,]
}

```



```{r newmodel,echo=F}
#We explore for best model using just half the data (randgroup = 1)
mygroup <-1
mydat<- ddati[ddati$randgroup==mygroup,]
model.2Fn.0 <- '
f1 =~  A_P1 + B_P2+C_P3+D_R1+ E_R2
f2 =~  F_R3+ B_P2+C_P3+D_R1+ E_R2 
'
fit.2Fn.0 <- cfa(model.2Fn.0, data=mydat)
summary(fit.2Fn.0)
wantfits <- c('CFI','rmsea','chisq','df')

fitmeasures(fit.2Fn.0,wantfits)

 #modification indices aren't much help - just recommend adding covs between obs variables.
mi <- modindices(fit.2Fn.0, sort = TRUE, maximum.number = 20)
mi[mi$op == "=~",]

#Changes to model based on observation of size of paths

#After checking model from group 1, now try running with 2nd group and with full sample

model.2Fn <- '
f1 =~  A_P1 + B_P2+C_P3+ E_R2
f2 =~  F_R3 + C_P3+D_R1+ E_R2   #2 factor model: no constraint on covariance
'
fit.1F.1 <-cfa(model.1F, data=mydat)
fit.2Fn.1 <- cfa(model.2Fn, data=mydat)
bigsummary<- addmodel(bigsummary,fit.2Fn.1,fit.1F.1 ,'Model.2Fn.1',writecol=4) #summary from this model written to col 4 of bigsummary - becauase different subsample, can't compare with prior model in bigsummary


#model2Fn.1 is best-fitting model from exploration with group 1.


#After checking model from group 1, now try running with 2nd group and with full sample
#It's less good with group 2 but not disastrous, but it does give negative variance estimates for B for group 2, which I can't get to the bottom of.

mydat<- ddati[ddati$randgroup==2,] #explore model just with group 1
fit.2Fn.2 <- cfa(model.2Fn, data=mydat)
fit.1F.2 <- cfa(model.1F, data=mydat)
bigsummary<- addmodel(bigsummary,fit.2Fn.2,fit.1F.2 ,'Model.2Fn.2',writecol=5)

mydat<-ddati #full sample
fit.2Fn <- cfa(model.2Fn, data=mydat)
bigsummary<- addmodel(bigsummary,fit.2Fn,fit1,'Model.2Fn',writecol=6) #summary from this m


pathfigname<-paste0(mydir,"/03-graphic-outputs/pathfigF2_revised")
semPaths(fit.2Fn, "diagram", weighted = FALSE,  shapeMan = "rectangle", sizeMan = 8, 
    sizeMan2 = 5,filetype='jpg',filename=pathfigname,width=3,height=2) #draws a path diagram

#look at residuals for diagnostics
res<-resid(fit.2Fn, type = "cor") #large abs values (> .1) indicate relationships not well accounted for

#Now with L and R hander
mydat<- ddati[ddati$Handed=='Left',] #explore model just with group 1
fit.2Fn.L <- cfa(model.2Fn, data=mydat)
fit.1F.L <- cfa(model.1F, data=mydat)
bigsummary<- addmodel(bigsummary,fit.2Fn.L,fit.1F.L,'Model.2Fn.L',writecol=7) #summary from this m

mydat<- ddati[ddati$Handed=='Right',] #explore model just with group 1
fit.2Fn.R <- cfa(model.2Fn, data=mydat)
fit.1F.R <- cfa(model.1F, data=mydat)
bigsummary<- addmodel(bigsummary,fit.2Fn.R,fit.1F.R,'Model.2Fn.R',writecol=8) #summary from this m

#Compared model obtained from covariance matrix to see if we still get neg covariances - yes we do (not shown here)
#Can't get 2nd col to reformat - need to convert to character - then OK
bigsummary[,2]<-as.character(bigsummary[,2])
flextable(bigsummary)

```

```{r makefacscatter,echo=F}
myscatter <- function(myx,myy,xlabel,ylabel,thisdat){
thisscatter<-ggplot(thisdat, aes(x = myx, y = myy,color=Handed)) + 
  geom_point(shape = 4,  size = 1)+
 scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
    xlab(xlabel)+
    ylab(ylabel)
 #NB scale of factors is NOT centred on LI of zero!
  return(thisscatter)
}
```

```{r factorscores}
#FUnction to extract factor scores and plot them and save plot
makefactorplot <- function(myfit,fitname,thisdat){
lastc <- ncol(thisdat)
myfacs<-predict(myfit)
myfacs<-as.data.frame(myfacs)
colnames(myfacs)<-c("Factor1","Factor2")
w<-which(colnames(ddati)=='Factor1') #check if we already have col for factors
if(length(w)==0){
thisdat<-cbind(thisdat,as.data.frame(myfacs))
}
if(length(w)>0){
  thisdat$Factor1 <- as.data.frame(myfacs[,1])
  thisdat$Factor2 <- as.data.frame(myfacs[,2])
}

plotf1f2 <- myscatter(thisdat$Factor1,thisdat$Factor2,xlabel='Factor1',ylabel='Factor2',thisdat)

plotname <- paste0(mydir,"/03-graphic-outputs/factors_",fitname,".png")

ggsave(plotname,width = 5, height = 3)
return(thisdat)
}

```


 


```{r plotfacs, echo=F}
#Creates a plot and saves it; also adds factor scores to ddati
ddati <- makefactorplot(cfa(model.2Fn, data=ddati),'fit.2Fn',ddati)


```



```{r makeSEMtab2,echo=F}
#Table for SEM output side by side for 2 groups
makeSEMtab2 <- function(mymodel,mygroups){
ss<- summary(mymodel)$PE
 srow<-nrow(ss)
 scol<-ncol(ss)
mySEMout <- cbind(ss[1:(srow/2),c(1:3,(scol-3):scol)],ss[(1+srow/2):srow,c((scol-3):scol)])
mySEMout[,4:11]<-round(mySEMout[,4:11],3)
mySEMout[(srow/2+1):(srow/2+2),]<-NA

fm<-fitmeasures(mymodel)
mySEMout[(srow/2+1),1]<-'CFI'
mySEMout[(srow/2+1),4]<-round(fm[9],3)
mySEMout[(srow/2+2),1]<-'rmsea'
mySEMout[(srow/2+2),4]<-round(fm[23],3)
colnames(mySEMout)[4:7]<-paste0(colnames(mySEMout)[4:7],mygroups[1])
colnames(mySEMout)[8:11]<-paste0(colnames(mySEMout)[8:11],mygroups[2])
return(mySEMout)
}
```
## Model equivalency for left- and right-handers  
<!---Level of measurement equivalency are assessed through model fit of a series of nested multiple group models.  
Substantial decrease in goodness of fit indicates non-invariance
Xu: It is a good practice to look at several model fit indices rather than relying on a single one
â€¢ Î”Ï‡2
â€¢ Î”RMSEA
â€¢ Î”CFI
â€¢ Î”TLI
â€¢ Î”BIC
â€¢ Î”AIC  


Step 1: Configural invariance
â€€ Same factor structure in each group
â€€ First, fit model separately in each group
â€€ Second, fit model in multiple group but let all parameters vary freely in each group
â€€ No latent mean difference is estimated
â€€ 
â€€ 
Step 2: Weak/metric invariance
â€€ Constrain factor loadings equal across groups
â€€ This shows that the construct has the same meaning across groups
â€€ No latent mean difference is estimated
â€€ 
Step 3: Strong/scalar invariance
â€€ Constrain item intercepts equal across groups
â€€ Constrain factor loadings
â€€ This is important for assessing mean difference of the latent variable across groups
â€€ Latent mean difference is estimated
â€€ 
Step 4: Strict invariance
â€€ Constrain item residual variances to be equal across groups
â€€ Constrain item factor loadings and intercepts equal across groups. 
â€€ Strict invariance is important for group comparisons based on the sum of observed item scores, because observed variance is a combination of true score variance and residual variance
â€€ Latent mean difference is estimated --->
â€€ 

Table x shows the fit of the revised 2-factor model in each handedness group separately. It can be seen that the CFI was greater than .9 for both groups, but the RMSEA indicated there was still significant amounts of variance unexplained, especially in left-handers. Our next preplanned analysis was designed to test whether such variability in model parameters and model fit was meaningful, or whether it could be explained by random variation. 

The approach we adopted is a standard one used when structural equation modeling (SEM) is applied to evaluation of measurement models in other domains, where it is described as a test of measurement invariance. Essentially, the data from left- and right-handers are analysed together in a single model, in a series of nested models, which pose constraints on which parameters of the model are allowed to vary. Initially, a model is fit in which all the paths, covariances, and intercepts are free to differ between left- and right-handers (as shown in the last two columns of Table x, where data from the two groups were fitted separately). This model is tested against a model of 'metric invariance', which sets the loadings from each observed variable to the factors to be the same for the two groups. If the fit of the model does not worsen, we can assume the basic model structure is equivalent for the two groups. This test of equivalence was passed (see Table x2).

At the next step, (scalar invariance), the item intercepts are set to be the same across groups. Once again, the model fit did not worsen (see Table x3).

In a final step (strict invariance), we constrain item residual variances as well as factor loadings and intercepts to be equal across groups. Here we obtained a substantial worsening of model fit, indicating that the mean difference between groups on the latent factors is not the same. This is as we would expect, given the substantial differences seen in mean factor scores of left- and right-handers. 


```{r measurementinvariance,echo=F}
#https://towardsdatascience.com/measurement-invariance-definition-and-example-in-r-15b4efcab351


#library(semTools) fits increasingly restrictive models in one command


measurementInvariance(model=model.2Fn,data=ddati,group="Handed")
#This gives error message re negative variance; also says command is deprecated.
#It gives substantial change to model fit for the final fit.means model.




#However, doing the same thing a different way gives different result for fit4.
#Aargh - note the DFs are different.

#Both results are weird in that I would have thought the estimates of observed intercepts are v different for the 2 groups, and so would expect fit3 to be highly significant. Makes me think it will be necessary to check this another way - maybe in OpenMx.  
#3/2/22 - Kline book p 256 is helpful: problem is that we scale both groups so that indicator variable is 1, but means differ, this can be misleading. So should 

#C


#Problems with model fit - seems due to extreme case in task D.
#w<-which(modeldat$D_R1< (-6))
#try with this case excluded
#modeldat<-modeldat[-w,] - makes no difference!

# configural invariance
fit1 <- cfa(model.2Fn, data = ddati,meanstructure=T, group = "Handed")
tab1 <- makeSEMtab2(fit1,c('_L','_R'))  #in tab1, everything varies for the 2 groups

# weak invariance
fit2 <- cfa(model.2Fn, ddati, meanstructure=T,group = "Handed",
            group.equal = "loadings")
tab2 <- makeSEMtab2(fit2,c('_L','_R'))
#in tab2, the factor loadings are same for the 2 groups, but everything else can vary




# strong invariance
fit3 <- cfa(model.2Fn, ddati, meanstructure=T,group = "Handed",
            group.equal = c("intercepts", "loadings"))
tab3 <- makeSEMtab2(fit3,c('_L','_R'))
#THis does have same intercepts and loadings, but the correlation between f1/f2 and the variances of each factor can differ, as well as the residuals.

# strong invariance with variances too?
fit3a <- cfa(model.2Fn, ddati,meanstructure=T, group = "Handed",
            group.equal = c("intercepts", "loadings","lv.variances","lv.covariances"))
tab3a <- makeSEMtab2(fit3a,c('_L','_R'))


#THis does have same intercepts and loadings, but the correlation between f1/f2 and the variances of each factor can differ, as well as the residuals.


fit4 <- cfa(model.2Fn, ddati, meanstructure=T,group = "Handed",
 group.equal = c("residuals","loadings","intercepts","lv.variances","lv.covariances"))

tab4 <- makeSEMtab2(fit4,c('_L','_R'))

# model comparison tests
lavTestLRT(fit1, fit2, fit3,fit3a,fit4)

#see https://users.ugent.be/~yrosseel/lavaan/multiplegroup6Dec2012.pdf


#measurementInvariance(model=model.h2a, data = ddati, group = "Handed")

```

So results from measurement invariance test are very counterintuitive. The model fit does not worsen when intercepts are fixed - yet the intercepts are hugely different for L and R handers, with massive effect size on t-test. So I am wondering if I am misunderstanding what the analysis does here. 

```{r alternativefactorss}
#Fixing variance of F1 and F2 rather than one path
#2 factor production/reception model
model.2Fx <- '
f1 =~  NA*A_P1 + B_P2+C_P3
f2 =~ NA*F_R3 + D_R1 + E_R2  #original 2 factor model: 
#covariance unspecified, which means there is no constraint on covariance
f1 ~~ 1*f1
f2 ~~ 1*f2
'
fit5 <- cfa(model.2Fx, ddati, meanstructure=T,group = "randgroup")
tab5G <- makeSEMtab2(fit5,c('_G1','_G2'))

#Just use group 1
ddatg1 <- ddati[ddati$randgroup==1,]
ddatg2 <- ddati[ddati$randgroup==2,]

model.2Fy <- '
f1 =~  NA*A_P1 + B_P2+C_P3+D_R1+ E_R2  #5 loadings per factor
f2 =~  NA*F_R3+ B_P2+C_P3+D_R1+ E_R2
#covariance unspecified, which means there is no constraint on covariance
f1 ~~ 1*f1
f2 ~~1 *f2
'
fit6 <- cfa(model.2Fy, ddatg1, meanstructure=T,group = "randgroup")
tab6G1 <- makeSEMtab(fit6) 

#remove D from F1 and B from F2 and run again
model.2Fw <- '
f1 =~  NA*A_P1 + B_P2+C_P3+F_R3+ E_R2  #5 loadings per factor
f2 =~  NA*F_R3+ A_P1+C_P3+D_R1+ E_R2
#covariance unspecified, which means there is no constraint on covariance
f1 ~~ 1*f1
f2 ~~1 *f2
'
fit7 <- cfa(model.2Fw, ddatg1, meanstructure=T,group = "randgroup")
tab7G1 <- makeSEMtab(fit7) 

#remove F from F1 and A from F2!  So this leaves:

model.2Fp <- '
f1 =~  NA*A_P1 + B_P2+C_P3+E_R2  #5 loadings per factor
f2 =~  NA*F_R3+ C_P3+D_R1+ E_R2
#covariance unspecified, which means there is no constraint on covariance
f1 ~~ 1*f1
f2 ~~1 *f2
'
fit8 <- cfa(model.2Fp, ddatg1, meanstructure=T)
tab8G1 <- makeSEMtab(fit8) 
#Still get -ve covariance for B - and for some others


fit9 <- cfa(model.2Fp, ddatg2, meanstructure=T)
tab8G2 <- makeSEMtab(fit9) 
#Perfect fit !!!  But still some -ve covs


measurementInvariance(model=model.2Fp,data=ddati,group="randgroup")

measurementInvariance(model=model.2Fp,data=ddati,group="Handed")

fit0 <- cfa(model.2Fp, ddati, meanstructure=T,group = "Handed")
tab0H <- makeSEMtab2(fit0,c('_L','_R'))
#This has 12 df  #npar = 42
#16 paths, 12 vars, 2 cov, 12 means
#16+12+2+12 =42


fit10 <- cfa(model.2Fp, ddati, meanstructure=T,group = "Handed",
 group.equal = c("residuals","loadings","intercepts","lv.variances","lv.covariances"))

tab10H <- makeSEMtab2(fit10,c('_L','_R'))
#This has 31 df  #npar = 23
#8 paths, 6 vars, 1 cov, 6 means, 2 factor vars (just for group R - fixed in group L)
# 8+6+1+6+2 = 23


fit11 <- cfa(model.2Fp, ddati, meanstructure=T,group = "Handed",
 group.equal = c("residuals","loadings","intercepts","means"))

tab11H <- makeSEMtab2(fit11,c('_L','_R'))
#This has 32 df  #npar = 22
#8 paths, 6 vars, 2 covs (differ for L and R), 6 means, 
# 8+6+2+6 = 22

fit12 <- cfa(model.2Fp, ddati, meanstructure=T,group = "Handed",
 group.equal = c("residuals","loadings","means"))

tab12H <- makeSEMtab2(fit12,c('_L','_R'))
#This has 26 df  #npar = 28
#So this is same df as for the final model in the measurement invariance
#8 paths, 6 vars, 12 means, 2 covs
#Note that the fixed factor means are both zero, but the intercepts vary between groups


#This has 26 df  #npar = 28
#So this is same df as for the final model in the measurement invariance
#8 paths, 6 vars, 12 means, 2 covs
#Note that the fixed factor means are both zero, but the intercepts vary between groups


fit13 <- cfa(model.2Fp, ddati, meanstructure=T,group = "Handed",
 group.equal = c("residuals","loadings","intercepts","means"))

tab13H <- makeSEMtab2(fit13,c('_L','_R'))
#This has 32 df, 22 parameters


fit14 <- cfa(model.2Fp, ddati, meanstructure=T,group = "Handed",
 group.equal = c("loadings","intercepts"))

tab14H <- makeSEMtab2(fit14,c('_L','_R'))
fitmeasures(fit14,c("npar","chisq", "df", "pvalue", "cfi", "rmsea"))
#This has 24 df, 30 parameters



#DF for the 4 outputs from measurement invariance are 12, 20, 24 and 26


# model comparison tests
lavTestLRT(fit1, fit2, fit3,fit3a,fit4)


measurementInvariance(model=model.2Fp,data=ddati,group="Handed")

```




```{r facdir}   
#this is not sensible! Factor scale is NOT related to LI so above or below 0 is not meaningful. 
ddati$F1F2<-0
w<-which(ddati$Factor1>0)
ddati$F1F2[w]<-ddati$F1F2[w]+10
w<-which(ddati$Factor2>0)
ddati$F1F2[w]<-ddati$F1F2[w]+1
table(ddati$F1F2,ddati$Handed)


```


## Analyses using odd and even LIs

```{r prepare-oddevens,echo=F}
oddeven <-0
if(oddeven == 1){

#I had hoped having 2 measures per task - ie odd and even LI - would improve fit, but it gets much worse!
#Not sure why - I guess there are many more terms in the model and also the individual measures are less reliable.
#I'll leave this here for historic interest, but for the analysis of 2 handedness groups, seems preferable to revert to the previous analysis with the regular mean LIs. 

#We will adopt the approach used in previous paper by Woodhead et al, using the odd and even LIs for each task as repeated measures. 

#We first need to create a file with the relevant measures renamed

wantcols <-c('ID','male' ,'Handed','DopExclude','A_mean_even','A_mean_odd','B_mean_even','B_mean_odd','C_mean_even','C_mean_odd','D_mean_even','D_mean_odd','E_mean_even','E_mean_odd','F_mean_even','F_mean_odd')
ddat2 <- ddat[,wantcols]

w<-which(is.na(ddat2$A_mean_even)) #subject with missing data on all LI; remove
if(length(w)>0){
ddat2<-ddat2[-w,]
}

#thisdat <- ddat[,c('A_mean_even','A_mean_odd','B_mean_even','B_mean_odd','C_mean_even','C_mean_odd','D_mean_even','D_mean_odd','E_mean_even','E_mean_odd','F_mean_even','F_mean_odd')]

#concerned that the BoxCox transform is distorting distributions - plots suggest ti is inadvisable, so ignore here 
#normed.dat<-normalisedf(thisdat)
#now select variables to be used in SEM analysis - will use the _bc versions if they exist. The collist in normed.dat gives the col numbers in correct order
#thisdat <- normed.dat[[1]]

#some checks on how the normalisation has worked.
# nc <- ncol(thisdat)
# for (i in 1:6){
#   plot(thisdat[,i],thisdat[,(nc-6+i)])
#   m1<-round(mean(thisdat[,i],na.rm=T),2)
#   m2<-round(mean(thisdat[,(nc-6+1)],na.rm=T),2)
#   text(-2,4,paste0(m1,': ',m2))
# }


w<-which(colnames(ddat2)=='A_mean_even')
nunames<- c('A_P1_e','A_P1_o','B_P2_e','B_P2_o','C_P3_e','C_P3_o','D_R1_e','D_R1_o',
            'E_R2_e','E_R2_o','F_R3_e','F_R3_o') #col names for SEM
colnames(ddat2)[w:(w+11)]<-nunames


#Interpolate missing values using mice package
thisdat.i <- mice(thisdat[,nunames], m=1, maxit = 50, method = 'pmm', seed = 500)
ddati2 <-cbind(ddat[,c('ID','male','Handed')],complete(thisdat.i,1))
set.seed(1)
ddati2$randgroup<-1+rbinom(nrow(ddati2),1,.5) #create random group 1 or 2 for later split
table(ddati2$Handed,ddati2$randgroup) #just check handedness not too unevenly split
}

```


```{r exploremodeloddeven,echo=F}

if(oddeven==1){
mydat<- ddati2[ddati$randgroup==1,]
#Start by having all measures linked to F1 and F2, with A as indicator variable for F1 and F as indicator variable for F2

model.1fbase <- '
f1 =~  1*A_P1_e + 1*A_P1_o + b*B_P2_e + b*B_P2_o + c*C_P3_e + c*C_P3_o+f*F_R3_e + f*F_R3_o + d*D_R1_e + d*D_R1_o + e*E_R2_e + e*E_R2_o
 A_P1_e + A_P1_o ~ meanA*1
 B_P2_e + B_P2_o ~ meanB*1
  C_P3_e + C_P3_o ~ meanC*1
 D_R1_e + D_R1_o ~ meanD*1
 E_R2_e + E_R2_o ~ meanE*1
  F_R3_e + F_R3_o ~ meanF*1
'

fit1fbase <- cfa(model.1fbase, data=mydat,meanstructure=T)
summary(fit1fbase)
fitmeasures(fit1fbase)

model.2forig <- '
f1 =~  1*A_P1_e + 1*A_P1_o + b*B_P2_e + b*B_P2_o + c*C_P3_e + c*C_P3_o
f2 =~  1*F_R3_e + 1*F_R3_o + d*D_R1_e + d*D_R1_o + e*E_R2_e + e*E_R2_o
 A_P1_e + A_P1_o ~ meanA*1
 B_P2_e + B_P2_o ~ meanB*1
  C_P3_e + C_P3_o ~ meanC*1
 D_R1_e + D_R1_o ~ meanD*1
 E_R2_e + E_R2_o ~ meanE*1
  F_R3_e + F_R3_o ~ meanF*1
'


#2 modified factor production/reception model 
#started with all vars linked to both factors, - that would not converge, so dropped those with weak paths
#for final model C and E load on both factors - gives reasonable (not brilliant!) fit


fit2forig <- cfa(model.2forig, data=mydat,meanstructure=T)

summary(fit2forig)
fitmeasures(fit2forig)
anova(fit1fbase,fit2forig)

#Now try original Woodhead et al model with everything attached to both factors, except for indicators.


model.2full <- '
f1 =~  1*A_P1_e + 1*A_P1_o + b1*B_P2_e + b1*B_P2_o + c1*C_P3_e + c1*C_P3_o+0*F_R3_e + 0*F_R3_o + d1*D_R1_e + d1*D_R1_o + e1*E_R2_e + e1*E_R2_o
f2 =~  0*A_P1_e + 0*A_P1_o + b2*B_P2_e + b2*B_P2_o + c2*C_P3_e + c2*C_P3_o+1*F_R3_e + 1*F_R3_o + d2*D_R1_e + d2*D_R1_o + e2*E_R2_e + e2*E_R2_o
 A_P1_e + A_P1_o ~ meanA*1
 B_P2_e + B_P2_o ~ meanB*1
  C_P3_e + C_P3_o ~ meanC*1
 D_R1_e + D_R1_o ~ meanD*1
 E_R2_e + E_R2_o ~ meanE*1
  F_R3_e + F_R3_o ~ meanF*1
'

fit2full <- cfa(model.2full, data=mydat,meanstructure=T)

summary(fit2full)
fitmeasures(fit2full)
anova(fit1fbase,fit2full)

model.2drop <- '
f1 =~  1*A_P1_e + 1*A_P1_o + b1*B_P2_e + b1*B_P2_o + c1*C_P3_e + c1*C_P3_o
f2 =~  1*F_R3_e + 1*F_R3_o + c2*C_P3_e + c2*C_P3_o++ d2*D_R1_e + d2*D_R1_o + e2*E_R2_e + e2*E_R2_o
 A_P1_e + A_P1_o ~ meanA*1
 B_P2_e + B_P2_o ~ meanB*1
  C_P3_e + C_P3_o ~ meanC*1
 D_R1_e + D_R1_o ~ meanD*1
 E_R2_e + E_R2_o ~ meanE*1
  F_R3_e + F_R3_o ~ meanF*1
'

fit2drop <- cfa(model.2drop, data=mydat,meanstructure=T)

summary(fit2drop)
fitmeasures(fit2drop)
anova(fit1fbase,fit2drop)

#Trying to constrain intercepts for odds and evens to be the same

meanA <- (mean(mydat$A_P1_e)+mean(mydat$A_P1_o))/2
meanB <- (mean(mydat$B_P2_e)+mean(mydat$B_P2_o))/2
meanC <- (mean(mydat$C_P3_e)+mean(mydat$C_P3_o))/2
meanA <- (mean(mydat$A_P1_e)+mean(mydat$A_P1_o))/2
meanA <- (mean(mydat$A_P1_e)+mean(mydat$A_P1_o))/2
meanA <- (mean(mydat$A_P1_e)+mean(mydat$A_P1_o))/2

model.2dropi <- '
f1 =~  1*A_P1_e + 1*A_P1_o + b1*B_P2_e + b1*B_P2_o + c1*C_P3_e + c1*C_P3_o
f2 =~  1*F_R3_e + 1*F_R3_o + c2*C_P3_e + c2*C_P3_o++ d2*D_R1_e + d2*D_R1_o + e2*E_R2_e + e2*E_R2_o
 A_P1_e + A_P1_o ~ meanA*1
 B_P2_e + B_P2_o ~ meanB*1
  C_P3_e + C_P3_o ~ meanC*1
 D_R1_e + D_R1_o ~ meanD*1
 E_R2_e + E_R2_o ~ meanE*1
  F_R3_e + F_R3_o ~ meanF*1
'
fit2dropi <- cfa(model.2dropi, data=mydat,meanstructure=T)

summary(fit2dropi)
fitmeasures(fit2dropi)
anova(fit1fbase,fit2dropi)
}

```

