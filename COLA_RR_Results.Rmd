---
title: "Registered Report: Results"
author: "COLA consortium"
date: "20 Feb 2022"
output:
  word_document: default
  html_document:
    df_print: paged
  pdf_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# If the package is not installed, install it. If it is installed, load it.
usePackage <- function(p) {
    if (!is.element(p, installed.packages()[,1]))
        install.packages(p, dep = TRUE)
    require(p, character.only = TRUE)
}
usePackage('dplyr')
usePackage('tidyr')
usePackage('ggplot2')
usePackage('yarrr') #for pirate plots
usePackage("osfr") #for reading files from OSF
usePackage("stringr")
usePackage("table1") #useful for making simple tables of demographic etc
usePackage("ggExtra") #for marginal plots
usePackage("ggpubr")
usePackage("patchwork") #for combining plots in ggplot
usePackage("flextable")

usePackage("tidyverse")
usePackage("here") #to find filepaths
usePackage("kableExtra")

usePackage("ggstatsplot")
usePackage("MASS") #includes boxcox function
usePackage("MBESS")
usePackage("nlme")
usePackage("semPower")
usePackage("semTools")
usePackage("bookdown")
usePackage("lavaan")
usePackage("semPlot")

usePackage("officer")
usePackage("corrr") #added by DB for easy correlations
usePackage("plyr")
usePackage("qpcR") #used in Kievit script
usePackage("ggpubr")
usePackage("reshape2")
usePackage("mice")
usePackage("MVN") #multivariate normality
usePackage("viridis") #possibly for control over colours in ggplot
usePackage("hrbrthemes") #for fancy themes in ggplot?

#hrbrthemes::import_roboto_condensed()  #not sure if needed - was in example with ggplot

options(scipen=999)

tabnumber <-0 #initialise counter for tables
fignumber <-0 # initialise counter for figs
suppfignumber<-0 
```

<!--- GENERIC FUNCTIONS START -->
```{r pformat, echo=F}
#function to format p-values
pformat=function(myp){
  pout <- paste('p =',round(myp,3))
  if(myp<.001){pout = 'p < .001'}
  return(pout)
}
```

```{r pformat2, echo=F}
#function to format p-values, without the 'p = ' bit
pformat2=function(myp){
  pout <- round(myp,3)
  if(myp<.001){pout ='< .001'}
  return(pout)
}
```


```{r LIdensityplot,echo=F}
#generic density plot, subsetted by handedness or another categorical variable
#includes line showing zero point.
#takes as input allsum; temp and cattemp are dummy columns that are assigned prior to call to the function
doLIplot <- function(myfile,temp,cattemp,mysubsetname,mysubsetlabels,xlabel,xrange){
LI.plot <- ggplot(myfile, aes(x=temp, color=as.factor(cattemp))) +
  geom_density()+
  xlab(xlabel)+
  geom_vline(xintercept = 0,lty=3)+
  xlim(xrange)+
  scale_color_manual(name=mysubsetname,
                       labels=mysubsetlabels,
                       values=c("blue","red"))


return(LI.plot)
}
```


```{r densfunction,echo=F}
#Function to do a density plot for specified file coded by group
#(Not sure if output differs from previous function! - need to check)

dodensity <- function(mydat,myx,myy,mygroup,namex,namey,namegroup,grouplabels,mylines,mytitle){
  
 
scatterdens <- ggplot(mydat,aes(x = myx, y = myy,col=mygroup)) + 
  geom_point(shape = 4,  size = 1)+
  scale_color_manual(name=namegroup,
                       labels=grouplabels,
                       values=c("blue","red"))+
  labs(x = namex,y=namey)+
  ggtitle(mytitle) 
if (mylines==1){
   scatterdens<-scatterdens+geom_abline(intercept = 0, slope = 1)
}
if (mylines==2){
   scatterdens<-scatterdens+geom_hline(yintercept = 0,linetype="dashed")+
   geom_vline(xintercept = 0,linetype="dashed")
}

 
dens1 <- ggplot(mydat, aes(x = myx, fill = mygroup)) + 
  geom_density(alpha = 0.4) + 
  geom_vline(xintercept = 0, linetype="dotted")+
  theme_void() + 
  theme(legend.position = "none")

dens2 <- ggplot(mydat, aes(x = myy, fill = mygroup)) + 
  geom_density(alpha = 0.4) + 
  geom_vline(xintercept = 0, linetype="dotted")+
  theme_void() + 
  theme(legend.position = "none") + 
  coord_flip()

alldens<-dens1 + plot_spacer() + scatterdens + dens2 + 
  plot_layout(ncol = 2, nrow = 2, widths = c(4, 1), heights = c(1, 4))

#ggsave(paste0(mydir,"/03-graphic-outputs/",plotname,".png"),width = 5, height = 4)
return(alldens)
}
```

```{r filltable,echo=F,warning=F}
#Generate function to populate summary data frame - same steps for all online tasks
#This function also repurposed for ftcd
populate <- function(mydf,mytask,mysummary,writecolnum,tasktype){
  if(tasktype=='beh'){
w<-which(colnames(mydf)==paste0(mytask,'.zlat'))
x<-which(colnames(mydf)==paste0('exclude',mytask))
odds<-which(colnames(mydf)==paste0(mytask,'.odd.zlat'))
evens<-which(colnames(mydf)==paste0(mytask,'.even.zlat'))
  }
    if(tasktype=='ftcd'){
w<-which(colnames(mydf)==paste0(mytask,'_mean_LI'))
x<-which(colnames(mydf)==paste0(mytask,'_exclude'))
odds<-which(colnames(mydf)==paste0(mytask,'_mean_odd'))
evens<-which(colnames(mydf)==paste0(mytask,'_mean_even'))
  }
  
thisdat<-mydf[mydf[,x]==0,]
thisdat$thiscol<-thisdat[,w]
normp <-shapiro.test(thisdat$thiscol)$p.value
nL <- nrow(filter(thisdat,Rhanded==0))
nR <- nrow(filter(thisdat,Rhanded==1))
tL<- t.test(thisdat$thiscol[thisdat$Rhanded==0])
tR<- t.test(thisdat$thiscol[thisdat$Rhanded==1])
sdL<-round(sd(thisdat$thiscol[thisdat$Rhanded==0],na.rm=T),2)
sdR<-round(sd(thisdat$thiscol[thisdat$Rhanded==1],na.rm=T),2)
sdall <-round(sd(thisdat$thiscol,na.rm=T),2) 
tcompare <- t.test(thisdat$thiscol~thisdat$Rhanded,alternative='less')
#write N for L and R in row 1 of online summary
mysummary[1,writecolnum]<-paste0(nL,' LH + ',nR,' RH')
mysummary[2,writecolnum]<-paste0(round(mean(thisdat$thiscol,na.rm=T),2)," (",sdall,")")
mysummary[3,writecolnum]<-paste0(round(skew(thisdat$thiscol)[1],2),' (',pformat(skew(thisdat$thiscol)[4]),')')
mysummary[4,writecolnum]<-paste0(round(kurtosis(thisdat$thiscol)[1],2),' (',pformat(kurtosis(thisdat$thiscol)[4]),')')
mysummary[5,writecolnum]<-pformat(normp)
mysummary[6,writecolnum]<-paste0(round(tL$estimate,2)," (",sdL,")")
mysummary[7,writecolnum]<-paste0(round(tR$estimate,2)," (",sdR,")")
mysummary[8,writecolnum]<- paste0('t = ',round(tL$statistic,1),'; ', pformat(tL$p.value))
mysummary[9,writecolnum]<- paste0('t = ',round(tR$statistic,1),'; ', pformat(tR$p.value))
mysummary[10,writecolnum]<- paste0('t = ',-round(tcompare$statistic,1),'; ', pformat(tcompare$p.value))
mysummary[11,writecolnum]<- round(cor(thisdat[,odds],thisdat[,evens],use='complete.obs'),3)



return(mysummary)
}
```

```{r trianglefunction,echo=F}
#Gets upper triangle of a correlation matrix
 get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat,diag=T)]<- NA
    return(cormat)
  }
```



```{r heatmapfunction,echo=F}
#Make a heatmap
# http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization
makeheatmap <- function(mydf,mycols){
cormat <- cor(mydf[,mycols],use="complete.obs")

melted_cormat <- melt(cormat)
head(melted_cormat)

upper_tri <- get_upper_tri(cormat)
# Melt the correlation matrix

melted_cormat <- melt(upper_tri, na.rm = TRUE)
# Heatmap

ggheatmap <- ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()

myheatmap <- ggheatmap + 
geom_text(aes(Var2, Var1, label = round(value,3)), color = "black", size = 4) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.5, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))

return(myheatmap)}
```


```{r bivplotfunction,echo=F,warning=F}
#Function to make a bivariate plot subdivided by group, with spearman correlation in plot. Handedness coded by shape.

#When we call this function we have already created temporary x and y cols (tempx and tempy) to be used in this function
bivplot<-function(bivdat,name1,name2){
#correlations for each group
cor1 <- cor.test(bivdat$tempx[bivdat$Group==1],bivdat$tempy[bivdat$Group==1],method="spearman")
cor2 <- cor.test(bivdat$tempx[bivdat$Group==2],bivdat$tempy[bivdat$Group==2],method="spearman")
lab1<- paste0("Group 1: rs = ",round(cor1$estimate,3))
lab2<- paste0("Group 2: rs = ",round(cor2$estimate,3))   

myplot <- ggplot(bivdat, aes(x=tempx, y=tempy, color=Handedness,shape=as.factor(Group))) +
  xlab(name1)+
   ylab(name2)+
  xlim(-10,10)+
  ylim(-10,10)+
  geom_point()+
  scale_shape_manual(name="Group (random)",
                     labels=c(1,2),
                     values=c(1,16))+
  scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
  geom_hline(yintercept=0,linetype="dashed")+
   geom_vline(xintercept=0,linetype="dashed")+
  annotate("text", x=-5, y=9.5, label= lab1,size=3) +
  annotate("text", x=-5, y=8, label= lab2,size=3)   
  return(myplot)
}


```


```{r bivplotfunction2,echo=F,warning=F}
#Function to make a bivariate plot colour coded by handedness, with spearman correlation in plot

#When we call this function we have already created temporary x and y cols (tempx and tempy) to be used in this function
bivplot2<-function(bivdat,name1,name2){
#correlations for each group
cor1 <- cor.test(bivdat$tempx[bivdat$Rhanded==0],bivdat$tempy[bivdat$Rhanded==0],method="spearman")
cor2 <- cor.test(bivdat$tempx[bivdat$Rhanded==1],bivdat$tempy[bivdat$Rhanded==1],method="spearman")
lab1<- paste0("L-handers: rs = ",round(cor1$estimate,2))
lab2<- paste0("R-handers: rs = ",round(cor2$estimate,2))   

myplot <- ggplot(bivdat, aes(x=tempx, y=tempy, color=Handedness)) +
  xlab(name1)+
   ylab(name2)+
  xlim(-10,10)+
  ylim(-10,10)+
   geom_point(shape=1, size=1)+
  scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
  geom_hline(yintercept=0,linetype="dashed")+
   geom_vline(xintercept=0,linetype="dashed")+
  annotate("text", x=-5.5, y=9.5, label= lab1,size=2.8) +
  annotate("text", x=-5.5, y=8, label= lab2,size=2.8)   
  return(myplot)
}


```

```{r scatterplusfunction,echo=F}
#Function for doing scatterplot with marginal density distributions
doscatterplus <- function(myfile, taskname,mycolnames,myrange){ #myfile contains group in col 1 and the 2 cols to plot in cols 2-3
colnames(myfile)[2:4]<- mycolnames

r <- round(cor(myfile$Odd,myfile$Even,use="complete.obs",method="spearman"),3)
myt<-t.test(myfile$All~myfile$Handed)
myt2 <- t.test(myfile$All) #single group t-test
text1 <- paste0('Mean diff from zero: \nt = ',round(myt2$statistic,2),"; ", pformat(myt2$p.value))
text2 <- paste0('L vs R handers \n(all trials):\n t = ',round(myt$statistic,2),"; ", pformat(myt$p.value))
text3 <-paste0('Spearman rho: ',r)
p <- ggplot(myfile, aes_string('Odd','Even')) +
  aes_string(colour = 'Handed') +
  geom_point() + theme_bw(15)+
  annotate("text", x = 3.2, y = -3,label=text3,size=3)+
  annotate("text", x = -2.5, y = 5.5,label=text1,size=3)+
  annotate("text", x = -2.5, y = 3.5,label=text2,size=3)+
  geom_hline(yintercept=0,linetype="dashed",colour="grey")+
  geom_vline(xintercept=0,linetype="dashed",colour="grey")+
  xlim(myrange)+
  ylim(myrange)+
  ggtitle(taskname)

p2 <- ggExtra::ggMarginal(
  p,
  type = 'density',
  margins = 'both',
  size = 5,
  groupColour = TRUE,
  groupFill = TRUE
)
return(p2)

}

```

```{r makeSEMtabfunction,echo=F}
#Function to make tidy table for SEM output for model with one group
#This is now largely superseded by bigsummary, though SEMtab has more complete information
makeSEMtab <- function(myfit){
ss<- summary(myfit)$PE
 srow<-nrow(ss)
 scol<-ncol(ss)
mySEMout <-ss[,-4]
mySEMout[,4:7]<-round(mySEMout[,4:7],3)

#add fit measures
myfitmeasures<-c('CFI','rmsea','chisq','DF') #can modify this if need be
fm<-fitmeasures(myfit,myfitmeasures)
for(i in 1:length(myfitmeasures)){
mySEMout[(i+srow),1]<-myfitmeasures[i] #write name of fit measure to col 1
mySEMout[(i+srow),2]<-round(fm[i],3) #write value to col 2
}
return(mySEMout)
}
```

```{r makeSEMtab2function,echo=F}
#Table for SEM output side by side for 2 groups
makeSEMtab2 <- function(myfit,mygroups){
ss<- summary(myfit)$PE
 srow<-nrow(ss)
 scol<-ncol(ss)
mySEMout <- cbind(ss[1:(srow/2),c(1:3,(scol-3):scol)],ss[(1+srow/2):srow,c((scol-3):scol)])
mySEMout[,4:11]<-round(mySEMout[,4:11],3)
mySEMout[(srow/2+1):(srow/2+4),]<-NA #additional rows for fit measures


#add fit measures
myfitmeasures<-c('CFI','rmsea','chisq','DF') #can modify this if need be
fm<-fitmeasures(myfit,myfitmeasures)
for(i in 1:length(myfitmeasures)){
mySEMout[(i+srow/2),1]<-myfitmeasures[i] #write name of fit measure to col 1
mySEMout[(i+srow/2),2]<-round(fm[i],3) #write value to col 2
}

#make different col headers for the 2 groups
colnames(mySEMout)[4:7]<-paste0(colnames(mySEMout)[4:7],mygroups[1])
colnames(mySEMout)[8:11]<-paste0(colnames(mySEMout)[8:11],mygroups[2])
return(mySEMout)
}
```


```{r add-to-bigsummaryfunction,echo=F}
#This function adds to the bigsummary data frame - it takes the paths and a few diagnostic stats from myfit and writes to writecol in bigsummary. If comparisonfit is specified, it will also do a chi square comparison with that model and put p value in final row.
addmodel <- function(bigsummary,myfit,comparisonfit,myfitname,writecol){
  colnames(bigsummary)[writecol]<-myfitname #name of the model as column name
ss<- summary(myfit)$PE #get coefficients from current model
bigsummary[1,writecol]<-lavInspect(myfit,'nobs') #sample size goes in 1st row
thisrow<-1 #initialise counter for factor loadings
#if a relevant path exists, put its estimate in correct row; first Factor1, then Factor2
#If  no path in the model, it just skips it
for (f in 1:2){
for (m in 1:6){
  thisrow<-thisrow+1
  w<-which(ss$lhs==paste0('f',f))
  x<-which(substr(ss$rhs,1,1) == LETTERS[m])
  myrow<-intersect(w,x)[1]
  bigsummary[thisrow,writecol]<-round(ss$est[myrow],3)
}
}
#Correlation between factors is added (if it exists)
myrow<-intersect(which(ss$lhs=='f1'),which(ss$rhs=='f2'))[1]
if(length(myrow)>0){
  bigsummary[14,writecol]<-round(ss$est[myrow],3)
}

wantfits <- c('CFI','rmsea','chisq','df') #fit indices to include
#NB there are many other fit indices we could add, but if we do, would need to modify bigsummary.
#Currently script assumes we will have these 4 fit indices 

fm<-fitmeasures(myfit,wantfits)
#find first row to write to
r<-which(bigsummary[,1]==wantfits[1]) #find row corresponding to first fit index
bigsummary[r:(r+length(wantfits)-1),writecol]<-fm #write the fit indices in successive rows, starting with r



#because this is data.frame, all values in a column must be same format.
#Starts numeric, but that means Nobs and DF is shown to 3 decimal places. 
#Can remove decimal places with line below, but then all values become characters
#In practice, I've found trying to format creates problems, so better to format bigsummary outside this function
#bigsummary[1,mycol]<-as.character(round(bigsummary[1,mycol],0))

return(bigsummary)
}
```

```{r factorscores,echo=F}
#FUnction to extract factor scores and plot them and save plot
makefactorplot <- function(myfit,fitname,thisdat){
lastc <- ncol(thisdat)
myfacs<-predict(myfit)
myfacs<-as.data.frame(myfacs)
colnames(myfacs)<-c("Factor1","Factor2")
w<-which(colnames(ddati)=='Factor1') #check if we already have col for factors
if(length(w)==0){
thisdat<-cbind(thisdat,as.data.frame(myfacs))
}
if(length(w)>0){
  thisdat$Factor1 <- as.data.frame(myfacs[,1])
  thisdat$Factor2 <- as.data.frame(myfacs[,2])
}

plotf1f2 <- myscatter(thisdat$Factor1,thisdat$Factor2,xlabel='Factor1',ylabel='Factor2',thisdat)

plotname <- paste0(mydir,"/03-graphic-outputs/factors_",fitname,".png")

ggsave(plotname,width = 5, height = 3)
return(thisdat)
}

```


```{r densplotsfunction,echo=F}
#overlapping density plots 
densplots<-function(mydat,thiscol,thisgroup,mylab){
  myplot <- ggplot(data=ddati, aes_string(x=thiscol, color=thisgroup, fill=thisgroup)) +
  geom_density(alpha=0.5,kernel="gaussian") +
  annotate(geom="text", x=-6, y=.3, label=mylab,hjust=0,size=4)+
  #scale_fill_viridis(discrete=TRUE) +
  #scale_color_viridis(discrete=TRUE) +
  xlim(-6,8)+
    ylim(0,.36)+
  geom_vline(xintercept = 0,linetype="dotted")+
  theme(axis.title.x=element_blank(),
        axis.title.y=element_blank(),legend.position="none")
  return(myplot)
}
```
<!--- GENERIC FUNCTIONS END -->

## Participants

 
```{r readcombined,echo=F}
mydir <- "~/Dropbox/COLA_RR_Analysis"
readfile<-paste0(mydir,"/02-data/combined_data.csv")
combdat <- read.csv(readfile)

#We will create a code that reflects whether strict exclusion criteria are met for language. This is 1 for lextale < 80, 10 for gram12err >1, and 11 if both are met

#One subject was wrongly coded as ftcd=1: they came for testing but could not get signal and so we reset ftcd to zero for them.
w<-which(combdat$ID==3495951)
combdat$ftcd[w]<-0

combdat$lang2exclude <- 0
w<- which(combdat$gramerr12>1)
combdat$lang2exclude[w]<-10
w<- which(combdat$lexTALE<80)
combdat$lang2exclude[w]<-combdat$lang2exclude[w]+1
#For now we set 'excluded' for any who were below 80 on LexTale or more than 1 error on short grammar AND are not native English speakers. I.e. all native English are included, regardless of language tests
combdat$excluded <- 0
w<-intersect(which(combdat$lang2exclude>0),which(combdat$nativeEnglish==0))
combdat$excluded[w] <-1

testtab <- table(combdat$Rhanded,combdat$ftcd)
ftcd.dat <- filter(combdat,ftcd==1) #has demog, behav and ftcd data for those who did ftcd
langtab <- table(ftcd.dat$Rhanded, ftcd.dat$lang2exclude,ftcd.dat$nativeEnglish)
#Langtab cells can be accessed with 3 indices corresponding to Rhanded,langexclude and native English status).

myftcd <- filter(ftcd.dat,excluded==0) #we'll do analysis on myftcd
```

_Departures from pre-registration plan_  
Our plan had been to recruit 300 left-handers and 150 right-handers for the online behavioural battery, and from these to select 112 left-handers and 112 right-handers for in-person testing. Because of disruption to in-person testing caused by pandemic restrictions, we did not meet our target numbers for in-person testing, despite over-recruiting for online testing. In total we tested `r testtab[1,2] + testtab[1,1]` left-handers, `r testtab[1,2]` of whom were tested with fTCD, and `r testtab[2,2] + testtab[2,1]` right-handers, `r testtab[2,2]` of whom were tested with fTCD. However, `r langtab[1,2,1]+langtab[1,3,1]+langtab[1,4,1]` left-handers and `r langtab[2,2,1]+langtab[2,3,1]+langtab[2,4,1]` right-handers tested with fTCD were non-native English speakers who met our criteria for excluding participants on the basis of either the lexTALE or Games with Words measures of English language competence (see below), giving final samples of `r langtab[1,1,1]+langtab[1,1,2]+langtab[1,2,2]+langtab[1,3,2]+langtab[1,4,2]` left-handers and `r langtab[2,1,1]+langtab[2,1,2]+langtab[2,2,2]+langtab[2,3,2]+langtab[2,4,2]` right-handers. An unexpected issue was that a few native English speakers failed the language screen (`r langtab[1,2,2]+langtab[1,3,2]+langtab[1,4,2]` left-handers and `r langtab[2,2,2]+langtab[2,3,2]+langtab[2,4,2]` right-handers): however, the purpose of these tests had been to exclude non-native speakers with inadequate language skills, so we retained all native English participants in the study.




### Demographic data 

```{r demog.table,echo=F}
#We'll use the table1 package to create a nice-looking summary table for demographics; this excludes those excluded on language tests, but otherwise includes everyone given the online testing.

mycomb <- filter(combdat,excluded==0) #create a version of the main data file that excludes anyone excluded on Lextale or Grammar task

mycomb$Gender <- factor(mycomb$male,levels=c(0,1),labels=c("Female","Male"))
label(mycomb$age) <- "Age (yr)"
mycomb$Native <- factor(mycomb$nativeEnglish,levels=c(0,1),labels=c("No","Yes"))
label(mycomb$Native)<-"Native English speaker"
mycomb$Bilingual <- factor(mycomb$bilingual,levels=c("No","Yes"),labels=c("No","Yes"))
mycomb$Handedness <- factor(mycomb$Rhanded,levels=c(0,1),labels=c("Left-Handed","Right-Handed"))
mycomb$ftcd <- factor(mycomb$ftcd,levels=c(0,1),labels=c("No FTCD data","With FTCD data"))

demog.table <- table1(~ Gender + age + Native+ Bilingual+EHI.LI|(factor(ftcd)+Handedness) , data=mycomb,overall=F)
ftab <- t1flex(demog.table) #convert from table1 to flextable format to allow formatting of width etc
autofit(ftab)
tabnumber<-tabnumber+1
```
Table `r tabnumber` shows demographic data for the subset of individuals tested on the online battery only, and the subset who also completed the session with FTCD. It is evident from inspection that there are no systematic differences between the two subgroups. 

Subsample for test-retest study_
```{r test-retest, echo=F}
#need to read in sess1 and sess3 for the test-retest data

sess1 <- read.csv(paste0(mydir,'/02-data/02.1_gorilla/sess1.csv'))
sess3 <- read.csv(paste0(mydir,'/02-data/02.1_gorilla/sess3.csv'))
mtab<-table(sess3$male,sess3$Rhanded)

# Calculate delay between session 1 and session 3 for retest participants
for (i in 1:length(sess3$ID)){
  sess3$date_S1 <- sess1$date[which(sess1$ID == sess3$ID)]
}

# Reformat dates
sess3$date <- as.Date(sess3$date, format='%d/%m/%Y')
sess3$date_S1 <- as.Date(sess3$date_S1, format='%d/%m/%Y')
sess3$date_diff <- difftime(sess3$date, sess3$date_S1)

date_diff_stats <- fivenum(sess3$date_diff)

```

A subsample of `r nrow(sess3)` participants was retested on part of the online battery within `r ceiling(as.numeric(date_diff_stats[5]) / 7)` weeks of the first session (median = `r date_diff_stats[3]` days, range = `r date_diff_stats[1]` to `r date_diff_stats[5]` days). There were `r mtab[1,1]` left-handed females, `r mtab[2,1]` left-handed males, `r mtab[1,2]` right-handed females, and `r mtab[2,2]` right-handed males. The retest session included the Rhyme Decision and Word Comprehension tasks, which were new, but not Dichotic Listening, for which we had adequate evidence of test-retest reliability from the previous study by Parker et al (2021). 


## Online behavioural battery  

### Derivation of laterality indices  

```{r dichotic.plot,echo=F}


mycomb$DLsig <-0
w<-which(abs(mycomb$DL.zlat)>1.96)
mycomb$DLsig[w]<-1


DLdat <- mycomb[mycomb$excludeDL==0,]
DLsides <- ggplot(DLdat, aes(x=DL.L, y=DL.R, color=Handedness,shape=as.factor(DLsig))) +
  xlab("N correct L ear")+
   ylab("N correct R ear")+
  geom_point()+
  ggtitle("Dichotic Listening") +
  scale_shape_manual(name="Significantly lateralised",
                     labels=c("No","Yes"),
                     values=c(4,16))+
  scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
  geom_abline(intercept = 0, slope = 1)
ggsave(paste0(mydir,"/03-graphic-outputs/AFig1_DLsides.png"),width = 6, height = 4)

fignumber<-fignumber+1

```



```{r RDT,echo=F}
mycomb$RDTsig <-0
w<-which(abs(mycomb$RDT.zlat)>1.96)
mycomb$RDTsig[w]<-1


RDTdat <- mycomb[mycomb$excludeRDT==0,]
RDTsides <- ggplot(RDTdat, aes(x=RDT.Lmean, y=RDT.Rmean, color=Handedness,shape=as.factor(RDTsig))) +
  xlab("Mean correct RT (ms) left VHF")+
   ylab("Mean correct RT (ms) right VHF")+
  geom_point()+
  ggtitle("Rhyme Decision") +
  scale_shape_manual(name="Significantly lateralised",
                     labels=c("No","Yes"),
                     values=c(4,16))+
  scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
  geom_abline(intercept = 0, slope = 1)
ggsave(paste0(mydir,"/03-graphic-outputs/AFig2_RDTsides.png"),width = 6, height = 4)


```


```{r WC,echo=F}
mycomb$WCsig <-0
w<-which(abs(mycomb$WC.zlat)>1.96)
mycomb$WCsig[w]<-1


WCdat <- mycomb[mycomb$excludeWC==0,]
WCsides <- ggplot(WCdat, aes(x=WC.Lmean, y=WC.Rmean, color=Handedness,shape=as.factor(WCsig))) +
  xlab("Mean correct RT (ms) left VHF")+
   ylab("Mean correct RT (ms) right VHF")+
  geom_point()+
  ggtitle("Word Comprehension") +
  scale_shape_manual(name="Significantly lateralised",
                     labels=c("No","Yes"),
                     values=c(4,16))+
  scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
  geom_abline(intercept = 0, slope = 1)
ggsave(paste0(mydir,"/03-graphic-outputs/AFig3_WCsides.png"),width = 6, height = 4)

#plot with all 3 L vs R scatterplots together in a row.
allplot <- ggarrange(DLsides, RDTsides, WCsides, ncol = 3, nrow = 1,common.legend=TRUE)
ggsave(paste0(mydir,"/03-graphic-outputs/allLRbeh.png"),width = 9, height = 3)

```
Visualisations of the raw data from which laterality indices were computed, colour-coded by handedness, are shown for the three behavioural task in Figure `r fignumber`. The point type indicates whether the absolute individual z-lat score was greater than 1.96, indicating reliable lateralisation for that individual. Note that on the Dichotic Listening task there is inevitably a negative correlation between these totals, because on each trial the response is either left or right: hence, the more responses are 'left' the fewer are 'right', and vice versa. The black line shows the point of equality, where the number correct is the same for left and right. For this task, the points are distinguished for those where the proportion on one side is significantly different from .5 (filled circles), versus those who are equally likely to respond to L or R (crosses, which are bunched around the black line). It is evident from this plot that a high proportion of participants are significantly lateralised, and that overall the sample shows a right ear advantage, i.e. there are more points above the black line than below it. 

In our pre-registration we stated that we would compute a conventional laterality index, 100* (Left-Right)/(Left+Right), as well as a laterality z-score:
(pL-.5)/sqrt(pL*pR/n).  We made one small modification, which was to flip the sign of these quotients, so that for all our laterality measures, left-hemisphere superiority is reflected in a positive score. This has no material effect on any computations, but gives better consistency with other research. For the laterality z-score, scores were censored at +/- 10, to avoid undue influence from a handful of extreme scores (participants who responded overwhelmingly to one ear).  


As is evident from Supplementary figure `r suppfignumber`, the z-LI score is very highly correlated with the conventional LI, the principal difference being that the z-LI follows the normal distribution, with a sigmoid shape at the extremes (truncated in the Figure because of the censored scale). For subsequent analyses, we use z-LI, as this allows us to compare different tasks on a common scale. 

In Rhyme Decision, the task was to judge which of two pictured items had a name that rhymed with a centrally-presented written word. The pictures were presented in the left or right periphery. This was an easy task, as the words used were common, and the participants were given a practice session where they were introduced to all the pictures and their names. We could therefore use accuracy as an index of engagement with the task, and we excluded `r length(intersect(which(combdat$excludeRDT==1),which(combdat$lang2exclude==0)))` participants with accuracy of less than 75% correct. 

The dependent variable of interest was Response Time (RT), which was computed for correct responses in each half-field, after excluding outliers using a participant-specific algorithm. This involved taking the complete set of correct RTs for a participant, and applying the Hoaglin-Iglewicz (1987) criterion of outlier detection with cutoff set to 1.65 to remove unusually long RTs. In addition, any RTs less than 200 ms were excluded. A z-LI was then computed by computing a t-test for each participant, to compare the mean correct RT to left- and right-sided stimuli. Absolute values of z-LI greater than 1.96 can be regarded as evidence that an individual is significantly faster to one side than the other. Because RT is scaled so that high values correspond to poor performance, the z-LI was computed so that it was positive when left-sided RT was greater than right-sided RT. Thus those with a left-hemisphere advantage should cluster below the black line.


As with Rhyme Decision, Word Comprehension involved responding to laterally presented visual stimuli. The stimuli are pictures of semantically-related words, and the task is simply to respond to the picture whose name is spoken. Again, this is an easy task, where the focus is on RT of correct responses. Participants who made more than 75% errors on the task (N = `r length(intersect(which(combdat$excludeWC==1),which(combdat$lang2exclude==0)))`) were excluded from analysis.  The same method as for Rhyme Decision was used to remove outlier RTs participant by participant before computing a z-LI based on comparison of mean RT to left and right sides, where a positive value indicated faster RTs to stimuli presented in the right visual field.  


```{r DLdensities,echo=F,include=F}


plot1 <- ggplot(DLdat, aes(x = DL.LI, y = DL.zlat)) + 
  geom_point(shape = 4,  size = 1)+
  ggtitle("Dichotic Listening") 

  

dens1 <- ggplot(DLdat, aes(x = DL.LI, fill = Handedness)) + 
  geom_density(alpha = 0.4) + 
  geom_vline(xintercept = 0, linetype="dotted")+
  theme_void() + 
  theme(legend.position = "none")

dens2 <- ggplot(DLdat, aes(x = DL.zlat, fill = Handedness)) + 
  geom_density(alpha = 0.4) + 
  geom_vline(xintercept = 0, linetype="dotted")+
  theme_void() + 
  theme(legend.position = "none") + 
  coord_flip()

dens1 + plot_spacer() + plot1 + dens2 + 
  plot_layout(ncol = 2, nrow = 2, widths = c(4, 1), heights = c(1, 4))

DLdens <- ggsave(paste0(mydir,"/03-graphic-outputs/AFig4_DLdens.png"),width = 5, height = 4)
suppfignumber<- suppfignumber+1
```







## Online battery: Distribution of Laterality indices  

Before testing specific predictions about interrelationships between measures, we conducted preliminary analysis on z-LI values for all three online tasks, to test for normality, to check for significant lateralisation in left- and right-handers, to compare laterality between handedness groups, and to compute split-half and test-retest reliability for laterality indices.  


```{r DL-ttests,echo=F, message=F, warning=F}
#Make a table to show characteristics of different tests

onlinesummary <- data.frame(matrix(NA,nrow=12,ncol=4))
colnames(onlinesummary)<-c('Statistic','Dichotic','Rhyme','Comprehension')
onlinesummary[,1]<-c('N','Mean (SD)','Skew','Kurtosis','Shapiro-Wilk normality','Mean (SD) L-hander','Mean (SD) R-hander','one-group t L-hander','one-group t R-hander','R-hander vs L-hander t','Split half r','Test-retest r (N = 53)')
```


```{r fillonlinesummary,echo=F, message=F, warning=F}



mytask<-'DL'
writecolnum <- 2 #column of online summary to write to for this task
onlinesummary <- populate(mycomb,mytask,onlinesummary,writecolnum,'beh')

mytask<-'RDT'
writecolnum <- 3 #column of online summary to write to for this task
onlinesummary <- populate(mycomb,mytask,onlinesummary,writecolnum,'beh')
onlinesummary[12,3]<-round(cor(sess1$RDT.zlat,sess3$RDT.zlat,use='complete.obs'),3)

mytask<-'WC'
writecolnum <- 4 #column of online summary to write to for this task
onlinesummary <- populate(mycomb,mytask,onlinesummary,writecolnum,'beh')
onlinesummary[12,4]<-round(cor(sess1$WC.zlat,sess3$WC.zlat,use='complete.obs'),3)


ft<-flextable(onlinesummary)
ft<-autofit(ft)
ft
tabnumber <- tabnumber+1
```

```{r behpirates, echo=FALSE, warning=FALSE,message=F}
#NB Formatting of this figure needs to be tweaked to achieve good resolution and legibility, but we can do that when we know what format figures need to be created in. 


 mypath<-paste0(mydir,"/03-graphic-outputs")
  plotname<-paste0(mypath,'/beh_pirates.jpg')
jpeg(plotname, width = 800, height = 500)

bLIdat <- combdat %>% 
  dplyr::select(ID, Rhanded, DL.zlat,RDT.zlat,WC.zlat)
colnames(bLIdat) <- c('ID','Handed','Dichotic','Rhyme','Comprehension')

bLIdat$Handed <- as.factor(bLIdat$Handed)
levels(bLIdat$Handed)<-c("L","R")
longdata.b <- pivot_longer(data = bLIdat, cols = c(3:5), names_to = 'Task', values_to = 'LI')
longdata.b$Task<-as.factor(longdata.b$Task)
longdata.b$Task <- factor(longdata.b$Task, levels = c('Dichotic', 'Rhyme', 'Comprehension'))
pirateplot(data = longdata.b, LI ~ Handed * Task)
abline(h=0)

title(main=paste0('Distributions of z-LI \non behavioural tasks; N = ',length(bLIdat$ID)))

fignumber<-fignumber+1

```


![Pirate plot distributions of z-LI scores on online tasks](~/Dropbox/COLA_RR_Analysis/03-graphic-outputs/beh_pirates.jpg)  



As shown in Figure `r fignumber` and Table `r tabnumber`, distributions of z-LI on the three tasks were non-normal, and the three tasks showed very different patterns of laterality. As expected from previous studies, on dichotic listening there was a clear right ear advantage in both left- and right-handers. In addition, there was a small but statistically reliable difference between handedness groups, with stronger laterality in the R handers.  We did not assess test-retest reliability for this task, as we had done this in our previous study with this task and found it to be high. Here we confirm excellent split-half reliability for this task. 



The Rhyme Decision task was far less reliable, with split half reliability of .432 and test-retest reliability of .539.  These figures indicate that laterality on this test is far from being at chance, but there is a great deal of random variation. In addition, although the task showed statistically reliable laterality in both left- and right-handers in this large sample, the effect size was small, and most individuals were not significantly lateralised. Furthermore, there was no effect of handedness on laterality on this task.  

The Word Comprehension task did rather better in terms of reliability, with split half reliability of .664, though test-retest reliability was lower at .555. The striking observation about this task was that it showed a laterality bias in the opposite direction to what is usually seen in language tasks, with faster responses to pictures viewed in the left visual half-field, which projects directly to the right hemisphere. Furthermore, there was a significant effect of handedness, with the laterality index being more negative in left-handers than in right-handers.  


## Testing a two-factor model using behavioural data

Prediction 1 stated: __The pattern of correlation between laterality indices from online measures will reflect the extent to which they involve implicit speech production, rather than whether they involve spoken or written language. Thus we anticipate dissociation between the rhyme judgement task and the other two measures (dichotic listening and OVP task), which is not accountable for in terms of low reliability of measures.__ 

We had planned to do a formal comparison of model fit using AIC weights, but we realised our data were inadequate for this because our Model A was, in formal terms, just-identified: it simply estimated three pairwise correlations from the data, and always gave perfect fit, regardless of the size or direction of correlations. We considered alternative approaches to the analysis, but decided to just report the correlations at this stage, as the pattern of results was distinctive, and we had already planned to incorporate the online behavioural measures into the SEM analysis that includes the fTCD measures (see section x below).  




```{r behavcorrs,echo=F,warning=F,include=F}


bivdat <- filter(mycomb,excludeRDT==0,excludeDL==0,excludeWC==0)
#Assign Group at random
set.seed(50) #make reproducible
bivdat$Group<-1+rbinom(nrow(bivdat),1,.5)
#Check handedness distribution
handgrouptab <- table(bivdat$Handed,bivdat$Group) #This is just to confirm roughly equal distribution of L and R handers in the 2 random groups.


#tempx and tempy are reassigned to variables of interest before calling generic function that will base plot on these two variables
bivdat$tempx <- bivdat$DL.zlat
bivdat$tempy <- bivdat$RDT.zlat
name1 <-"Dichotic Listening z-lat"
name2 <- "Rhyme decision z-lat"
DL_RD_plot <- bivplot2(bivdat,name1,name2) #this is our specially created function
#I've commented out saving the individual plots, just because I've used ggarange to make a composite plot with all 3 pairings
#ggsave(paste0(mydir,"/03-graphic-outputs/DL-RDT.png"),width = 6, height = 4)

bivdat$tempy <- bivdat$WC.zlat
name2 <- "Word Comprehension z-lat"
DL_WC_plot <- bivplot2(bivdat,name1,name2)
#ggsave(paste0(mydir,"/03-graphic-outputs/DL-WC.png"),width = 6, height = 4)

bivdat$tempx <- bivdat$RDT.zlat
name1 <- "Rhyme Decision z-lat"
RD_WC_plot <- bivplot2(bivdat,name1,name2)
#ggsave(paste0(mydir,"/03-graphic-outputs/RD-WC.png"),width = 6, height = 4)

#For now am making a plot with all 3 scatterplots together in a row. Easy to change layout if needed.
allplot <- ggarrange(DL_RD_plot, DL_WC_plot, RD_WC_plot, ncol = 3, nrow = 1,common.legend=TRUE)
ggsave(paste0(mydir,"/03-graphic-outputs/allbiv.png"),width = 9, height = 3)
fignumber<-fignumber+1
```
![Bivariate distributions of LIs on behavioural tasks](~/Dropbox/COLA_RR_Analysis/03-graphic-outputs/allbiv.png) 

Figure `r fignumber` shows scatterplots of the bivariate relationships between the three variables. It is evident from inspection that we can reject model C, in which all three LIs are independent, and model B1, where only Dichotic Listening and Rhyme Decision are correlated. The strongest correlation is between the two visual tasks, Rhyme Decision and Word Comprehension, as predicted by model B2. 

Note that correlations will be influenced by test reliability. Indeed, the correlation between Rhyme Decision and Word Comprehension is close in magnitude to the split half reliability of the two measures. An estimate of the association between these measures after adjusting for the split-half reliabilities can be obtained using the Spearman-Brown correction for attentuation, r.xy(corrected) = r.xy(observed)/sqrt(r.xx * r.yy), which gives a value of `r round(.44/sqrt(.436*.664),3)`.


## Functional Transcranial Doppler (fTCD) measures

### Data Quality and Outliers for fTCD
``` {r dataqual, echo=FALSE, warning=FALSE}
#As pre-registered, we identify cases who have high values of SE for the LI, as this indicates the measurements are unusually variable from trial to trial
#We compute Qlimit based on Hoaglan-Iglewicz formula and exclude cases that exceed this value.
# Identify outliers
for (t in 1:6){
  SEcol <- which(colnames(combdat) == paste0(LETTERS[t], '_mean_se'))
  Q3<-quantile(combdat[ , SEcol],.75,na.rm=TRUE)
  Q1<-quantile(combdat[ , SEcol],.25,na.rm=TRUE)
  Qlimit<-Q3+2.2*(Q3-Q1)
  
# If there are at least 10 trials, include the data
  excludecol = which(colnames(combdat) == paste0(LETTERS[t], '_exclude'))
  trialscol = which(colnames(combdat) == paste0(LETTERS[t], '_N'))
  combdat[,excludecol] <- NA #initialise with NA
  combdat[which(combdat[ , trialscol] > 9), excludecol] <- 0
  combdat[which(combdat[ , trialscol] < 10), excludecol] <- 1
  
  # If the SE is too high, exclude the datatrials
  combdat[which(combdat[ , SEcol] > Qlimit) , excludecol] <- 1
}

# Count number of missing or excluded datapoints per task. 


# First set to NA those who did not do ftcd
n_excludeLI = matrix(data=NA, nrow=1, ncol=6)
for (t in 1:6){
  excludecol = which(colnames(combdat) == paste0(LETTERS[t], '_exclude'))
  n_excludeLI[t] = length(which(combdat[ , excludecol] > 0)) 
}

# Should any participants be excluded? If a participant has more than one excluded task, the participant is excluded entirely
combdat$DopExclude <- 0
tmp <- combdat$A_exclude + combdat$B_exclude + combdat$C_exclude + combdat$D_exclude + combdat$E_exclude + combdat$F_exclude
combdat$DopExclude[which(tmp > 1)] = 1
n_excluded = length(which(combdat$DopExclude == 1))

ddat<- filter(combdat,DopExclude==0,excluded==0,ftcd==1) #not really necessary to create this, but done for consistency with previous script : this has just those who did ftcd and who were not excluded on demographics or ftcd.

# We now count how many included participants omitted a single task
n_excludeLI = matrix(data=NA, nrow=1, ncol=6)
for (t in 1:6){
  excludecol = which(colnames(ddat) == paste0(LETTERS[t], '_exclude'))
  n_excludeLI[t] = length(which(ddat[ , excludecol] > 0)) 
}

```

We excluded `r n_excluded` participants who met our criteria for outliers on two or more fTCD tasks. For the remaining `r nrow(ddat)` participants, the numbers with missing data on the six tasks (A = Word Generation, B = Sentence Generation, C = Phonological Decision, D = Word Comprehension, E = Sentence Comprehension and F = Syntactic Decision) were `r n_excludeLI[1]`, `r n_excludeLI[2]`, `r n_excludeLI[3]`, `r n_excludeLI[4]`, `r n_excludeLI[5]` and `r n_excludeLI[6]` respectively.


<!--- Need to decide on colour scheme and stick to it-->
```{r selectplots,echo=F, warning=F,message=F}
tasknames <- c('Word generation','Sentence generation','Phonological decision','Word comprehension','Sentence comprehension','Syntactic decision')
ddat$Handed<-as.factor(ddat$Rhanded)
levels(ddat$Handed)<-c("Left","Right")
for (i in 1:length(tasknames)){
  col1<- paste0(LETTERS[i],"_mean_odd")
  col2<- paste0(LETTERS[i],"_mean_even")
  col3 <-paste0(LETTERS[i],"_mean_LI")
  c1<-which(colnames(ddat)==col1)
  c2<-which(colnames(ddat)==col2)
  c3<-which(colnames(ddat)==col3)
  h <- which(colnames(ddat)=='Handed')
  
  myfile <- ddat[,c(h,c1,c2,c3)]
  mycolnames <- c('Odd','Even','All')
  myrange=c(-5,6)
  p2<-doscatterplus(myfile,tasknames[i],mycolnames,myrange)
  p2
   mypath<-paste0(mydir,"/03-graphic-outputs")
  plotname<-paste0(mypath,'/OddEven_',LETTERS[i],'.png')
  ggsave(plotname,p2,width = 5, height = 4)

  
}

```



```{r LIpirates, echo=FALSE, warning=FALSE,message=F}
#NB Formatting of this figure needs to be tweaked to achieve good resolution and legibility, but we can do that when we know what format figures need to be created in. 
#NOW REORDERED IN TERMS OF BIAS TO LEFT HEMISPHERE

#Make task names that will print on 2 lines for compactness
tasknames2 <- c("Sentence\ngeneration","Word\ngeneration","Phonological\ndecision","Sentence\ncomprehension","Syntactic\ndecision","Word\ncomprehension")
#Now make text locations for these on pirate plot: we'll place A-C below plot and D-F above it
horizpts<-rep(1,6)
for (i in 1:6){
  horizpts[i] <- 1+(i-1)*3
}
vertpts <- rep(-5.5,6)
vertpts[4:6]<-6

 mypath<-paste0(mydir,"/03-graphic-outputs")
  plotname<-paste0(mypath,'/ftcd_pirates.jpg')
jpeg(plotname, width = 800, height = 500)

LIdata <- ddat %>% 
  dplyr::select(ID, Rhanded, B_mean_LI,A_mean_LI,  C_mean_LI, E_mean_LI, F_mean_LI, D_mean_LI)
colnames(LIdata) <- c('ID','Handed','1B','2A','3C','4E','5F','6D')
LIdata$Handed <- as.factor(LIdata$Handed)
levels(LIdata$Handed)<-c("L","R")


longdata.d <- pivot_longer(data = LIdata, cols = c(3:8), names_to = 'Task', values_to = 'LI')
longdata.d$Task <- as.factor(longdata.d$Task)
levels(longdata.d$Task)<-c('B','A','C','E','F','D')
pirateplot(data = longdata.d, LI ~ Handed * Task,ylim=c(-6,8))
abline(h=0)

for (i in 1:6){
  text(horizpts[i], vertpts[i], tasknames2[i], #add label for task
     cex = 1)
}
#title(main=paste0('Distributions of LI Data \nN = ',length(LIdata$ID)))

dev.off()
fignumber<-fignumber+1
piratenumber <- fignumber
```
![Distributions of fTCD LIs on six tasks for 104 left-handers and 91 right-handers ](~/Dropbox/COLA_RR_Analysis/03-graphic-outputs/ftcd_pirates.jpg)

## LI Summary Statistics

The pirate plot in Figure `r fignumber` shows LI values for the six tasks (A = Word Generation, B = Sentence Generation, C = Phonological Decision, D = Word Comprehension, E = Sentence Comprehension and F = Syntactic Decision) for left and right handed participants. One sample t-tests were computed to test whether the group LI values differed significantly from zero, i.e. showed significant lateralisation. In left handers, tasks A, B and C were left lateralised; task D was right lateralised; and tasks E and F were not significantly lateralised. In right handers, tasks A, B, C and E were significantly left-lateralised.  Between-group t-tests were also computed to test whether lateralisation differed between left and right handers, using a one-tailed test as we had a directional prediction that R-handers would be more left-lateralised than L-handers. LI values were significantly stronger in the right handers than the left handers for all tasks except Word Comprehension.

Split-half reliability was computed for LI indices by computing the LI based just on odd or even trials, and taking the correlation between the two values. As can be seen in Table `r tabnumber`, values were generally between .73 and .83, except for Word Comprehension, where split-half reliability was only .58. 

```{r doppler-ttests,echo=F}
#Make a table to show characteristics of different tests


ftcdsummary <- data.frame(matrix(NA,nrow=11,ncol=7))
colnames(ftcdsummary)<-c('Statistic',LETTERS[1:6])
ftcdsummary[,1]<-c('N','Mean (SD)','Skew','Kurtosis','Shapiro-Wilk normality','Mean (SD) L-hander','Mean (SD) R-hander','one-group t L-hander','one-group t R-hander','R-hander vs L-hander t','Split half r')
```

```{r fill-ftcdsummary, echo=FALSE, warning=FALSE,message=F}
#We use the same function 'populate' to populate the data frame as we had for behavioural tasks - the 'ftcd' term at end of function call ensures correct columns are found
for (t in 1:6){
mytask<-LETTERS[t]
writecolnum <- 1+t #column of online summary to write to for this task

ftcdsummary <- populate(ddat,mytask,ftcdsummary,writecolnum,'ftcd')
#ftcdsummary <- ftcdsummary[1:(nrow(ftcdsummary)-1),]
}
ft<-flextable(ftcdsummary)
ft <- fontsize(ft, size = 10)
ft<-autofit(ft)
ft
tabnumber<-tabnumber+1
```

Table `r tabnumber` shows basic statistics for the fTCD laterality indices, in the same format as for the online tasks. Right-handers showed significant left-lateralisation on Word Generation, Sentence Generation, Phonological Decision, and Sentence Comprehension, but were not lateralised for Word Comprehension or Syntactic Decision.  Left-handers were significantly left-lateralised for Word Generation, Sentence Generation and Phonological Decision, were not lateralised for Sentence Comprehension or Syntactic Decision, and were significantly right-lateralised for Word Comprehension.  The direct comparison between left- and right-handers showed significantly greater left-lateralisation on all tasks except Word Comprehension, which showed only a trend in that direction.

All tasks had split half reliability coefficients of .73 or above, except for Word Comprehension, where the coefficient was only .58.

Shapiro Wilk tests revealed significant non-normality for Sentence Generation, Phonological Decision and Sentence Comprehension, though values of skewness and kurtosis were generally not extreme.

As noted above, there were a few participants with missing data on a single measure. Before running the SEM analysis, the _mice_ package @vanbuuren2011 was run in R to impute these missing values. 


```{r imputemissing,echo=F, include=F}
nunames<- c('A_P1','B_P2','C_P3','D_R1','E_R2','F_R3') #cols for SEM; short names useful here. These cols with have imputed values

thisdat <- ddat[,c('A_mean_LI','B_mean_LI','C_mean_LI','D_mean_LI','E_mean_LI','F_mean_LI')]

#Interpolate missing values using mice package
thisdat.i <- mice(thisdat, m=1, maxit = 50, method = 'pmm', seed = 500,print=FALSE)
ddati <-cbind(ddat[,c('ID','male','Handed')],complete(thisdat.i,1))
colnames(ddati)<-c('ID','male','Handed',nunames)

```






## Hypothesis 2: Testing a two-factor model using fTCD data

Our second preregistered prediction was: __The data will fit a model where 'language generation' tasks cluster together on one factor, and 'receptive language’ tasks on a second factor.__   It was further predicted that factors will be correlated, but the fit of a 2-factor model will be superior to a single-factor model where all LIs load on a common factor.
 
The analysis conducted by Woodhead et al (2019, 2020) used an exploratory bifactor model in which each task could load on each of two factors. Because there were two measures for each task (from test and retest sessions), this exploratory approach was adequately powered. In the current study, some of the tasks were different, and we only had one measurement occasion for each of the six measures. Accordingly we used confirmatory factor analysis, using a prespecified two-factor model that constrains which indicators can load on two factors. This was compared to a unitary model, in which all tasks load on a single factor.  

`r fignumber <- fignumber+1`Figure `r fignumber` shows the pattern of correlations between LIs for the different tasks as a heat map. The two-factor model predicts that correlations will form two clusters, with positive correlations within tests A-C, and within tests D-F, but weaker or absent correlations across these two clusters of measures. The heatmap shows moderate correlations within both clusters of measures, and generally lower correlations across clusters, but there are some exceptions. Notably, there is a moderate correlation between task C (Phonological Decision) and task E (Sentence comprehension), which is not predicted by the two-factor model. 


```{r modelfit,echo=F,include=F}

#nb we will use the variables from ddati
set.seed(50)

ddati$randgroup<-1+rbinom(nrow(ddati),1,.5) #create random group 1 or 2 for later split

#Add correlation matrix

LIcols <- c("A_P1","B_P2","C_P3","D_R1","E_R2","F_R3")

myheatmap <- makeheatmap(ddati,LIcols)

#Saved heatmap needs a bit of tweaking! Size of axis labels and grey background need fixing
ggsave(paste0(mydir,"/03-graphic-outputs/ftcd-heatmap.png"),width = 6, height = 6)

```
![Heatmap showing correlations between laterality indices from six fTCD tasks](~/Dropbox/COLA_RR_Analysis/03-graphic-outputs/ftcd-heatmap.png) 

__Note from DB: I am planning on putting in quite a detailed explanation of SEM as it not widely used in this field. This is a bit of a placeholder for now.__

### Summary of the Structural equation modeling approach  
Because Structural equation modeling (SEM) is not widely used in laterality research, we provide here a brief explanation, to aid interpretation of the subsequent analysis.  

Structural equation modeling (Kline, 2011) is a method that allows a formal test of adequacy of competing models for explaining patterns of association between variables. The underlying assumption of this approach is that observed variables can be treated as indicators of underlying, unobserved latent variables. `r fignumber<-fignumber+1`Figure `r fignumber` shows the single factor model on the left, and a two-factor model on the right. The latent factors are shown in ovals, and the observed variables in boxes. Single-headed arrows indicate causal paths, and double-headed arrows indicate variances. Although means can be incorporated in SEM (and we shall be doing this in our analysis), the main use of SEM is to analyse patterns of covariances. The important point to note is that the path diagrams shown in Figure 1x have a precise mathematical interpretation, and can be converted into linear equations that specify the covariances between observed variables. Thus it is possible to obtain a measure of goodness of fit for observed data in relation to a model by comparing whether the observed covariances agree with those predicted by the model. We can already see by inspecting the heatmap of Figure `r fignumber-1` that a single-factor model is unlikely to provide a good fit to the observed data, because it would not predict the clustering of correlations that is evident. 

<!---Diagram could be done better!--->
![One-factor vs two-factor structural equation model](~/Dropbox/COLA_RR_Analysis/03-graphic-outputs/path_diags_asppt.jpg)

SEM does not arrive at a single algebraic estimation of model fit, but rather uses a maximum likelihood approach, whereby values for the paths from the factors to the observed variables are first assigned starting values, and the expected covariances between variables are computed with these values, and then compared to observed covariances. This process is iterated many times with different path estimates, with an algorithm adjusting paths on each run to reduce the mismatch between observed and expected covariances. 

It may be noted that the path diagrams shown in Figure `r fignumber` include one path to each factor shown as a dotted line. This is a fixed parameter, set to 1, which is used to scale the estimates. All the other paths are free to vary, and the estimation process will consider different values, to converge on a solution that gives the best fit. Some paths may have little impact on the solution, and may be dropped without any deterioration of fit. 

There is no single method for evaluating the fit of a model to observed data. A chi square test can given an estimate of the extent of departure of a observed values from expectation: a good model is one where chi square is small and has a high associated p-value, indicating that any difference between expectation and observation is likely to just reflect sampling error. However, it is usually possible to improve fit by including additional paths or factors in a model until good fit is achieved, but this does not mean that the model is better: the goal is rather to obtain a parsimonious and theoretically meaningful model that does not include arbitrary parameters that are specified solely to fit the data. Where models are 'nested', with a more complex model including all the parameters of a base model, then model fit can be compared by subtraction of the chi square and degrees of freedom for the two models; the difference in chi square is then evaluated, and can indicate whether the simpler model gives as good a fit as a complex model with more parameters - in which case the simpler model is preferred.

Other indices have been developed that penalise models with a large number of parameters. The Comparative Fit Index (CFI) measures relative improvement of fit of a model relative to a model that assumes independence of all variables. CFI values of .95 or more are conventionally regarded as indicating acceptable fit. Another measure is the Root Mean Square Error of Approximation (RMSEA) is a 'badness of fit' measure, where a value of zero indicates good fit, and values below .05 are widely regarded as indicating acceptable fit. We report here values for chi square, CFI and RMSEA. 
 


```{r initialisebigsummary,echo=F}
#Initialise a table to show factors loadings and some other stuff (CFA, rmsea) for each model in a column, so we can compare them
bigsummary <- data.frame(matrix(NA,nrow=18,ncol=4))
colnames(bigsummary)<-c('Estimate','Model.1F','Model.2F','Model.2Fn')
bigsummary[,1]<-c('NObs','A -> Fac1','B -> Fac1','C -> Fac1','D -> Fac1','E -> Fac1','F -> Fac1','A -> Fac2','B -> Fac2','C -> Fac2','D -> Fac2','E -> Fac2','F -> Fac2','Fac1~~Fac2','CFI','rmsea','chisq','DF')
```





```{r factormodels,echo=F,include=F, warning=F,message=F}
#In lavaan, we first define the factor model between quotes
#So this step doesn't do anything - just sets up the model to be run later

#This is definition of single factor model we will use here. A_P1 will be index variable with path of 1. 
model.1F <- 'f1 =~  A_P1 + B_P2  + C_P3 +D_R1 + E_R2 + F_R3' 


#2 factor production/reception model
model.2F <- '
f1 =~  A_P1 + B_P2+C_P3
f2 =~ F_R3 + D_R1 + E_R2  #2 factor model: 
#covariance unspecified, which means there is no constraint on covariance

'


fit1 <- cfa(model.1F, estimator="WLSMV",data=ddati) #runs the model and saves results in fit1
sfit1 <- makeSEMtab(fit1) #saves the results from the model in a neat format
bigsummary<- addmodel(bigsummary,fit1,NA,'Model.1F',writecol=2) #summary from this model written to col 2 of bigsummary. For single factor model we don't specify a comparison model, hence NA.

#lavResiduals(fit1) #if we want to understand reasons for poor fit, we can look at residuals - shows size of covariances that aren't explained by model.

#Creates a structural diagram for single factor model: nb does NOT include path estimates (these can be shown if we put 'par' rather than 'diagram', but it gets messy, and the parameters are shown instead in a table)
#lots of details of this here
#https://www.rdocumentation.org/packages/semPlot/versions/1.1.2/topics/semPaths

pathfigname<-paste0(mydir,"/03-graphic-outputs/pathfigF1")
semPaths(fit1, "diagram", weighted = FALSE,  shapeMan = "rectangle", sizeMan = 8, 
    sizeMan2 = 5,filetype='jpg',filename=pathfigname,width=3,height=2) #draws a path diagram

fit2 <- cfa(model.2F, estimator="WLSMV",data=ddati)
sfit2 <- makeSEMtab(fit2)
bigsummary<- addmodel(bigsummary,fit2,fit1,'Model.2F',writecol=3) #summary from this model written to col 3 of bigsummary

#lavResiduals(fit2)
pathfigname<-paste0(mydir,"/03-graphic-outputs/pathfigF2")
semPaths(fit2, "diagram", weighted = FALSE,  shapeMan = "rectangle", sizeMan = 8, 
    sizeMan2 = 5,filetype='jpg',filename=pathfigname,width=3,height=2) #draws a path diagram
#anova(fit1,fit2) #compares model fit -if significant, means 2nd model is better fit than 1st. This information is now included in bigsummary.


#I checked whether the one-factor model would fit better if we specified a 2-group solution with handedness groups; it did not
#measurementInvariance(model=model.1F,estimator="WLSMV",data=ddati,group="Handed")
#CFI very low.

#Also true with the 2F model: fit is better but still well below acceptable on CFI and RMSEA/ Also gives negative variance estimates.
#measurementInvariance(model=model.2F,estimator="WLSMV",data=ddati,group="Handed")

```


```{r newmodel,echo=F,include=F}


#make oddeven file so can try same model as Woodhead et al
wantcols <- c("ID","male","Rhanded","A_mean_odd","A_mean_even","B_mean_odd","B_mean_even","C_mean_odd","C_mean_even","D_mean_odd","D_mean_even","E_mean_odd","E_mean_even","F_mean_odd","F_mean_even","DopExclude")
ddat_OE <- combdat[combdat$ftcd==1,wantcols]
ddat_OE<-ddat_OE[ddat_OE$DopExclude==0,]

#impute missing values using mice package
nunames<-wantcols[4:15]
thisdat.j <- mice(ddat_OE[,nunames], m=1, maxit = 50, method = 'pmm', seed = 500,print=FALSE)
ddat_OE <-cbind(ddat_OE[,1:3],complete(thisdat.j,1))

colnames(ddat_OE)<-c("ID","male","Rhanded","A_o","A_e","B_o","B_e","C_o","C_e","D_o","D_e","E_o","E_e","F_o","F_e")
set.seed(50)

#We explore for best model using just half the data (randgroup = 1)

ddat_OE$randgroup<-1+rbinom(nrow(ddat_OE),1,.5) #create random group 1 or 2 for later split
mygroup <-1
mydat<- ddat_OE[ddat_OE$randgroup==mygroup,]

#Model that is same as Woodhead et al
model.2FZW <- '
f1 =~  1*B_o+equal("f1=~B_o")*B_e+  #same path value for B_o and B_e
       a*A_o+a*A_e +
       c*C_o+c*C_e+
       d*D_o+d*D_e+
       e*E_o+e*E_e+
       f*F_o+f*F_e
f2 =~  NA*D_o+equal("f2=~D_o")*D_e+ #same path value for D_o and D_e
       e2*E_o+e2*E_e+
       f2*F_o+f2*F_e+
       a2*A_o+a2*A_e +
       c2*C_o+c2*C_e  #only B is omitted from f2

f2~~1*f2
A_o~~av*A_o #equate variances for odds and evens
A_e~~av*A_e
B_o~~bv*B_o
B_e~~bv*B_e
C_o~~cv*C_o
C_e~~cv*C_e
D_o~~dv*D_o
D_e~~dv*D_e
E_o~~ev*E_o
E_e~~ev*E_e
F_o~~fv*F_o
F_e~~fv*F_e
'

fit.2FZW.1 <- cfa(model.2FZW,estimator="WLSMV", data=mydat)
wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2FZW.1,wantfits)
#This gives matrix not positive definite

#Paths suggest dropping nonsig paths from f1 to D and F 
# and A from F2 (B is already missing as was excluded from model)
model.2FZWa <- '
f1 =~  NA*B_o+equal("f1=~B_o")*B_e+
       a*A_o+a*A_e +
       c*C_o+c*C_e+
       e*E_o+e*E_e
f2 =~  NA*D_o+equal("f2=~D_o")*D_e+ 
       c*C_o+c*C_e+
       e2*E_o+e2*E_e+
       f2*F_o+f2*F_e
f1~~1*f1
f2~~1*f2
A_o~~av*A_o #equate variances for odds and evens
A_e~~av*A_e
B_o~~bv*B_o
B_e~~bv*B_e
C_o~~cv*C_o
C_e~~cv*C_e
D_o~~dv*D_o
D_e~~dv*D_e
E_o~~ev*E_o
E_e~~ev*E_e
F_o~~fv*F_o
F_e~~fv*F_e
'

fit.2FZWa.1 <- cfa(model.2FZWa,estimator="WLSMV", data=mydat)

wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2FZWa.1,wantfits)
#Converges nicely


#Now test model with hold-out group 2
mygroup <-2
mydat<- ddat_OE[ddat_OE$randgroup==mygroup,]
fit.2FZWa.2 <- cfa(model.2FZWa,estimator="WLSMV", data=mydat)

wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2FZWa.2,wantfits)
```


```{r new2factor-with-all,echo=F}


mydat<- ddati #back to full LI scores and full group
model.2Fn <- '
f1 =~  B_P2+A_P1 +C_P3+ E_R2
f2 =~  NA*F_R3 + C_P3+D_R1+ E_R2   #2 factor model:

f2~~1*f2
'

fit.2F<-cfa(model.2F, estimator="WLSMV",data=mydat)
fit.2Fn <- cfa(model.2Fn, estimator="WLSMV",data=mydat)
tab.2Fn <- makeSEMtab(fit.2Fn)
fitmeasures(fit.2Fn,wantfits)

bigsummary<- addmodel(bigsummary,fit.2Fn ,fit.2F,'Model.2Fn',writecol=4) #summary from this model written to col 4 of bigsummary 
anova(fit.2Fn,fit.2F)



pathfigname<-paste0(mydir,"/03-graphic-outputs/pathfigF2_revised")
semPaths(fit.2Fn, "diagram", weighted = FALSE,  shapeMan = "rectangle", sizeMan = 8, 
    sizeMan2 = 5,filetype='jpg',filename=pathfigname,width=3,height=2) #draws a path diagram

#look at residuals for diagnostics
res<-resid(fit.2Fn, type = "cor") #large abs values (> .1) indicate relationships not well accounted for


#Can't get 2nd col to reformat - need to convert to character - then OK
#bigsummary[,2]<-as.character(bigsummary[,2])
flextable(bigsummary)
tabnumber<-tabnumber+1
fignumber<-fignumber+1


```



We used the lavaan() package to perform the pre-registered model comparison. To take into account non-normality of some variables, the WLSMV estimator was specified; this uses weighted least squares with robust standard errors and a mean- and variance adjusted test statistic, and makes no distributional assumptions about the observed variables. Table `r tabnumber` summarises the main output of the model-fitting. The fit of both the one-factor and the two-factor model is poor. 

Therefore, as planned we divided the sample into two random subsamples, 1 and 2. The first subsample was used in an exploratory analysis, based on that used by Woodhead et al (2021). Fuller details of the analysis are given in Supplementary material.  Because we had data from a single session, we used the LIs from the odd and even trials to give two indicators per task. We started with a model with two factors, where all 12 measures (2 measures from 6 tasks) were allowed to load on both factors, except for Sentence Generation. This was an indicator variable with a loading of 1 on Factor 1, and no loading on Factor 2. To ensure model identification, the variance of Factor 1 was free to vary, and variance of Factor 2 was set to 1. Although this model converged with good fit, there were warnings indicating problems with unfeasibly small eigenvalues. However, when non-significant paths were dropped from the model (from Word Comprehension and Syntactic Decision to Factor 1, and from Word Generation to Factor 2), there was good model convergence with plausible parameters and excellent fit (CFI = 1 and RMSEA = 0). This same model was then evaluated with the hold-out sample, and again the fit was excellent. We therefore took this model forward to the next stage of analysis, first checking the fit with the original full sample and with the original LIs based on all trials. The path diagram is shown in Figure `r fignumber` and summary output is shown in Table `r tabnumber`;  the fit was a significant improvement on the fit of the original 2-factor model. 

![Factor structure of best-fitting model.](~/Dropbox/COLA_RR_Analysis/03-graphic-outputs/pathfigF2_revised.jpg)

```{r makefacscatter,echo=F}
myscatter <- function(myx,myy,xlabel,ylabel,thisdat){
thisscatter<-ggplot(thisdat, aes(x = myx, y = myy,color=Handed)) + 
  geom_point(shape = 4,  size = 1)+
 scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
    xlab(xlabel)+
    ylab(ylabel)+
  geom_hline(yintercept=0, linetype="dashed")+
    geom_vline(xintercept=0, linetype="dashed")
 #NB scale of factors is NOT centred on LI of zero!
  return(thisscatter)
}

```


```{r plotfacs, echo=F}
#Creates a plot and saves it; also adds factor scores to ddati
ddati <- makefactorplot(cfa(model.2Fn, estimator="WLSMV",data=ddati),'fit.2Fn',ddati)


```




## Model equivalency for left- and right-handers  
Our third prediction was: _better model fit will be obtained when different parameters are estimated for left- vs right handers, compared with when all parameters are equated for the two handedness groups._ 


<!---Level of measurement equivalency are assessed through model fit of a series of nested multiple group models.  
Substantial decrease in goodness of fit indicates non-invariance
Xu: It is a good practice to look at several model fit indices rather than relying on a single one
• Δχ2
• ΔRMSEA
• ΔCFI
• ΔTLI
• ΔBIC
• ΔAIC  


Step 1: Configural invariance
  Same factor structure in each group
  First, fit model separately in each group
  Second, fit model in multiple group but let all parameters vary freely in each group
  No latent mean difference is estimated
  
  
Step 2: Weak/metric invariance
  Constrain factor loadings equal across groups
  This shows that the construct has the same meaning across groups
  No latent mean difference is estimated
  
Step 3: Strong/scalar invariance
  Constrain item intercepts equal across groups
  Constrain factor loadings
  This is important for assessing mean difference of the latent variable across groups
  Latent mean difference is estimated
  
Step 4: Strict invariance
  Constrain item residual variances to be equal across groups
  Constrain item factor loadings and intercepts equal across groups. 
  Strict invariance is important for group comparisons based on the sum of observed item scores, because observed variance is a combination of true score variance and residual variance
  Latent mean difference is estimated --->
  


```{r measurementinvariance,echo=F,include=F}
#https://towardsdatascience.com/measurement-invariance-definition-and-example-in-r-15b4efcab351


#library(semTools) fits increasingly restrictive models in one command

#same model in different syntax to make it easier to interpret

model.2Fn <- '
f1 =~  A_P1 + B_P2+C_P3+ E_R2
f2 =~  F_R3 + C_P3+D_R1+ E_R2   #2 factor model: no constraint on covariance
'


#measurementInvariance(model=model.2Fn,estimator="WLSMV",data=ddati,group="Handed")
#This gives message to say command is deprecated.
#It gives substantial change to model fit for the final fit.means model.


#NB makeSEMtab2 is a function defined at top of script - just makes it easier to compare parameters across groups

fitM0 <- cfa(model.2Fn,estimator="WLSMV", data = ddati,meanstructure=T)
tabM0 <- makeSEMtab(fitM0)  #

# configural invariance
fitM1 <- cfa(model.2Fn, estimator="WLSMV",data = ddati,meanstructure=T, group = "Handed")

tabM1 <- makeSEMtab2(fitM1,c('_L','_R'))  #in tab1, everything varies for the 2 groups

# weak invariance
fitM2 <- cfa(model.2Fn, ddati, estimator="WLSMV",meanstructure=T,group = "Handed",
            group.equal = "loadings")
tabM2 <- makeSEMtab2(fitM2,c('_L','_R'))
#in tab2, the factor loadings are same for the 2 groups, but everything else can vary


# strong invariance
fitM3 <- cfa(model.2Fn, ddati, estimator="WLSMV",meanstructure=T,group = "Handed",
            group.equal = c("intercepts", "loadings"))
tabM3 <- makeSEMtab2(fitM3,c('_L','_R'))
#THis does have same intercepts and loadings, but the correlation between f1/f2 and the variances of each factor can differ, as well as the residuals.

#strong invariance with covariance between factors too
fitM3a <- cfa(model.2Fn, ddati,estimator="WLSMV",meanstructure=T, group = "Handed",
            group.equal = c("intercepts","loadings","lv.covariances"))
tabM3a <- makeSEMtab2(fitM3a,c('_L','_R'))
# This is not included in usual hierarchy of models but seems relevant for our hypothesis, which assumes the two factors have equivalent correlation in L and R handers?. but including this at step 2 gives error re standardized measures

fitM4 <- cfa(model.2Fn, ddati, estimator="WLSMV",meanstructure=T,group = "Handed",
 group.equal = c("loadings","intercepts","means","lv.covariances"))

tabM4 <- makeSEMtab2(fitM4,c('_L','_R'))

# model comparison tests
mylavtest<-lavTestLRT(fitM1, fitM2, fitM3,fitM3a, fitM4)
ft<-flextable(mylavtest)
ft
#see https://users.ugent.be/~yrosseel/lavaan/multiplegroup6Dec2012.pdf
tabnumber<-tabnumber+1

```
 

The approach we adopted is a standard one used when structural equation modeling (SEM) is applied to evaluation of measurement models in other domains, where it is described as a test of measurement invariance. Essentially, the data from left- and right-handers are analysed together in a series of nested models; these pose increasingly stringent constraints on which parameters of the model are allowed to vary. Initially, a model is fit in which all the paths, covariances, and intercepts are free to differ between left- and right-handers. This model is tested against a model of 'metric invariance', which sets the loadings from each observed variable to the factors to be the same for the two groups. If the fit of the model does not worsen, we can assume the basic model structure is equivalent for the two groups. This test of equivalence was passed (see Table `r tabnumber`).

At the next step, (scalar invariance), the item intercepts are set to be the same across groups. Once again, the model fit did not worsen (see Table `r tabnumber`).

In a final step (strict invariance), we constrain item residual variances as well as factor loadings and intercepts to be equal across groups. Here we obtained a substantial worsening of model fit, indicating that the mean difference between groups on the latent factors is not the same. 

In sum, results from the measurement invariance test show that the same underlying structural model can be assumed to apply for both left- and right-handedness, with the differences between handedness groups being explained solely in terms of differences in means, rather than in covariances between the six LIs. 

## Categorical analysis of LIs
Prediction 4 was: _On categorical analysis, individuals who depart from left-brained laterality on one or more tasks will be more likely to be left-handed than those who are consistently left-lateralised._

The analysis so far has treated laterality as a continuum, but this continuum does have a zero-point, and negative scores indicate right-lateralisation and positive scores left-lateralisation. There are theoretical reasons to suppose that brain function might be influenced more by consistency in direction of lateralisation, than by degree. Thus, regardless of how strong or weak a laterality index is, brain functioning might be more efficient if all language functions are represented in the same hemisphere. 

As stated in our preregistration, we first adopted the simple approach of dichotomising laterality at a cutoff of zero for each task, and then performed a chi square analysis to test for association with handedness. For 6 measures, we adopted a Bonferroni-corrected alpha level of .02/6 = .003. 

```{r categorical-assignment, echo=F}
mycols<-paste0(LETTERS[1:6],'_mean_LI')
for (i in 1:6){
  c<-which(colnames(ddat)==mycols[i])
  myvector<-rep(NA, nrow(ddat))
  w<-which(ddat[,c]>0.000000001)
  myvector[w]<-1
  w<-which(ddat[,c]<0)
  myvector[w]<-0
  ddat[,(1+ncol(ddat))]<-myvector
  colnames(ddat)[ncol(ddat)] <- paste0(LETTERS[i],'_catlatL')
}
```

```{r category-analysis,echo=F}
chidf <- data.frame(matrix(NA,nrow=6,ncol=5))
colnames(chidf) <- c('Task','pL_Lhander','pL_Rhander','chisq','p')
nucols <- paste0(LETTERS[1:6],'_catlatL')
  myL <- filter(ddat,Handed=='Left')
  myR <- filter(ddat,Handed=='Right')
for (i in 1:6){
    c<-which(colnames(ddat)==nucols[i])
 
  myt<-table(ddat$Handed,ddat[,c])
  chitab<-chisq.test(myt)
  chidf[i,1]<-LETTERS[i]
  chidf[i,2]<-round(sum(myL[,c],na.rm=T)/nrow(myL),3)
  chidf[i,3]<-round(sum(myR[,c],na.rm=T)/nrow(myR),3)
  chidf[i,4]<-round(chitab$statistic,2)
  chidf[i,5]<-pformat2(chitab$p.value)

}
  ft<-flextable(chidf)
  autofit(ft)
  
  chidf$diffp<-chidf$pL_Rhander-chidf$pL_Lhander
  tabnumber<-tabnumber+1
```
Results are shown in Table `r tabnumber`.  The trend is similar for all six tasks, with the proportion who are left-lateralised averaging at 16% lower in left-handers than in right-handers, regardless of the mean LI for the task. The difference ranged from 11% for Syntactic Decision to 24% for Syntactic Comprehension, but met our prespecified significance criterion only for Syntactic Comprehension.

After testing associations for individual measures, we categorised individuals as either consistently left-lateralised on all tests, or right-lateralised on one or more tests. The proportions of left- and right-handers who are left-lateralised on between 0 and 6 tests is shown in table `tabnumber`. 

__Table `tabnumber`: Proportions of left- and right-handers with between 0 and 6 tasks left-lateralised on fTCD.__    




```{r ntestL,echo=F}
ddat$Nleft <- ddat$A_catlatL+ ddat$B_catlatL+ddat$C_catlatL+ddat$D_catlatL+ddat$E_catlatL+ddat$F_catlatL

tabN <- table(ddat$Nleft,ddat$Handed)
tabNp<-round(prop.table(tabN,2),3)
ntestL <- as.data.frame(cbind(0:6,tabNp[,1],tabNp[,2])) #N tests that are L lateralised
colnames(ntestL)<-c('N tasks L lateralised','L-handers','R-handers')

ft<-autofit(flextable(ntestL))
ft
tabnumber<-tabnumber+1

chitab<-matrix(c(sum(tabN[1:6,1]),sum(tabN[1:6,2]),tabN[7,1],tabN[7,2]),nrow=2)
mychi<-chisq.test(chitab)



#Some exploratory analyses folllow
#repeating just for factor 1
ddat$NleftABCE <- ddat$A_catlatL+ ddat$B_catlatL+ddat$C_catlatL+ddat$E_catlatL

tabN4 <- table(ddat$NleftABCE,ddat$Handed)

ntestL_F1 <- round(prop.table(tabN4,2),3)
chitab4<-matrix(c(sum(tabN4[1:4,1]),sum(tabN4[1:4,2]),tabN4[5,1],tabN4[5,2]),nrow=2)
mychi4<-chisq.test(chitab4)



#repeating with L handers subdivided into extreme or moderate 
ddat$hand3cat <- ddat$Rhanded
ddat$hand3cat[ddat$EHI.LI<(-90)]<--1

tabN4a <- table(ddat$NleftABCE,ddat$hand3cat)
ntestL_F1_LHs<-round(prop.table(tabN4a,2),3)



```
It is evident from Table `r tabnumber` that a minority of individuals are consistently left-lateralised on all six tasks, regardless of handedness. Although the trend is for more right-handers to show this pattern than left-handers, the difference is not significant on chi square test, chisquare = `r round(mychi$statistic,2)`, `r pformat(mychi$p.value)`.  However, two of the tests included in this analysis, Word Comprehension and Syntactic Decision, were not left-lateralised at the population level, and it could be argued they would just add noise to the analysis, which was intended to identify those who departed from the typical pattern of left-lateralisation. We therefore added an exploratory analysis, in which we excluded these two tests. When only Word Production, Sentence Production, Phonological Decision and Syntactic Comprehension were considered, `r round(100*tabN4[5,1]/sum(tabN4[1:5,1]),1)`% of left-handers and `r round(100*tabN4[5,2]/sum(tabN4[1:5,2]),1)`% of right-handers were consistently left-lateralised. 

It is noteworthy that this more categorical analysis finds rates of "atypical", i.e. non-left, lateralisation on language tasks that are on the one hand task-dependent, but on the other hand lower than typically observed when methods such as Wada test or fMRI are used. This is the case even for the most lateralised task, Sentence Generation, where 91% of right-handers vs 77% of left-handers were left-lateralised. The implications of this observation will be considered further in the Discussion.

## Relationship between fTCD and behavioural laterality indices

Our fifth prediction was: _the laterality profile obtained with the online language battery will be significantly associated with the profile seen with the direct measurement of cerebral blood flow using fTCD, with laterality on dichotic listening and word comprehension relating more strongly to receptive language tasks, and rhyme decision to language generation tasks._


A preliminary inspection of correlations between online and fTCD laterality indices showed very little relationship between the two, even for the two measures, Word Comprehension and Rhyme Decision, that have analogues in fTCD (tasks D and C respectively).


```{r preparedata,echo=F}
ncol<-ncol(ddati)
for (i in 1:nrow(ddati)){
  mysub <- ddati$ID[i]
  w<-which(combdat$ID==mysub)
  ddati$DL_z[i] <- combdat$DL.zlat.ex[w]
   ddati$RDT_z[i] <- combdat$RDT.zlat.ex[w]
   ddati$WC_z[i] <- combdat$WC.zlat.ex[w]
}

#impute missing values using mice package
nunames<-c('DL_z','RDT_z','WC_z')
thisdat.j <- mice(ddati[,nunames], m=1, maxit = 50, method = 'pmm', seed = 500,print=FALSE)
ddati <-cbind(ddati[,1:12],complete(thisdat.j,1))
```



```{r bigheatmap,echo=F}

 #need to ensure online LIs from excluded are not included
mycolsb<-c("DL_z","RDT_z","WC_z",  "A_P1", "B_P2","C_P3", "D_R1","E_R2", "F_R3")
bigheatmap <- makeheatmap(ddati,mycolsb)
bigheatmap

```

We had pre-registered two data checks: 1) Online measures that have split-half reliability below .6 will be excluded from further analysis. 2) Online measures of word comprehension and rhyme decision will only be taken forward to the next stage of analysis if they have a correlation of at least .11 with the counterpart measure from fTCD (word comprehension and phonological decision respectively).

The online Word Comprehension measure failed both checks: split-half reliability was below .6, and the correlation with the fTCD Word Comprehension laterality index was close to zero.  Rhyme Decision narrowly passed both checks: split-half reliability was `r onlinesummary[7,4]` (though test-retest reliability was only`r onlinesummary[8,4]`).  The correlation between the online Rhyme Decision and fTCD Phonological Decision was `r round(cor(ddati$RDT_z,ddati$C_P3,use='complete.obs'),3)`. Accordingly, we proceeded with the next step of analysis only with Dichotic Listening (which had good reliability, but no counterpart in the fTCD battery) and Rhyme Decision.

We had predicted that Dichotic Listening, as a receptive task, should load on the same factor as the Word Comprehension, Sentence Comprehension and Syntactic Decision, but it is evident from the heatmap that, insofar as it correlates with the fTCD tasks, the strongest association is with production tasks. 



```{r newmodelx,echo=F,include=F}

model.2Fnbase <- '
f1 =~  A_P1+B_P2+C_P3+E_R2+0*RDT_z
f2 =~  F_R3+D_R1+E_R2+C_P3+0*+DL_z
'
fit.2Fnbase <- cfa(model.2Fnbase,estimator="WLSMV", data=ddati)
summary(fit.2Fnbase)
wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2Fn,wantfits)

model.2Fnx <- '
f1 =~  A_P1+B_P2+C_P3+E_R2+RDT_z
f2 =~  F_R3+D_R1+E_R2+C_P3+DL_z
'
fit.2Fnx <- cfa(model.2Fnx,estimator="WLSMV", data=ddati)
summary(fit.2Fnx)
wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2Fnx,wantfits)
measurementInvariance(model=model.2Fnx,estimator="WLSMV",meanstructure=T,data=ddati,group="Handed")
anova(fit.2Fnbase,fit.2Fnx)

model.2Fny <- '
f1 =~  A_P1+B_P2+C_P3+E_R2+RDT_z+DL_z
f2 =~  F_R3+D_R1+E_R2+C_P3
'
fit.2Fny <- cfa(model.2Fny,estimator="WLSMV", data=ddati)
summary(fit.2Fny)
wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2Fny,wantfits)
measurementInvariance(model=model.2Fny,estimator="WLSMV",data=ddati,meanstructure=T,group="Handed")
anova(fit.2Fnbase,fit.2Fny)

tab.2Fnbase <- makeSEMtab(fit.2Fnbase)
tab.2Fnx <- makeSEMtab(fit.2Fnx)
tab.2Fny <- makeSEMtab(fit.2Fny)

model.2Fnz <- '
f1 =~  A_P1+B_P2+C_P3+E_R2+DL_z+0*RDT_z
f2 =~  F_R3+D_R1+E_R2+C_P3
'
fit.2Fnz <- cfa(model.2Fnz,estimator="WLSMV", data=ddati)
summary(fit.2Fnz)
wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2Fnz,wantfits)
measurementInvariance(model=model.2Fnz,estimator="WLSMV",data=ddati,meanstructure=T,group="Handed")
anova(fit.2Fnbase,fit.2Fnz)


pathfigname<-paste0(mydir,"/03-graphic-outputs/pathfigF2_bigA")
semPaths(fit.2Fnx, "diagram", weighted = FALSE,  shapeMan = "rectangle", sizeMan = 8, 
    sizeMan2 = 5,filetype='jpg',filename=pathfigname,width=3,height=2) #draws a path diagram

pathfigname<-paste0(mydir,"/03-graphic-outputs/pathfigF2_bigB")
semPaths(fit.2Fny, "diagram", weighted = FALSE,  shapeMan = "rectangle", sizeMan = 8, 
    sizeMan2 = 5,filetype='jpg',filename=pathfigname,width=3,height=2) #draws a path diagram


#Looking at residuals : dichotic has high variability and big residuals - may be worth trying to scale it - or possibly just censor at a less extreme value
```
In practice, a model including a link from Rhyme Decision to Factor 1, and Dichotic Listening to Factor 2 gave a better fit than a model with both paths set to zero, but this is a very weak test of our prediction, because it will be passed if there is any correlation between behavioural and fTCD laterality indices, however weak.  Guided by the data, we ran an alternative model (not pre-registered) with both Rhyme Judgement and Dichotic Listening loading on Factor 1. This also gave excellent fit, with significant paths from both of these measures. 

Overall, the associations between behavioural and fTCD laterality indices were low enough to give little confidence in the specific pattern of associations. The main conclusion is that behavioural measures of laterality based on accuracy or speed of responding to lateralised stimuli have little in common with measures of relative blood flow to the two hemispheres while performing the same tasks. 


# Discussion

To simplify the interpretation of this complex dataset, it helps to focus on two overarching questions addressed by the study. The first question concerns correlations between laterality measures: in brief, is there evidence for a single laterality dimension on which people vary? The second question concerns handedness: does the answer to our first question differ in groups of left- and right-handers? In addition we consider specific issues arising in this dataset, namely the finding of right hemisphere lateralisation on Word Comprehension, and the lack of agreement between behavioural and fTCD laterality indices. Finally, we consider how the particular factor structure seen in the fTCD analysis might be accounted for. 

## Evidence for a single laterality dimension  

Previous attempts to consider the dimensionality of language lateralisation have been obscured by two issues. First, many studies have been conducted with measures whose reliability was not established. If two laterality indices are not correlated, it could just be because they are unreliable, and so it has been easy to dismiss lack of correlation between laterality measures as uninformative. Second, researchers have tended to focus only on measures that show left-lateralisation at the population level, treating unlateralised language measures as uninteresting. 

Considering first the online behavioural data, the most noteworthy observation was that the correlations between laterality indices from the three tasks were generally weak. This could not be attributed solely to poor reliability: although reliabilities of the two new tasks, Rhyme Decision and Word Comprehension, were not impressive (ranging from .54-.55 for test-retest), they were higher than the intercorrelations between measures. Our data was not suitable for a more formal model comparison, but inspection of the pattern of correlations between measures made it clear that our proposed two-factor model, with Dichotic Listening and Word Comprehension being positively correlated and unrelated to Rhyme Decision could not be supported. Indeed, the strongest correlation was found between Word Comprehension and Rhyme Decision, consistent with the idea that task demands (speeded responding to visual pictures) might be a greater determinant of strength of lateralisation than whether receptive or expressive language was involved.  

For the fTCD data, we again found good evidence for reliability of LIs on most tasks, even though some tasks were not left-lateralised. Consistent with Woodhead et al (2021), the LI on Syntactic Decision task had good reliability, and a distinctive pattern of association with other LIs, despite being unlateralised. This observation shows that lack of lateralisation at the population level does not mean that individuals use both hemispheres equally for the task: it seems rather that the population contains a mixture of people, some of whom consistently prefer the left hemisphere, and others the right. The LI from the Word Comprehension task was the least reliable in the battery, yet again showed quite distinctive patterns of selective association with other tasks. In addition, this task showed a trend for right-hemisphere bias, particularly in left-handers. 

With fTCD we were able to subject the single factor model to a stronger test, because we had sufficient tasks for Structural Equation Modeling. Consistent with Woodhead et al (2021), we could reject a single factor model; this gave a poor fit to the data, as it could not account for the fact that the correlations between LIs tended to form clusters. We tested a preregistered alternative two-factor model that involved a division between language production and language reception. This accounted for significantly more variance than the single factor model, but still left a great deal unexplained, and overall the fit was poor. Accordingly, following our preregistration, we divided the sample into two subgroups to explore different models and found one that gave good fit, which was then replicated in the second half of the sample. This again had a two factor structure, but had two of the tasks, Phonological Decision and Syntactic Comprehension, loading on both the factors. 

In a final step of analysis, we considered adding the laterality indices from online tasks to the model. The correlations between LIs from online tasks and fTCD were generally weak, and these measures did not help differentiate models.  Models that included Dichotic Listening and Rhyme Decision gave better fit when non-zero paths were included from these measures, than when they were set to zero, but the best fit was seen for a model where both Dichotic Listening and Rhyme Judgement loaded on Factor 1 (with language production tasks), rather than when Dichotic Listening was regarded as an indicator of Factor 2 (with receptive tasks).

## Left vs Right Handers  
For both online and fTCD LIs, with just one exception, there was a consistent trend for stronger left-hemisphere bias in right-handers than in left-handers. This reached significance on all measures except fTCD Word Comprehension. The exception was online Rhyme Judgement, which was not left-lateralised and where means for left- and right-handers were very similar. 

The SEM analysis allowed us to go beyond simple comparison of means to test whether the association between LIs showed a similar pattern in the two handedness groups. In our previous fTCD study using four of the same measures, we had concluded that there may be more dissociation between factors in left-handers, but the sample size was small for this kind of analysis.  Even with the current sample size, power to detect model invariance is limited. Having noted those limitations, our analysis suggested that there was no reason to postulate different models for left- vs right-handers. The substantial differences between these groups could be entirely accounted for in terms of differences in factor means.  As an analogy, we could say this would be like showing that the relationship between height and weight is similar in males and females, even though females are on average shorter and lighter. 

## Right hemisphere lateralisation for Word Comprehension

## Differences between behavioural and fTCD measures of laterality

## Interpreting the two-factor structure    
```{r taskanalysis,echo=F}
tasks <- read.csv(paste0(mydir,"/05-writeup/task details.csv"))
ftasks <- flextable(tasks[,1:7])
ftasks
tabnumber <- tabnumber+1
```
Table `r tabnumber` summarises the characteristics of the six fTCD tasks, grouped according to the factors they load on. The online Dichotic Listening task is also shown. The task battery had been designed to include three tasks that involved language generation, and three that involved receptive language. As predicted the former loaded on Factor 1 and the latter on Factor 2, but in addition Phonological Decision loaded on Factor 2, and Sentence Comprehension on Factor 1. Phonological Decision, unlike the other tasks loading on Factor 2, did not involve auditory input, but did have in common the 2-choice response format of other Factor 2 tasks. 

Sentence Comprehension also behaved unexpectedly, in that it had significant loadings on Factor 1, despite being designed as a purely receptive task. While it is possible that participants might have covertly repeated sentences to themselves when doing the task, the fast pace of the task made that unlikely, and furthermore, our previous study suggested that simple production of spoken output was not the key attribute of Factor 1 tasks; we had dropped from the current study a List Generation task used by Woodhead, Rutherford and Bishop (2020) that simply involved repeating overlearned sequences such as days of the week, as it had been only weakly lateralised. The Sentence Comprehension task did, however, involve  using syntactic information to assign semantic roles and build meaning representations, and hence more linguistic computation than the tasks that used single word stimuli. 

```{r densplotexplore}

#This uses a function defined above which makes individual density plots by handedness.  Here they are assembled into a column so one can visualise the change in overall laterality.
latcols<-c("A_P1","B_P2","C_P3","D_R1","E_R2","F_R3")
latnames<-c("Word Generation","Sentence Generation","Phonological Decision","Word Comprehension","Sentence Comprehension","Syntactic Decision")
w<-which(colnames(ddati) %in% latcols)
means<-colMeans(ddati[,w])
nuorder <- order(means,decreasing=T)
nucols<-min(w)+nuorder-1
plotlist<-NULL
for (i in 1:6){
  filename<-paste0(mydir,'/03-graphic-outputs/densplot_',i,'.png')
  thiscol<-colnames(ddati)[nucols[i]]
  thisgroup<-'Handed'
  mylab<-latnames[nuorder[i]]
  thisplot <- densplots(ddati,thiscol,thisgroup,mylab)
  ggsave(filename,width = 6, height = 3)
  #There should be a way to avoid the next bit of clunky code!
  d6<-thisplot
  if(i==1){d1<-thisplot}
  if(i==2){d2<-thisplot}
  if(i==3){d3<-thisplot}
  if(i==4){d4<-thisplot}
  if(i==5){d5<-thisplot}

}
alldens <- ggarrange(d1,d2,d3,d4,d5,d6, ncol = 1, nrow = 6,common.legend=TRUE)
ggsave(paste0(mydir,"/03-graphic-outputs/alldens.png"),width = 3, height = 7)

```

<!---![Density plots showing z-LI scores for six fTCD tasks ordered by overall left-hemisphere bias.](~/Dropbox/COLA_RR_Analysis/03-graphic-outputs/alldens.png)
Nice density plots but they really just show same information as the pirate plots, so not so sure we need them here?-->
In Figure `r piratenumber` the six fTCD tasks are ordered according to the population bias to left-hemisphere processing, and it can be seen that the four tasks that load on Factor 1 are all significantly lateralised, at least in right-handers. In contrast, those loading on Factor 2 include two tasks that are not significantly lateralised.  

In interpreting this finding, we should first rule out two trivial explanations for the factor structure uncovered in SEM. First, this structure cannot be regarded as an artefact of including tasks differing in degree of laterality. This is because factor structure in SEM is computed solely on the basis of covariances between measures, and means do not affect it. Thus, we could add a constant to the means for tasks D and E to make them lateralised, and the factor solution would remain the same. 

Second, we can rule out an explanation that treats the  non-lateralised tasks as not relevant for studying individual differences in laterality. Such an explanation would be justified if tasks such as Word Comprehension, Sentence Comprehension and Syntactic Decision were simply unreliable. Low reliability would be expected if these tasks were not lateralised in individuals, because people used both hemispheres jointly, or switched from one to the other at random. The new task, Word Comprehension, was the least reliable in the battery, but nevertheless, the split-half reliability indicated was moderate. The other two receptive tasks had good test-retest reliability in our previous study (Woodhead et al, 2021) and good split-half reliability in the current study. Thus, even though there is weak or absent lateralisation at the population level on these tasks, the degree and direction of lateralisation is reasonably consistent within individuals. And indeed, if that were not the case, we would not expect the tasks to show moderate intercorrelations with one another. 

To account for the observed pattern of results, we postulate two language centres, one lateralised at the population level (centre L), and the other centred on zero (centre Z). An individual's observed fTCD laterality on a task will depend on the extent to which these two centres are implicated in task performance, with Word Generation and Sentence Generation being largely dominated by centre L, Syntactic Decision and Word Comprehension by centre Z, and Phonological Decision and Sentence Comprehension implicating both centres. 

To some extent, this is less of an explanation than a redescription of the data, but it does yield novel predictions that can be tested using fMRI, which gives information on localisation of activation within a hemisphere. The prediction would be that there would be more overlap in brain regions activated by tasks that load on the same Factor than for those loading on different Factors, and furthermore, activation would only be lateralised for brain regions supporting Factor 1 tasks.  


## Supplementary materials  

Supplementary Figure 1

![Density plots and scatterplot showing relationship between conventional laterality index and z-LI for dichotic listening.](~/Dropbox/COLA_RR_Analysis/03-graphic-outputs/Afig4_DLdens.png)
Supplementary Figure 2
![Density plots and scatterplot showing z-LI scores for Rhyme Decision and Word Comprehension listening.](~/Dropbox/COLA_RR_Analysis/03-graphic-outputs/Afig5_RDWCdens.png)
```{r saveworkspace,echo=F}
#save.image(file = "my_work_space.RData")
```


```{r anothertryfactors,echo=F}
#Trying to extract factor scores that are not centred on zero.
#Not sure this is legit!
mys<-standardizedSolution(fit.2Fn)
wts1 <- mys$est.std[1:4]
wts2 <-mys$est.std[5:8]
ddati$f1<-wts1[1]*ddati$A_P1+
  wts1[2]*ddati$B_P2+
  wts1[3]*ddati$C_P3+
  wts1[4]*ddati$E_R2

ddati$f2<-wts2[1]*ddati$F_R3+
  wts2[2]*ddati$C_P3+
  wts2[3]*ddati$D_R1+
  wts2[4]*ddati$E_R2

myx<-ddati$f1
myy<-ddati$f2
mygroup< ddati$Handed
mydat<-as.data.frame(cbind(myx,myy,mygroup))
mydat$mygroup<-as.factor(mydat$mygroup)
namex<-'Factor 1'
namey<-"Factor 2"
namegroup <- "Handedness"
grouplabels<- c("Left","Right")
mylines<-2 #horiz and vertical lines showing zero
mytitle <- "Factors derived from standardized weighted \nsum from modified 2 factor model"
mycompositeplot <- dodensity(mydat,myx,myy,mygroup,namex,namey,namegroup,grouplabels,mylines,mytitle)

mycompositeplot
ggsave(paste0(mydir,"/03-graphic-outputs/standfactors.png"),width = 5, height = 4)


```




