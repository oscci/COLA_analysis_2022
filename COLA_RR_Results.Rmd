---
title: "COLA_results"
author: "DVM Bishop"
date: "17 Feb 2022"
output:
  word_document: default
  html_document:
    df_print: paged
  pdf_document: default
---

<!--- Github project COLA_analysis_2022 initiated on 4 Feb 2022. Working version currently on Dropbox; this is a clone to allow version control --->
<!---DB Updated 7 Feb 2022; removing AIC analysis, with explanation--->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# If the package is not installed, install it. If it is installed, load it.
usePackage <- function(p) {
    if (!is.element(p, installed.packages()[,1]))
        install.packages(p, dep = TRUE)
    require(p, character.only = TRUE)
}
usePackage('dplyr')
usePackage('tidyr')
usePackage('ggplot2')
usePackage('yarrr') #for pirate plots
usePackage("osfr") #for reading files from OSF
usePackage("stringr")
usePackage("table1") #useful for making simple tables of demographic etc
usePackage("ggExtra") #for marginal plots
usePackage("ggpubr")
usePackage("patchwork") #for combining plots in ggplot
usePackage("flextable")

usePackage("tidyverse")
usePackage("here") #to find filepaths
usePackage("kableExtra")

usePackage("ggstatsplot")
usePackage("MASS") #includes boxcox function
usePackage("MBESS")
usePackage("nlme")
usePackage("semPower")
usePackage("semTools")
usePackage("bookdown")
usePackage("lavaan")
usePackage("semPlot")

usePackage("officer")
usePackage("corrr") #added by DB for easy correlations
usePackage("plyr")
usePackage("qpcR") #used in Kievit script
usePackage("ggpubr")
usePackage("reshape2")
usePackage("mice")
usePackage("MVN") #multivariate normality
usePackage("viridis") #possibly for control over colours in ggplot
usePackage("hrbrthemes") #for fancy themes in ggplot?

#hrbrthemes::import_roboto_condensed()  #not sure if needed - was in example with ggplot

options(scipen=999)

tabnumber <-0 #initialise counter for tables
fignumber <-0 # initialise counter for figs
suppfignumber<-0 
```

<!--- GENERIC FUNCTIONS START -->
```{r pformat, echo=F}
#function to format p-values
pformat=function(myp){
  pout <- paste('p =',round(myp,3))
  if(myp<.001){pout = 'p < .001'}
  return(pout)
}
```

```{r pformat2, echo=F}
#function to format p-values, without the 'p = ' bit
pformat2=function(myp){
  pout <- round(myp,3)
  if(myp<.001){pout ='< .001'}
  return(pout)
}
```


```{r LIdensityplot,echo=F}
#generic density plot, subsetted by handedness or another categorical variable
#includes line showing zero point.
#takes as input allsum; temp and cattemp are dummy columns that are assigned prior to call to the function
doLIplot <- function(myfile,temp,cattemp,mysubsetname,mysubsetlabels,xlabel,xrange){
LI.plot <- ggplot(myfile, aes(x=temp, color=as.factor(cattemp))) +
  geom_density()+
  xlab(xlabel)+
  geom_vline(xintercept = 0,lty=3)+
  xlim(xrange)+
  scale_color_manual(name=mysubsetname,
                       labels=mysubsetlabels,
                       values=c("blue","red"))


return(LI.plot)
}
```


```{r densfunction,echo=F}
#Function to do a density plot for specified file coded by group
#(Not sure if output differs from previous function! - need to check)

dodensity <- function(mydat,myx,myy,mygroup,namex,namey,namegroup,grouplabels,mylines,mytitle){
  
 
scatterdens <- ggplot(mydat,aes(x = myx, y = myy,col=mygroup)) + 
  geom_point(shape = 4,  size = 1)+
  scale_color_manual(name=namegroup,
                       labels=grouplabels,
                       values=c("blue","red"))+
  labs(x = namex,y=namey)+
  ggtitle(mytitle) 
if (mylines==1){
   scatterdens<-scatterdens+geom_abline(intercept = 0, slope = 1)
}
if (mylines==2){
   scatterdens<-scatterdens+geom_hline(yintercept = 0,linetype="dashed")+
   geom_vline(xintercept = 0,linetype="dashed")
}

 
dens1 <- ggplot(mydat, aes(x = myx, fill = mygroup)) + 
  geom_density(alpha = 0.4) + 
  geom_vline(xintercept = 0, linetype="dotted")+
  theme_void() + 
  theme(legend.position = "none")

dens2 <- ggplot(mydat, aes(x = myy, fill = mygroup)) + 
  geom_density(alpha = 0.4) + 
  geom_vline(xintercept = 0, linetype="dotted")+
  theme_void() + 
  theme(legend.position = "none") + 
  coord_flip()

alldens<-dens1 + plot_spacer() + scatterdens + dens2 + 
  plot_layout(ncol = 2, nrow = 2, widths = c(4, 1), heights = c(1, 4))

#ggsave(paste0(mydir,"/03-graphic-outputs/",plotname,".png"),width = 5, height = 4)
return(alldens)
}
```

```{r filltable,echo=F,warning=F}
#Generate function to populate summary data frame - same steps for all online tasks
#This function also repurposed for ftcd
populate <- function(mydf,mytask,mysummary,writecolnum,tasktype){
  if(tasktype=='beh'){
w<-which(colnames(mydf)==paste0(mytask,'.zlat'))
x<-which(colnames(mydf)==paste0('exclude',mytask))
odds<-which(colnames(mydf)==paste0(mytask,'.odd.zlat'))
evens<-which(colnames(mydf)==paste0(mytask,'.even.zlat'))
  }
    if(tasktype=='ftcd'){
w<-which(colnames(mydf)==paste0(mytask,'_mean_LI'))
x<-which(colnames(mydf)==paste0(mytask,'_exclude'))
odds<-which(colnames(mydf)==paste0(mytask,'_mean_odd'))
evens<-which(colnames(mydf)==paste0(mytask,'_mean_even'))
  }
  
thisdat<-mydf[mydf[,x]==0,]
thisdat$thiscol<-thisdat[,w]
normp <-shapiro.test(thisdat$thiscol)$p.value
nL <- nrow(filter(thisdat,Rhanded==0))
nR <- nrow(filter(thisdat,Rhanded==1))
tL<- t.test(thisdat$thiscol[thisdat$Rhanded==0])
tR<- t.test(thisdat$thiscol[thisdat$Rhanded==1])
sdL<-round(sd(thisdat$thiscol[thisdat$Rhanded==0],na.rm=T),2)
sdR<-round(sd(thisdat$thiscol[thisdat$Rhanded==1],na.rm=T),2)
sdall <-round(sd(thisdat$thiscol,na.rm=T),2) 
tcompare <- t.test(thisdat$thiscol~thisdat$Rhanded,alternative='less')
#write N for L and R in row 1 of online summary
mysummary[1,writecolnum]<-paste0(nL,' LH + ',nR,' RH')
mysummary[2,writecolnum]<-paste0(round(mean(thisdat$thiscol,na.rm=T),2)," (",sdall,")")
mysummary[3,writecolnum]<-paste0(round(skew(thisdat$thiscol)[1],2),' (',pformat(skew(thisdat$thiscol)[4]),')')
mysummary[4,writecolnum]<-paste0(round(kurtosis(thisdat$thiscol)[1],2),' (',pformat(kurtosis(thisdat$thiscol)[4]),')')
mysummary[5,writecolnum]<-pformat(normp)
mysummary[6,writecolnum]<-paste0(round(tL$estimate,2)," (",sdL,")")
mysummary[7,writecolnum]<-paste0(round(tR$estimate,2)," (",sdR,")")
mysummary[8,writecolnum]<- paste0('t = ',round(tL$statistic,1),'; ', pformat(tL$p.value))
mysummary[9,writecolnum]<- paste0('t = ',round(tR$statistic,1),'; ', pformat(tR$p.value))
mysummary[10,writecolnum]<- paste0('t = ',-round(tcompare$statistic,1),'; ', pformat(tcompare$p.value))
mysummary[11,writecolnum]<- round(cor(thisdat[,odds],thisdat[,evens],use='complete.obs'),3)



return(mysummary)
}
```

```{r trianglefunction,echo=F}
#Gets upper triangle of a correlation matrix
 get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat,diag=T)]<- NA
    return(cormat)
  }
```



```{r heatmapfunction,echo=F}
#Make a heatmap
# http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization
makeheatmap <- function(mydf,mycols){
cormat <- cor(mydf[,mycols],use="complete.obs")

melted_cormat <- melt(cormat)
head(melted_cormat)

upper_tri <- get_upper_tri(cormat)
# Melt the correlation matrix

melted_cormat <- melt(upper_tri, na.rm = TRUE)
# Heatmap

ggheatmap <- ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()

myheatmap <- ggheatmap + 
geom_text(aes(Var2, Var1, label = round(value,3)), color = "black", size = 4) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.5, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))

return(myheatmap)}
```


```{r bivplotfunction,echo=F,warning=F}
#Function to make a bivariate plot subdivided by group, with spearman correlation in plot. Handedness coded by shape.

#When we call this function we have already created temporary x and y cols (tempx and tempy) to be used in this function
bivplot<-function(bivdat,name1,name2){
#correlations for each group
cor1 <- cor.test(bivdat$tempx[bivdat$Group==1],bivdat$tempy[bivdat$Group==1],method="spearman")
cor2 <- cor.test(bivdat$tempx[bivdat$Group==2],bivdat$tempy[bivdat$Group==2],method="spearman")
lab1<- paste0("Group 1: rs = ",round(cor1$estimate,3))
lab2<- paste0("Group 2: rs = ",round(cor2$estimate,3))   

myplot <- ggplot(bivdat, aes(x=tempx, y=tempy, color=Handedness,shape=as.factor(Group))) +
  xlab(name1)+
   ylab(name2)+
  xlim(-10,10)+
  ylim(-10,10)+
  geom_point()+
  scale_shape_manual(name="Group (random)",
                     labels=c(1,2),
                     values=c(1,16))+
  scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
  geom_hline(yintercept=0,linetype="dashed")+
   geom_vline(xintercept=0,linetype="dashed")+
  annotate("text", x=-5, y=9.5, label= lab1,size=3) +
  annotate("text", x=-5, y=8, label= lab2,size=3)   
  return(myplot)
}


```


```{r bivplotfunction2,echo=F,warning=F}
#Function to make a bivariate plot colour coded by handedness, with spearman correlation in plot

#When we call this function we have already created temporary x and y cols (tempx and tempy) to be used in this function
bivplot2<-function(bivdat,name1,name2){
#correlations for each group
cor1 <- cor.test(bivdat$tempx[bivdat$Rhanded==0],bivdat$tempy[bivdat$Rhanded==0],method="spearman")
cor2 <- cor.test(bivdat$tempx[bivdat$Rhanded==1],bivdat$tempy[bivdat$Rhanded==1],method="spearman")
lab1<- paste0("L-handers: rs = ",round(cor1$estimate,2))
lab2<- paste0("R-handers: rs = ",round(cor2$estimate,2))   

myplot <- ggplot(bivdat, aes(x=tempx, y=tempy, color=Handedness)) +
  xlab(name1)+
   ylab(name2)+
  xlim(-10,10)+
  ylim(-10,10)+
   geom_point(shape=1, size=1)+
  scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
  geom_hline(yintercept=0,linetype="dashed")+
   geom_vline(xintercept=0,linetype="dashed")+
  annotate("text", x=-5.5, y=9.5, label= lab1,size=2.8) +
  annotate("text", x=-5.5, y=8, label= lab2,size=2.8)   
  return(myplot)
}


```

```{r scatterplusfunction,echo=F}
#Function for doing scatterplot with marginal density distributions
doscatterplus <- function(myfile, taskname,mycolnames,myrange){ #myfile contains group in col 1 and the 2 cols to plot in cols 2-3
colnames(myfile)[2:4]<- mycolnames

r <- round(cor(myfile$Odd,myfile$Even,use="complete.obs",method="spearman"),3)
myt<-t.test(myfile$All~myfile$Handed)
myt2 <- t.test(myfile$All) #single group t-test
text1 <- paste0('Mean diff from zero: \nt = ',round(myt2$statistic,2),"; ", pformat(myt2$p.value))
text2 <- paste0('L vs R handers \n(all trials):\n t = ',round(myt$statistic,2),"; ", pformat(myt$p.value))
text3 <-paste0('Spearman rho: ',r)
p <- ggplot(myfile, aes_string('Odd','Even')) +
  aes_string(colour = 'Handed') +
  geom_point() + theme_bw(15)+
  annotate("text", x = 3.2, y = -3,label=text3,size=3)+
  annotate("text", x = -2.5, y = 5.5,label=text1,size=3)+
  annotate("text", x = -2.5, y = 3.5,label=text2,size=3)+
  geom_hline(yintercept=0,linetype="dashed",colour="grey")+
  geom_vline(xintercept=0,linetype="dashed",colour="grey")+
  xlim(myrange)+
  ylim(myrange)+
  ggtitle(taskname)

p2 <- ggExtra::ggMarginal(
  p,
  type = 'density',
  margins = 'both',
  size = 5,
  groupColour = TRUE,
  groupFill = TRUE
)
return(p2)

}

```

```{r optimalBoxCoxFunction,echo=F}
#Where a measure is non-normal on Shapiro-Wilk test, the BoxCox transformation is applied.  Because the LIs include negative values, we first add a constant so all values are positive. After applying the BoxCox, we add another constant to ensure that the mean is unchanged by the transformation.

#The BoxCox function is similar to applying a transformation such as log or square root, but rather than pre-specifying the function, we hunt for the optimal value of lambda to give normal data, where newdata <- olddata^lambda-1/lambda

optimalBoxCox <- function(myvector){

#find optimal lambda for Box-Cox transformation 

offset<-min(myvector,na.rm=T)
origmean <- mean(myvector,na.rm=T)
respvar <- -offset+.1+myvector
bc <- boxcox(respvar ~ 1)
lambda <- bc$x[which.max(bc$y)]
newresp <- respvar^lambda-1/lambda
numean<-mean(newresp,na.rm=T)
newoffset <- numean-origmean
nuvector<-newresp-newoffset
return(nuvector)

}


```

```{r commonBoxCoxFunction,echo=F}
#Where a measure is non-normal on Shapiro-Wilk test, the BoxCox transformation is applied.  Because the LIs include negative values, we first add a constant so all values are positive. After applying the BoxCox, we add another constant to ensure that the mean is unchanged by the transformation.

#The BoxCox function is similar to applying a transformation such as log or square root, but rather than pre-specifying the function, we hunt for the optimal value of lambda to give normal data, where newdata <- olddata^lambda-1/lambda

commonBoxCox <- function(myvector){

#find optimal lambda for Box-Cox transformation 

offset<-min(myvector,na.rm=T)
origmean <- mean(myvector,na.rm=T)
respvar <- -offset+.1+myvector
lambda<-1.12
newresp <- respvar^lambda-1/lambda
numean<-mean(newresp,na.rm=T)
newoffset <- numean-origmean
nuvector<-newresp-newoffset
return(nuvector)

}


```


```{r do-boxcoxfunction,echo=F}
#This function identifies columns with nonnormal data and applies optimal BoxCox function, saving the new version of the variable with suffix _bc

#NB I have set this to only do transform if nonnormality gives p < pcut.
#To ignore normalisation, just use extreme pcut 
#Concern that otherwise transform  might distort relation between variables?
normalisedf <- function(thisdat,pcut){ #need a data frame with just the variables to be normalised
  mymvn <- mvn(thisdat,mvnTest = c("mardia", "hz", "royston", "dh",
  "energy")) #useful for getting normality check on all vars
  print(mymvn)
swlist <-mymvn$univariateNormality
collist<-vector() #initialise a vector that will hold list of cols to be normalised
for (i in 1:nrow(swlist)){
  if (as.numeric(swlist[i,4])>pcut){collist<-c(collist,i)}
  if (as.numeric(swlist[i,4])<pcut){ #pvalue
    thisdat$temp<-optimalBoxCox(thisdat[,i])
    newmean<-mean(thisdat$temp,na.rm=T)
    colnames(thisdat)[ncol(thisdat)]<-paste0(colnames(thisdat)[i],'_bc')
    collist<-c(collist,ncol(thisdat))
  }
}
return(list(thisdat,collist))
}
```


```{r do-commonboxcoxfunction,echo=F}
#This function applies common BoxCox function, saving the new version of the variable with suffix _bc

#NB I have set this to only do transform if nonnormality gives p < pcut.
#To ignore normalisation, just use extreme pcut 
#Concern that otherwise transform  might distort relation between variables?
normalisedfcommon <- function(thisdat,pcut){ #need a data frame with just the variables to be normalised. pcut not used here

collist<-1:6 #initialise a vector that will hold list of cols to be normalised
for (i in 1:6){

    thisdat$temp<-commonBoxCox(thisdat[,i])
    newmean<-mean(thisdat$temp,na.rm=T)
    colnames(thisdat)[ncol(thisdat)]<-paste0(colnames(thisdat)[i],'_bc')
    collist<-c(collist,ncol(thisdat))
  }

return(list(thisdat,collist))
}
```

```{r makeSEMtabfunction,echo=F}
#Function to make tidy table for SEM output for model with one group
#This is now largely superseded by bigsummary, though SEMtab has more complete information
makeSEMtab <- function(myfit){
ss<- summary(myfit)$PE
 srow<-nrow(ss)
 scol<-ncol(ss)
mySEMout <-ss[,-4]
mySEMout[,4:7]<-round(mySEMout[,4:7],3)

#add fit measures
myfitmeasures<-c('CFI','rmsea','chisq','DF') #can modify this if need be
fm<-fitmeasures(myfit,myfitmeasures)
for(i in 1:length(myfitmeasures)){
mySEMout[(i+srow),1]<-myfitmeasures[i] #write name of fit measure to col 1
mySEMout[(i+srow),2]<-round(fm[i],3) #write value to col 2
}
return(mySEMout)
}
```

```{r makeSEMtab2function,echo=F}
#Table for SEM output side by side for 2 groups
makeSEMtab2 <- function(myfit,mygroups){
ss<- summary(myfit)$PE
 srow<-nrow(ss)
 scol<-ncol(ss)
mySEMout <- cbind(ss[1:(srow/2),c(1:3,(scol-3):scol)],ss[(1+srow/2):srow,c((scol-3):scol)])
mySEMout[,4:11]<-round(mySEMout[,4:11],3)
mySEMout[(srow/2+1):(srow/2+4),]<-NA #additional rows for fit measures


#add fit measures
myfitmeasures<-c('CFI','rmsea','chisq','DF') #can modify this if need be
fm<-fitmeasures(myfit,myfitmeasures)
for(i in 1:length(myfitmeasures)){
mySEMout[(i+srow/2),1]<-myfitmeasures[i] #write name of fit measure to col 1
mySEMout[(i+srow/2),2]<-round(fm[i],3) #write value to col 2
}

#make different col headers for the 2 groups
colnames(mySEMout)[4:7]<-paste0(colnames(mySEMout)[4:7],mygroups[1])
colnames(mySEMout)[8:11]<-paste0(colnames(mySEMout)[8:11],mygroups[2])
return(mySEMout)
}
```


```{r add-to-bigsummaryfunction,echo=F}
#This function adds to the bigsummary data frame - it takes the paths and a few diagnostic stats from myfit and writes to writecol in bigsummary. If comparisonfit is specified, it will also do a chi square comparison with that model and put p value in final row.
addmodel <- function(bigsummary,myfit,comparisonfit,myfitname,writecol){
  colnames(bigsummary)[writecol]<-myfitname #name of the model as column name
ss<- summary(myfit)$PE #get coefficients from current model
bigsummary[1,writecol]<-lavInspect(myfit,'nobs') #sample size goes in 1st row
thisrow<-1 #initialise counter for factor loadings
#if a relevant path exists, put its estimate in correct row; first Factor1, then Factor2
#If  no path in the model, it just skips it
for (f in 1:2){
for (m in 1:6){
  thisrow<-thisrow+1
  w<-which(ss$lhs==paste0('f',f))
  x<-which(substr(ss$rhs,1,1) == LETTERS[m])
  myrow<-intersect(w,x)[1]
  bigsummary[thisrow,writecol]<-round(ss$est[myrow],3)
}
}
#Correlation between factors is added (if it exists)
myrow<-intersect(which(ss$lhs=='f1'),which(ss$rhs=='f2'))[1]
if(length(myrow)>0){
  bigsummary[14,writecol]<-round(ss$est[myrow],3)
}

wantfits <- c('CFI','rmsea','chisq','df') #fit indices to include
#NB there are many other fit indices we could add, but if we do, would need to modify bigsummary.
#Currently script assumes we will have these 4 fit indices 

fm<-fitmeasures(myfit,wantfits)
#find first row to write to
r<-which(bigsummary[,1]==wantfits[1]) #find row corresponding to first fit index
bigsummary[r:(r+length(wantfits)-1),writecol]<-fm #write the fit indices in successive rows, starting with r




#Add chi square comparison with comparisonfit
if(!is.na(comparisonfit)){
chicomp <- fitmeasures(comparisonfit,wantfits[3:4]) #get chisq and DF for comparison model
chidiff<-chicomp[1]-bigsummary[17,writecol]
dfdiff<-chicomp[2]-bigsummary[18,writecol]
pval<- 1-pchisq(chidiff,dfdiff) #p-value for chi sq difference
bigsummary[19,writecol]<-pformat2(pval) #p-value written to row 19
}

#because this is data.frame, all values in a column must be same format.
#Starts numeric, but that means Nobs and DF is shown to 3 decimal places. 
#Can remove decimal places with line below, but then all values become characters
#In practice, I've found trying to format creates problems, so better to format bigsummary outside this function
#bigsummary[1,mycol]<-as.character(round(bigsummary[1,mycol],0))

return(bigsummary)
}
```

```{r factorscores,echo=F}
#FUnction to extract factor scores and plot them and save plot
makefactorplot <- function(myfit,fitname,thisdat){
lastc <- ncol(thisdat)
myfacs<-predict(myfit)
myfacs<-as.data.frame(myfacs)
colnames(myfacs)<-c("Factor1","Factor2")
w<-which(colnames(ddati)=='Factor1') #check if we already have col for factors
if(length(w)==0){
thisdat<-cbind(thisdat,as.data.frame(myfacs))
}
if(length(w)>0){
  thisdat$Factor1 <- as.data.frame(myfacs[,1])
  thisdat$Factor2 <- as.data.frame(myfacs[,2])
}

plotf1f2 <- myscatter(thisdat$Factor1,thisdat$Factor2,xlabel='Factor1',ylabel='Factor2',thisdat)

plotname <- paste0(mydir,"/03-graphic-outputs/factors_",fitname,".png")

ggsave(plotname,width = 5, height = 3)
return(thisdat)
}

```
<!--- GENERIC FUNCTIONS END -->

# Results

## Descriptives  
```{r readcombined,echo=F}
mydir <- "~/Dropbox/COLA_RR_Analysis"
readfile<-paste0(mydir,"/02-data/combined_data.csv")
combdat <- read.csv(readfile)

#We will create a code that reflects whether strict exclusion criteria are met for language. This is 1 for lextale < 80, 10 for gram12err >1, and 11 if both are met

#One subject was wrongly coded as ftcd=1: they came for testing but could not get signal and so we reset ftcd to zero for them.
w<-which(combdat$ID==3495951)
combdat$ftcd[w]<-0

combdat$lang2exclude <- 0
w<- which(combdat$gramerr12>1)
combdat$lang2exclude[w]<-10
w<- which(combdat$lexTALE<80)
combdat$lang2exclude[w]<-combdat$lang2exclude[w]+1
#For now we set 'excluded' for any who were below 80 on LexTale or more than 1 error on short grammar AND are not native English speakers. I.e. all native English are included, regardless of language tests
combdat$excluded <- 0
w<-intersect(which(combdat$lang2exclude>0),which(combdat$nativeEnglish==0))
combdat$excluded[w] <-1

testtab <- table(combdat$Rhanded,combdat$ftcd)
ftcd.dat <- filter(combdat,ftcd==1) #has demog, behav and ftcd data for those who did ftcd
langtab <- table(ftcd.dat$Rhanded, ftcd.dat$lang2exclude,ftcd.dat$nativeEnglish)
#Langtab cells can be accessed with 3 indices corresponding to Rhanded,langexclude and native English status).

myftcd <- filter(ftcd.dat,excluded==0) #we'll do analysis on myftcd
```

_Departures from pre-registration plan_  
Our plan had been to recruit 300 left-handers and 150 right-handers for the online behavioural battery, and from these to select 112 left-handers and 112 right-handers for in-person testing. Because of disruption to in-person testing caused by pandemic restrictions, we did not meet our target numbers for in-person testing, despite over-recruiting for online testing. In total we tested `r testtab[1,2] + testtab[1,1]` left-handers, `r testtab[1,2]` of whom were tested with fTCD, and `r testtab[2,2] + testtab[2,1]` right-handers, `r testtab[2,2]` of whom were tested with fTCD. However, `r langtab[1,2,1]+langtab[1,3,1]+langtab[1,4,1]` left-handers and `r langtab[2,2,1]+langtab[2,3,1]+langtab[2,4,1]` right-handers tested with fTCD were non-native English speakers who met our criteria for excluding participants on the basis of either the lexTALE or Games with Words measures of English language competence, giving final samples of `r langtab[1,1,1]+langtab[1,1,2]+langtab[1,2,2]+langtab[1,3,2]+langtab[1,4,2]` left-handers and `r langtab[2,1,1]+langtab[2,1,2]+langtab[2,2,2]+langtab[2,3,2]+langtab[2,4,2]` right-handers. An unexpected issue was that a few native English speakers failed the language screen (`r langtab[1,2,2]+langtab[1,3,2]+langtab[1,4,2]` left-handers and `r langtab[2,2,2]+langtab[2,3,2]+langtab[2,4,2]` right-handers): however, the purpose of these tests had been to exclude non-native speakers with inadequate language skills, so we retained all native English participants in the study.



### Subsample for test-retest study
```{r test-retest, echo=F}
#need to read in sess1 and sess3 for the test-retest data

sess1 <- read.csv(paste0(mydir,'/02-data/02.1_gorilla/sess1.csv'))
sess3 <- read.csv(paste0(mydir,'/02-data/02.1_gorilla/sess3.csv'))
mtab<-table(sess3$male,sess3$Rhanded)

# Calculate delay between session 1 and session 3 for retest participants
for (i in 1:length(sess3$ID)){
  sess3$date_S1 <- sess1$date[which(sess1$ID == sess3$ID)]
}

# Reformat dates
sess3$date <- as.Date(sess3$date, format='%d/%m/%Y')
sess3$date_S1 <- as.Date(sess3$date_S1, format='%d/%m/%Y')
sess3$date_diff <- difftime(sess3$date, sess3$date_S1)

date_diff_stats <- fivenum(sess3$date_diff)

```

A subsample of `r nrow(sess3)` participants was retested on part of the online battery within `r ceiling(as.numeric(date_diff_stats[5]) / 7)` weeks of the first session (median = `r date_diff_stats[3]` days, range = `r date_diff_stats[1]` to `r date_diff_stats[5]` days). There were `r mtab[1,1]` left-handed females, `r mtab[2,1]` left-handed males, `r mtab[1,2]` right-handed females, and `r mtab[2,2]` right-handed males. The retest session included the Rhyme Decision and Word Comprehension tasks, which were new, but not Dichotic Listening, for which we had adequate evidence of test-retest reliability from the previous study by Parker et al (2021). 

### Demographic data 

```{r demog.table,echo=F}
#We'll use the table1 package to create a nice-looking summary table for demographics; this excludes those excluded on language tests, but otherwise includes everyone given the online testing.

mycomb <- filter(combdat,excluded==0) #create a version of the main data file that excludes anyone excluded on Lextale or Grammar task

mycomb$Gender <- factor(mycomb$male,levels=c(0,1),labels=c("Female","Male"))
label(mycomb$age) <- "Age (yr)"
mycomb$Native <- factor(mycomb$nativeEnglish,levels=c(0,1),labels=c("No","Yes"))
label(mycomb$Native)<-"Native English speaker"
mycomb$Bilingual <- factor(mycomb$bilingual,levels=c("No","Yes"),labels=c("No","Yes"))
mycomb$Handedness <- factor(mycomb$Rhanded,levels=c(0,1),labels=c("Left-Handed","Right-Handed"))
mycomb$ftcd <- factor(mycomb$ftcd,levels=c(0,1),labels=c("No FTCD data","With FTCD data"))

demog.table <- table1(~ Gender + age + Native+ Bilingual+EHI.LI|(factor(ftcd)+Handedness) , data=mycomb,overall=F)
ftab <- t1flex(demog.table) #convert from table1 to flextable format to allow formatting of width etc
autofit(ftab)
tabnumber<-tabnumber+1
```
Table `r tabnumber` shows demographic data for the subset of individuals tested on the online battery only, and the subset who also completed the session with FTCD. It is evident from inspection that there are no systematic differences between the two subgroups. 

### Lateralised responses on the online battery


```{r dichotic.plot,echo=F}


mycomb$DLsig <-0
w<-which(abs(mycomb$DL.zlat)>1.96)
mycomb$DLsig[w]<-1


DLdat <- mycomb[mycomb$excludeDL==0,]
DLsides <- ggplot(DLdat, aes(x=DL.L, y=DL.R, color=Handedness,shape=as.factor(DLsig))) +
  xlab("N correct L ear")+
   ylab("N correct R ear")+
  geom_point()+
  ggtitle("Dichotic Listening") +
  scale_shape_manual(name="Significantly lateralised",
                     labels=c("No","Yes"),
                     values=c(4,16))+
  scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
  geom_abline(intercept = 0, slope = 1)
ggsave(paste0(mydir,"/03-graphic-outputs/AFig1_DLsides.png"),width = 6, height = 4)

fignumber<-fignumber+1

```



```{r RDT,echo=F}
mycomb$RDTsig <-0
w<-which(abs(mycomb$RDT.zlat)>1.96)
mycomb$RDTsig[w]<-1


RDTdat <- mycomb[mycomb$excludeRDT==0,]
RDTsides <- ggplot(RDTdat, aes(x=RDT.Lmean, y=RDT.Rmean, color=Handedness,shape=as.factor(RDTsig))) +
  xlab("Mean correct RT (ms) left VHF")+
   ylab("Mean correct RT (ms) right VHF")+
  geom_point()+
  ggtitle("Rhyme Decision") +
  scale_shape_manual(name="Significantly lateralised",
                     labels=c("No","Yes"),
                     values=c(4,16))+
  scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
  geom_abline(intercept = 0, slope = 1)
ggsave(paste0(mydir,"/03-graphic-outputs/AFig2_RDTsides.png"),width = 6, height = 4)


```


```{r WC,echo=F}
mycomb$WCsig <-0
w<-which(abs(mycomb$WC.zlat)>1.96)
mycomb$WCsig[w]<-1


WCdat <- mycomb[mycomb$excludeWC==0,]
WCsides <- ggplot(WCdat, aes(x=WC.Lmean, y=WC.Rmean, color=Handedness,shape=as.factor(WCsig))) +
  xlab("Mean correct RT (ms) left VHF")+
   ylab("Mean correct RT (ms) right VHF")+
  geom_point()+
  ggtitle("Word Comprehension") +
  scale_shape_manual(name="Significantly lateralised",
                     labels=c("No","Yes"),
                     values=c(4,16))+
  scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
  geom_abline(intercept = 0, slope = 1)
ggsave(paste0(mydir,"/03-graphic-outputs/AFig3_WCsides.png"),width = 6, height = 4)

#plot with all 3 L vs R scatterplots together in a row.
allplot <- ggarrange(DLsides, RDTsides, WCsides, ncol = 3, nrow = 1,common.legend=TRUE)
ggsave(paste0(mydir,"/03-graphic-outputs/allLRbeh.png"),width = 9, height = 3)

```
Visualisations of the raw data from which laterality indices were computed, colour-coded by handedness, are shown for the three behavioural task in Figure `r fignumber`. The point type indicates whether the absolute individual z-lat score was greater than 1.96, indicating reliable lateralisation for that individual. Note that on the Dichotic Listening task there is inevitably a negative correlation between these totals, because on each trial the response is either left or right: hence, the more responses are 'left' the fewer are 'right', and vice versa. The black line shows the point of equality, where the number correct is the same for left and right. For this task, the points are distinguished for those where the proportion on one side is significantly different from .5 (filled circles), versus those who are equally likely to respond to L or R (crosses, which are bunched around the black line). It is evident from this plot that a high proportion of participants are significantly lateralised, and that overall the sample shows a right ear advantage, i.e. there are more points above the black line than below it. 

In Rhyme Decision, the task was to judge which of two pictured items had a name that rhymed with a centrally-presented written word. The pictures were presented in the left or right periphery. This was an easy task, as the words used were common, and the participants were given a practice session where they were introduced to all the pictures and their names. We could therefore use accuracy as an index of engagement with the task, and we excluded `r length(intersect(which(combdat$excludeRDT==1),which(combdat$lang2exclude==0)))` participants with accuracy of less than 75% correct. 

The dependent variable of interest was Response Time (RT), which was computed for correct responses in each half-field, after excluding outliers using a participant-specific algorithm. This involved taking the complete set of correct RTs for a participant, and applying the Hoaglin-Iglewicz (1987) criterion of outlier detection with cutoff set to 1.65 to remove unusually long RTs. In addition, any RTs less than 200 ms were excluded. A z-LI was then computed by computing a t-test for each participant, to compare the mean correct RT to left- and right-sided stimuli. Absolute values of z-LI greater than 1.96 can be regarded as evidence that an individual is significantly faster to one side than the other. Because RT is scaled so that high values correspond to poor performance, the z-LI was computed so that it was positive when left-sided RT was greater than right-sided RT. Thus those with a left-hemisphere advantage should cluster below the black line.


As with Rhyme Decision, Word Comprehension involved responding to laterally presented visual stimuli. The stimuli are pictures of semantically-related words, and the task is simply to respond to the picture whose name is spoken. Again, this is an easy task, where the focus is on RT of correct responses. Participants who made more than 75% errors on the task (N = `r length(intersect(which(combdat$excludeWC==1),which(combdat$lang2exclude==0)))`) were excluded from analysis.  The same method as for Rhyme Decision was used to remove outlier RTs participant by participant before computing a z-LI based on comparison of mean RT to left and right sides, where a positive value indicated faster RTs to stimuli presented in the right visual field.  


```{r DLdensities,echo=F,include=F}


plot1 <- ggplot(DLdat, aes(x = DL.LI, y = DL.zlat)) + 
  geom_point(shape = 4,  size = 1)+
  ggtitle("Dichotic Listening") 

  

dens1 <- ggplot(DLdat, aes(x = DL.LI, fill = Handedness)) + 
  geom_density(alpha = 0.4) + 
  geom_vline(xintercept = 0, linetype="dotted")+
  theme_void() + 
  theme(legend.position = "none")

dens2 <- ggplot(DLdat, aes(x = DL.zlat, fill = Handedness)) + 
  geom_density(alpha = 0.4) + 
  geom_vline(xintercept = 0, linetype="dotted")+
  theme_void() + 
  theme(legend.position = "none") + 
  coord_flip()

dens1 + plot_spacer() + plot1 + dens2 + 
  plot_layout(ncol = 2, nrow = 2, widths = c(4, 1), heights = c(1, 4))

DLdens <- ggsave(paste0(mydir,"/03-graphic-outputs/AFig4_DLdens.png"),width = 5, height = 4)
suppfignumber<- suppfignumber+1
```


### Laterality indices

__1. Dichotic Listening__  
In our pre-registration we stated that we would compute a conventional laterality index, 100* (Left-Right)/(Left+Right), as well as a laterality z-score:
(pL-.5)/sqrt(pL*pR/n).  We made one small modification, which was to flip the sign of these quotients, so that for all our laterality measures, left-hemisphere superiority is reflected in a positive score. This has no material effect on any computations, but gives better consistency with other research. For the laterality z-score, scores were censored at +/- 10, to avoid undue influence from a handful of extreme scores (participants who responded overwhelmingly to one ear).


As is evident from Supplementary figure `r suppfignumber`, the z-LI score is very highly correlated with the conventional LI, the principal difference being that the z-LI follows the normal distribution, with a sigmoid shape at the extremes (truncated in the Figure because of the censored scale). For subsequent analyses, we use z-LI, as this allows us to compare different tasks on a common scale.  

__2-3. Rhyme Decision and Word Comprehension__  
`r suppfignumber<- suppfignumber+1`Density plots and scattergrams for these two tasks are shown together in Supplementary Figure `r suppfignumber`, as both have z-LIs computed on the basis of correct RTs to visual stimuli presented to the left and right.  



```{r RD-WC-densities,echo=F,include=F}
#We will plot RD and WC together as they both involve z-LI from RTs

RDWCdat<-filter(mycomb,excludeRDT==0,excludeWC==0)

myx<-RDWCdat$RDT.zlat.ex
myy<-RDWCdat$WC.zlat.ex

mygroup <- RDWCdat$Rhanded
mydat<-as.data.frame(cbind(myx,myy,mygroup))
mydat$mygroup<-as.factor(mydat$mygroup)
namex<-'Rhyme Decision zLI'
namey<-"Word Comprehension zLI"
namegroup <- "Handedness"
grouplabels<- c("Left","Right")
mylines<-1 #diagonal line showing point of equality
mytitle <- 'Rhyme Decision vs Word Comprehension'
mycompositeplot <- dodensity(mydat,myx,myy,mygroup,namex,namey,namegroup,grouplabels,mylines,mytitle)

mycompositeplot
ggsave(paste0(mydir,"/03-graphic-outputs/AFig5_RDWCdens.png"),width = 5, height = 4)


```

Supplementary Figure `r suppfignumber` shows a suggestive trend for a bias to positive LI for the Rhyme Decision task, but for Word Comprehension, there is a more obvious bias in the opposite direction, with faster responses to stimuli presented in the left than the right visual half-field, i.e. points cluster below the line, indicating a right hemisphere advantage.  

### Preliminary analyses on online laterality measures  

Before testing specific predictions about interrelationships between measures, we conducted preliminary analysis on z-LI values for all three online tasks, to test for normality, to check for significant lateralisation in left- and right-handers, to compare laterality between handedness groups, and to compute split-half and test-retest reliability for laterality indices.  



```{r DL-ttests,echo=F, message=F, warning=F}
#Make a table to show characteristics of different tests

onlinesummary <- data.frame(matrix(NA,nrow=12,ncol=4))
colnames(onlinesummary)<-c('Statistic','Dichotic','Rhyme','Comprehension')
onlinesummary[,1]<-c('N','Mean (SD)','Skew','Kurtosis','Shapiro-Wilk normality','Mean (SD) L-hander','Mean (SD) R-hander','one-group t L-hander','one-group t R-hander','R-hander vs L-hander t','Split half r','Test-retest r (N = 53)')
```


```{r fillonlinesummary,echo=F, message=F, warning=F}



mytask<-'DL'
writecolnum <- 2 #column of online summary to write to for this task
onlinesummary <- populate(mycomb,mytask,onlinesummary,writecolnum,'beh')

mytask<-'RDT'
writecolnum <- 3 #column of online summary to write to for this task
onlinesummary <- populate(mycomb,mytask,onlinesummary,writecolnum,'beh')
onlinesummary[12,3]<-round(cor(sess1$RDT.zlat,sess3$RDT.zlat,use='complete.obs'),3)

mytask<-'WC'
writecolnum <- 4 #column of online summary to write to for this task
onlinesummary <- populate(mycomb,mytask,onlinesummary,writecolnum,'beh')
onlinesummary[12,4]<-round(cor(sess1$WC.zlat,sess3$WC.zlat,use='complete.obs'),3)

tabnumber <- tabnumber+1
ft<-flextable(onlinesummary)
ft<-autofit(ft)
ft

```

```{r behpirates, echo=FALSE, warning=FALSE,message=F}
#NB Formatting of this figure needs to be tweaked to achieve good resolution and legibility, but we can do that when we know what format figures need to be created in. 


 mypath<-paste0(mydir,"/03-graphic-outputs")
  plotname<-paste0(mypath,'/beh_pirates.jpg')
jpeg(plotname, width = 800, height = 500)

bLIdat <- combdat %>% 
  dplyr::select(ID, Rhanded, DL.zlat,RDT.zlat,WC.zlat)
colnames(bLIdat) <- c('ID','Handed','Dichotic','Rhyme','Comprehension')

bLIdat$Handed <- as.factor(bLIdat$Handed)
levels(bLIdat$Handed)<-c("L","R")
longdata.b <- pivot_longer(data = bLIdat, cols = c(3:5), names_to = 'Task', values_to = 'LI')
longdata.b$Task<-as.factor(longdata.b$Task)
longdata.b$Task <- factor(longdata.b$Task, levels = c('Dichotic', 'Rhyme', 'Comprehension'))
pirateplot(data = longdata.b, LI ~ Handed * Task)
abline(h=0)

title(main=paste0('Distributions of z-LI \non behavioural tasks; N = ',length(bLIdat$ID)))



```


As shown in Table `r tabnumber`, distributions of z-LI on the three tasks were non-normal, and the three tasks showed very different patterns of laterality. As expected from previous studies, on dichotic listening there was a clear right ear advantage in both left- and right-handers. In addition, there was a small but statistically reliable difference between handedness groups, with stronger laterality in the R handers.  We did not assess test-retest reliability for this task, as we had done this in our previous study with this task and found it to be high. Here we confirm excellent split-half reliability for this task. 

The Rhyme Decision task was far less reliable, with split half reliability of .432 and test-retest reliability of .539.  These figures indicate that laterality on this test is far from being at chance, but there is a great deal of random variation. In addition, although the task showed statistically reliable laterality in both left- and right-handers in this large sample, the effect size was small, and most individuals were not significantly lateralised. Furthermore, there was no effect of handedness on laterality on this task.

The Word Comprehension task did rather better in terms of reliability, with split half reliability of .664, though test-retest reliability was lower at .555. The striking observation about this task was that it showed a laterality bias in the opposite direction to what is usually seen in language tasks, with faster responses to pictures viewed in the left visual half-field, which projects directly to the right hemisphere. Furthermore, there was a significant effect of handedness, with the laterality index being more negative in left-handers than in right-handers.  



## Functional Transcranial Doppler measures

### Data Quality and Outliers for fTCD
``` {r dataqual, echo=FALSE, warning=FALSE}
#As pre-registered, we identify cases who have high values of SE for the LI, as this indicates the measurements are unusually variable from trial to trial
#We compute Qlimit based on Hoaglan-Iglewicz formula and exclude cases that exceed this value.
# Identify outliers
for (t in 1:6){
  SEcol <- which(colnames(combdat) == paste0(LETTERS[t], '_mean_se'))
  Q3<-quantile(combdat[ , SEcol],.75,na.rm=TRUE)
  Q1<-quantile(combdat[ , SEcol],.25,na.rm=TRUE)
  Qlimit<-Q3+2.2*(Q3-Q1)
  
# If there are at least 10 trials, include the data
  excludecol = which(colnames(combdat) == paste0(LETTERS[t], '_exclude'))
  trialscol = which(colnames(combdat) == paste0(LETTERS[t], '_N'))
  combdat[,excludecol] <- NA #initialise with NA
  combdat[which(combdat[ , trialscol] > 9), excludecol] <- 0
  combdat[which(combdat[ , trialscol] < 10), excludecol] <- 1
  
  # If the SE is too high, exclude the datatrials
  combdat[which(combdat[ , SEcol] > Qlimit) , excludecol] <- 1
}

# Count number of missing or excluded datapoints per task. 


# First set to NA those who did not do ftcd
n_excludeLI = matrix(data=NA, nrow=1, ncol=6)
for (t in 1:6){
  excludecol = which(colnames(combdat) == paste0(LETTERS[t], '_exclude'))
  n_excludeLI[t] = length(which(combdat[ , excludecol] > 0)) 
}

# Should any participants be excluded? If a participant has more than one excluded task, the participant is excluded entirely
combdat$DopExclude <- 0
tmp <- combdat$A_exclude + combdat$B_exclude + combdat$C_exclude + combdat$D_exclude + combdat$E_exclude + combdat$F_exclude
combdat$DopExclude[which(tmp > 1)] = 1
n_excluded = length(which(combdat$DopExclude == 1))

ddat<- filter(combdat,DopExclude==0,excluded==0,ftcd==1) #not really necessary to create this, but done for consistency with previous script : this has just those who did ftcd and who were not excluded on demographics or ftcd.

# We now count how many included participants omitted a single task
n_excludeLI = matrix(data=NA, nrow=1, ncol=6)
for (t in 1:6){
  excludecol = which(colnames(ddat) == paste0(LETTERS[t], '_exclude'))
  n_excludeLI[t] = length(which(ddat[ , excludecol] > 0)) 
}

```

We excluded `r n_excluded` participants who met our criteria for outliers on two or more fTCD tasks. For the remaining `r nrow(ddat)` participants, the numbers with missing data on the six tasks (A = Word Generation, B = Sentence Generation, C = Phonological Decision, D = Word Comprehension, E = Sentence Comprehension and F = Syntactic Decision) were `r n_excludeLI[1]`, `r n_excludeLI[2]`, `r n_excludeLI[3]`, `r n_excludeLI[4]`, `r n_excludeLI[5]` and `r n_excludeLI[6]` respectively.


<!--- Need to decide on colour scheme and stick to it-->
```{r selectplots,echo=F, warning=F,message=F}
tasknames <- c('Word generation','Sentence generation','Phonological decision','Word comprehension','Sentence comprehension','Syntactic decision')
ddat$Handed<-as.factor(ddat$Rhanded)
levels(ddat$Handed)<-c("Left","Right")
for (i in 1:length(tasknames)){
  col1<- paste0(LETTERS[i],"_mean_odd")
  col2<- paste0(LETTERS[i],"_mean_even")
  col3 <-paste0(LETTERS[i],"_mean_LI")
  c1<-which(colnames(ddat)==col1)
  c2<-which(colnames(ddat)==col2)
  c3<-which(colnames(ddat)==col3)
  h <- which(colnames(ddat)=='Handed')
  
  myfile <- ddat[,c(h,c1,c2,c3)]
  mycolnames <- c('Odd','Even','All')
  myrange=c(-5,6)
  p2<-doscatterplus(myfile,tasknames[i],mycolnames,myrange)
  p2
   mypath<-paste0(mydir,"/03-graphic-outputs")
  plotname<-paste0(mypath,'/OddEven_',LETTERS[i],'.png')
  ggsave(plotname,p2,width = 5, height = 4)

  
}

```



```{r LIpirates, echo=FALSE, warning=FALSE,message=F}
#NB Formatting of this figure needs to be tweaked to achieve good resolution and legibility, but we can do that when we know what format figures need to be created in. 

#Make task names that will print on 2 lines for compactness
tasknames2 <- c("Word\ngeneration","Sentence\ngeneration","Phonological\ndecision","Word\ncomprehension","Sentence\ncomprehension","Syntactic\ndecision")
#Now make text locations for these on pirate plot: we'll place A-C below plot and D-F above it
horizpts<-rep(1,6)
for (i in 1:6){
  horizpts[i] <- 1+(i-1)*3
}
vertpts <- rep(-5.5,6)
vertpts[4:6]<-6

 mypath<-paste0(mydir,"/03-graphic-outputs")
  plotname<-paste0(mypath,'/ftcd_pirates.jpg')
jpeg(plotname, width = 800, height = 500)

LIdata <- ddat %>% 
  dplyr::select(ID, Rhanded, A_mean_LI, B_mean_LI, C_mean_LI, D_mean_LI, E_mean_LI, F_mean_LI)
colnames(LIdata) <- c('ID','Handed','A','B','C','D','E','F')
LIdata$Handed <- as.factor(LIdata$Handed)
levels(LIdata$Handed)<-c("L","R")


longdata.d <- pivot_longer(data = LIdata, cols = c(3:8), names_to = 'Task', values_to = 'LI')
pirateplot(data = longdata.d, LI ~ Handed * Task,ylim=c(-6,8))
abline(h=0)

for (i in 1:6){
  text(horizpts[i], vertpts[i], tasknames2[i], #add label for task
     cex = .8)
}
#title(main=paste0('Distributions of LI Data \nN = ',length(LIdata$ID)))

dev.off()
fignumber<-fignumber+1
```
![Distributions of fTCD LIs on six tasks for 104 left-handers and 91 right-handers ](~/Dropbox/COLA_RR_Analysis/03-graphic-outputs/ftcd_pirates.jpg)

## LI Summary Statistics

The pirate plot in Figure `r fignumber` shows LI values for the six tasks (A = Word Generation, B = Sentence Generation, C = Phonological Decision, D = Word Comprehension, E = Sentence Comprehension and F = Syntactic Decision) for left and right handed participants. One sample t-tests were computed to test whether the group LI values differed significantly from zero, i.e. showed significant lateralisation. In left handers, tasks A, B and C were left lateralised; task D was right lateralised; and tasks E and F were not significantly lateralised. In right handers, tasks A, B, C and E were significantly left-lateralised.  Between-group t-tests were also computed to test whether lateralisation differed between left and right handers, using a one-tailed test as we had a directional prediction that R-handers would be more left-lateralised than L-handers. LI values were significantly stronger in the right handers than the left handers for all tasks except Word Comprehension.

Split-half reliability was computed for LI indices by computing the LI based just on odd or even trials, and taking the correlation between the two values. As can be seen in Table `r tabnumber`, values were generally between .73 and .83, except for Word Comprehension, where split-half reliability was only .58. 

```{r doppler-ttests,echo=F}
#Make a table to show characteristics of different tests


ftcdsummary <- data.frame(matrix(NA,nrow=11,ncol=7))
colnames(ftcdsummary)<-c('Statistic',LETTERS[1:6])
ftcdsummary[,1]<-c('N','Mean (SD)','Skew','Kurtosis','Shapiro-Wilk normality','Mean (SD) L-hander','Mean (SD) R-hander','one-group t L-hander','one-group t R-hander','R-hander vs L-hander t','Split half r')
```

```{r fill-ftcdsummary, echo=FALSE, warning=FALSE,message=F}
#We use the same function 'populate' to populate the data frame as we had for behavioural tasks - the 'ftcd' term at end of function call ensures correct columns are found
for (t in 1:6){
mytask<-LETTERS[t]
writecolnum <- 1+t #column of online summary to write to for this task

ftcdsummary <- populate(ddat,mytask,ftcdsummary,writecolnum,'ftcd')
#ftcdsummary <- ftcdsummary[1:(nrow(ftcdsummary)-1),]
}
ft<-flextable(ftcdsummary)
ft <- fontsize(ft, size = 10)
ft<-autofit(ft)
ft
tabnumber<-tabnumber+1
```
<!---### Checking normality of fTCD data
DB note: Can do this now in one step using the MVN function - this checks multivariate normality as well as normality of individual measures. I wrote some functions to fix measures that were not univariate normal; this involves the Box Cox function.  However, I then got concerned that fixing the non-normality might introduce undesirable other changes in the data- it did, for instance greatly increase the variance of just task E. I have analysed data with and without data normalisation, and it does not have much impact, so I think we could report main result without any changes, and just have alternative analysis available for those who want it. Importantly, the data don't seem wildly skewed or bimodal for any of the tasks (as can be seen from the density plots). But we could discuss this further - or perhaps just flag it up and see if reviewers want more on this.  Also, even after fixing univariate normality, we don't achieve multivariate normality. I think just describing these issues may be best way forward, rather than trying to fix everything and getting further and further from original data. __--->

Table `r tabnumber` shows basic statistics for the fTCD laterality indices, in the same format as for the online tasks. Right-handers showed significant left-lateralisation on Word Generation, Sentence Generation, Phonological Decision, and Sentence Comprehension, but were not lateralised for Word Comprehension or Syntactic Decision.  Left-handers were significantly left-lateralised for Word Generation, Sentence Generation and Phonological Decision, were not lateralised for Sentence Comprehension or Syntactic Decision, and were significantly right-lateralised for Word Comprehension.  The direct comparison between left- and right-handers showed significantly greater left-lateralisation on all tasks except Word Comprehension, which showed only a trend in that direction.

All tasks had split half reliability coefficients of .73 or above, except for Word Comprehension, where the coefficient was only .58.

Shapiro Wilk tests revealed significant non-normality for Sentence Generation, Phonological Decision and Sentence Comprehension. However, values of skewness and kurtosis were generally not extreme, and so for our initial data analysis we used untransformed data. We subsequently explored the impact of transforming LIs to normalise distributions, as described below. 

As noted above, there were a few participants with missing data on a single measure. Before running the SEM analysis, the _mice_ package @vanbuuren2011 was run in R to impute these missing values. 

```{r checkMVN, echo=FALSE, include=F,warning=FALSE,message=F}
#https://www.statisticssolutions.com/testing-normality-in-structural-equation-modeling/
#"In SEM, where your sample size is expected to be very large, this means that Mardia’s coefficient is almost always guaranteed to be significant. Thus, the significance test on its own does not provide very useful information. In light of this, it is recommended that the significance tests be used in conjunction with descriptive statistics, namely the kurtosis values for individual variables (Stevens, 2009). Kurtosis values greater than 3.00 in magnitude may indicate that a variable is not normally distributed (Westfall & Henning, 2013)."
#https://centerstat.org/can-i-estimate-an-sem-if-the-sample-data-are-not-normally-distributed/
#First, the assumption of normality is a characteristic of the estimator and not the model itself. So “the SEM” doesn’t assume normality, but the widely-used normal-theory maximum likelihood (ML) estimator does. Second, the assumption of normality applies to the residuals and is thus only relevant for dependent variables as defined in a given model; in contrast, the independent variables can take any distributional form at all (e.g., binary, count, bi-modal, long tail, etc.). Third, there are no well-defined numerical cut-offs for skew or kurtosis to determine whether a sample distribution is sufficiently non-normal to introduce problems in estimation, and tests of multivariate skew and kurtosis tend to be over-powered (significant even when the departure from normality is too slight to matter). Similarly, because the assumption of normality is on the residuals, the overall distributions of the observed variables are only indirectly indicative of the residual distributions. Nevertheless, in practice, we tend to examine histograms and scatter plots of the dependent variables to make a (somewhat subjective) determination of whether univariate and bivariate normality appear to be approximately satisfied.  NB recommends robust maximum likelihood methods.

#check multivariate normality
thisdat <- ddat[,c('A_mean_LI','B_mean_LI','C_mean_LI','D_mean_LI','E_mean_LI','F_mean_LI')]
pcut <- .01 #found normalisation by BoxCox created abnormally large variance for variable E - better work with non-normalised. Achieve that by just setting pcut very extreme, so no variable meets criterion for nonnormality
normed.dat<-normalisedf(thisdat,pcut)
#now select variables to be used in SEM analysis - will use the _bc versions if they exist. The collist in normed.dat gives the col numbers in correct order


thisdat <- normed.dat[[1]]
nunames<- c('A_P1','B_P2','C_P3','D_R1','E_R2','F_R3') #cols for SEM; will use _bc if it exists
thisdat[,nunames]<-thisdat[,normed.dat[[2]]]

#some checks on how the normalisation has worked.
normcheck <- 0 #Can set to 1 to see plots of data pre- vs post-normalisation
if(normcheck==1){
nc <- ncol(thisdat)
for (i in 1:6){
  plot(thisdat[,i],thisdat[,(nc-6+i)],xlab=colnames(thisdat)[i],ylab=colnames(thisdat)[(nc-6+i)])
  m1<-round(mean(thisdat[,i],na.rm=T),2)
  m2<-round(mean(thisdat[,(nc-6+i)],na.rm=T),2)
  text(-2,4,paste0(m1,': ',m2))
  abline(0,1,lty=2)
}
}

```



```{r imputemissing,echo=F, include=F}
#Interpolate missing values using mice package
thisdat.i <- mice(thisdat[,nunames], m=1, maxit = 50, method = 'pmm', seed = 500,print=FALSE)
ddati <-cbind(ddat[,c('ID','male','Handed')],complete(thisdat.i,1))


```





## Hypothesis 1: Testing a two-factor model using behavioural data

__The pattern of correlation between laterality indices from online measures will reflect the extent to which they involve implicit speech production, rather than whether they involve spoken or written language. Thus we anticipate dissociation between the rhyme judgement task and the other two measures (dichotic listening and OVP task), which is not accountable for in terms of low reliability of measures.__ 

We had planned to do a formal comparison of model fit using AIC weights, but we realised our data were inadequate for this because our Model A was, in formal terms, just-identified: it simply estimated three pairwise correlations from the data, and always gave perfect fit, regardless of the size or direction of correlations. We considered alternative approaches to the analysis, but decided to just report the correlations at this stage, as the pattern of results was distinctive, and we had already planned to incorporate the online behavioural measures into the SEM analysis that includes the fTCD measures (see section x below).  




```{r behavcorrs,echo=F,warning=F,include=F}


bivdat <- filter(mycomb,excludeRDT==0,excludeDL==0,excludeWC==0)
#Assign Group at random
set.seed(50) #make reproducible
bivdat$Group<-1+rbinom(nrow(bivdat),1,.5)
#Check handedness distribution
handgrouptab <- table(bivdat$Handed,bivdat$Group) #This is just to confirm roughly equal distribution of L and R handers in the 2 random groups.


#tempx and tempy are reassigned to variables of interest before calling generic function that will base plot on these two variables
bivdat$tempx <- bivdat$DL.zlat
bivdat$tempy <- bivdat$RDT.zlat
name1 <-"Dichotic Listening z-lat"
name2 <- "Rhyme decision z-lat"
DL_RD_plot <- bivplot2(bivdat,name1,name2) #this is our specially created function
#I've commented out saving the individual plots, just because I've used ggarange to make a composite plot with all 3 pairings
#ggsave(paste0(mydir,"/03-graphic-outputs/DL-RDT.png"),width = 6, height = 4)

bivdat$tempy <- bivdat$WC.zlat
name2 <- "Word Comprehension z-lat"
DL_WC_plot <- bivplot2(bivdat,name1,name2)
#ggsave(paste0(mydir,"/03-graphic-outputs/DL-WC.png"),width = 6, height = 4)

bivdat$tempx <- bivdat$RDT.zlat
name1 <- "Rhyme Decision z-lat"
RD_WC_plot <- bivplot2(bivdat,name1,name2)
#ggsave(paste0(mydir,"/03-graphic-outputs/RD-WC.png"),width = 6, height = 4)

#For now am making a plot with all 3 scatterplots together in a row. Easy to change layout if needed.
allplot <- ggarrange(DL_RD_plot, DL_WC_plot, RD_WC_plot, ncol = 3, nrow = 1,common.legend=TRUE)
ggsave(paste0(mydir,"/03-graphic-outputs/allbiv.png"),width = 9, height = 3)
fignumber<-fignumber+1
```
![Bivariate distributions of LIs on behavioural tasks](~/Dropbox/COLA_RR_Analysis/03-graphic-outputs/allbiv.png) 

Figure `r fignumber` shows scatterplots of the bivariate relationships between the three variables. It is evident from inspection that we can reject model C, in which all three LIs are independent, and model B1, where only Dichotic Listening and Rhyme Decision are correlated. The strongest correlation is between the two visual tasks, Rhyme Decision and Word Comprehension, as predicted by model B2. 

Note that correlations will be influenced by test reliability. Indeed, the correlation between Rhyme Decision and Word Comprehension is close in magnitude to the split half reliability of the two measures. An estimate of the association between these measures after adjusting for the split-half reliabilities can be obtained using the Spearman-Brown correction for attentuation, r.xy(corrected) = r.xy(observed)/sqrt(r.xx * r.yy), which gives a value of `r round(.44/sqrt(.436*.664),3)`.


## Hypothesis 2: Testing a two-factor model using fTCD data

Our second preregistered prediction was: __The data will fit a model where 'language generation' tasks cluster together on one factor, and 'receptive language’ tasks on a second factor.__  

It was further predicted that factors will be correlated, but the fit of a 2-factor model will be superior to a single-factor model where all LIs load on a common factor.
 
The analysis conducted by Woodhead et al (2019, 2020) used an exploratory bifactor model in which each task could load on each of two factors. Because there were two measures for each task (from test and retest sessions), this exploratory approach was adequately powered. In the current study, some of the tasks were different, and we only had one measurement occasion for each of the six measures. Accordingly we used confirmatory factor analysis, using a prespecified two-factor model that constrains which indicators can load on two factors. This was compared to a unitary model, in which all tasks load on a single factor.  

`r fignumber <- fignumber+1`Figure `r fignumber` shows the pattern of correlations between LIs for the different tasks as a heat map. The two-factor model predicts that correlations will form two clusters, with positive correlations within tests A-C, and within tests D-F, but weaker or absent correlations across these two clusters of measures. The heatmap shows moderate correlations within both clusters of measures, and generally lower correlations across clusters, but there are some exceptions. Notably, there is a moderate correlation between task B (Sentence generation) and task E (Sentence comprehension), which is not predicted by the two-factor model. 
_ZW: should this text say task C (Phonological Decision) and task E (sentence comprehension), R=.665?_

```{r modelfit,echo=F,include=F}

#nb we will use the variables from ddati
set.seed(50)

ddati$randgroup<-1+rbinom(nrow(ddati),1,.5) #create random group 1 or 2 for later split

#Add correlation matrix

LIcols <- c("A_P1","B_P2","C_P3","D_R1","E_R2","F_R3")

myheatmap <- makeheatmap(ddati,LIcols)

#Saved heatmap needs a bit of tweaking! Size of axis labels and grey background need fixing
ggsave(paste0(mydir,"/03-graphic-outputs/ftcd-heatmap.png"),width = 6, height = 6)

```
![Heatmap showing correlations between laterality indices from six fTCD tasks](~/Dropbox/COLA_RR_Analysis/03-graphic-outputs/ftcd-heatmap.png) 

__Note from DB: I am planning on putting in quite a detailed explanation of SEM as it not widely used in this field. This is a bit of a placeholder for now.__

### Summary of the Structural equation modeling approach  
Because Structural equation modeling (SEM) is not widely used in laterality research, we provide here a brief explanation, to aid interpretation of the subsequent analysis.  

Structural equation modeling (Kline, 2011) is a method that allows a formal test of adequacy of competing models for explaining patterns of association between variables. The underlying assumption of this approach is that observed variables can be treated as indicators of underlying, unobserved latent variables. `r fignumber<-fignumber+1`Figure `r fignumber` shows the single factor model on the left, and a two-factor model on the right. The latent factors are shown in ovals, and the observed variables in boxes. Single-headed arrows indicate causal paths, and double-headed arrows indicate variances. Although means can be incorporated in SEM (and we shall be doing this in our analysis), the main use of SEM is to analyse patterns of covariances. The important point to note is that the path diagrams shown in Figure 1x have a precise mathematical interpretation, and can be converted into linear equations that specify the covariances between observed variables. Thus it is possible to obtain a measure of goodness of fit for observed data in relation to a model by comparing whether the observed covariances agree with those predicted by the model. We can already see by inspecting the heatmap of Figure `r fignumber-1` that a single-factor model is unlikely to provide a good fit to the observed data, because it would not predict the clustering of correlations that is evident. 

<!---Diagram could be done better!--->
![One-factor vs two-factor structural equation model](~/Dropbox/COLA_RR_Analysis/03-graphic-outputs/path_diags_asppt.jpg)

SEM does not arrive at a single algebraic estimation of model fit, but rather uses a maximum likelihood approach, whereby values for the paths from the factors to the observed variables are first assigned starting values, and the expected covariances between variables are computed with these values, and then compared to observed covariances. This process is iterated many times with different path estimates, with an algorithm adjusting paths on each run to reduce the mismatch between observed and expected covariances. 

It may be noted that the path diagrams shown in Figure `r fignumber` include one path to each factor shown as a dotted line. This is a fixed parameter, set to 1, which is used to scale the estimates. All the other paths are free to vary, and the estimation process will consider different values, to converge on a solution that gives the best fit. Some paths may have little impact on the solution, and may be dropped without any deterioration of fit. 

There is no single method for evaluating the fit of a model to observed data. A chi square test can given an estimate of the extent of departure of a observed values from expectation: a good model is one where chi square is small and has a high associated p-value, indicating that any difference between expectation and observation is likely to just reflect sampling error. However, it is usually possible to improve fit by including additional paths or factors in a model until good fit is achieved, but this does not mean that the model is better: the goal is rather to obtain a parsimonious and theoretically meaningful model that does not include arbitrary parameters that are specified solely to fit the data. Where models are 'nested', with a more complex model including all the parameters of a base model, then model fit can be compared by subtraction of the chi square and degrees of freedom for the two models; the difference in chi square is then evaluated, and can indicate whether the simpler model gives as good a fit as a complex model with more parameters - in which case the simpler model is preferred.

Other indices have been developed that penalise models with a large number of parameters. The Comparative Fit Index (CFI) measures relative improvement of fit of a model relative to a model that assumes independence of all variables. CFI values of .95 or more are conventionally regarded as indicating acceptable fit. Another measure is the Root Mean Square Error of Approximation (RMSEA) is a 'badness of fit' measure, where a value of zero indicates good fit, and values below .05 are widely regarded as indicating acceptable fit. We report here values for chi square, CFI and RMSEA. 
 


```{r initialisebigsummary,echo=F}
#Initialise a table to show factors loadings and some other stuff (CFA, rmsea) for each model in a column, so we can compare them
bigsummary <- data.frame(matrix(NA,nrow=19,ncol=8))
colnames(bigsummary)<-c('Estimate','Model.1F','Model.2F','Model2Fn.1','Model2Fn.2','Model2Fn','Model2Fn.L','Model2Fn.R')
bigsummary[,1]<-c('NObs','A -> Fac1','B -> Fac1','C -> Fac1','D -> Fac1','E -> Fac1','F -> Fac1','A -> Fac2','B -> Fac2','C -> Fac2','D -> Fac2','E -> Fac2','F -> Fac2','Fac1~~Fac2','CFI','rmsea','chisq','DF','p vs Model.1F')
```





```{r factormodels,echo=F,include=F, warning=F,message=F}
#In lavaan, we first define the factor model between quotes
#So this step doesn't do anything - just sets up the model to be run later

#This is definition of single factor model we will use here. A_P1 will be index variable with path of 1. 
model.1F <- 'f1 =~  A_P1 + B_P2  + C_P3 +D_R1 + E_R2 + F_R3' 


#2 factor production/reception model
model.2F <- '
f1 =~  A_P1 + B_P2+C_P3
f2 =~ F_R3 + D_R1 + E_R2  #2 factor model: 
#covariance unspecified, which means there is no constraint on covariance

'


fit1 <- cfa(model.1F, estimator="WLSMV",data=ddati) #runs the model and saves results in fit1
sfit1 <- makeSEMtab(fit1) #saves the results from the model in a neat format
bigsummary<- addmodel(bigsummary,fit1,NA,'Model.1F',writecol=2) #summary from this model written to col 2 of bigsummary. For single factor model we don't specify a comparison model, hence NA.

#lavResiduals(fit1) #if we want to understand reasons for poor fit, we can look at residuals - shows size of covariances that aren't explained by model.

#Creates a structural diagram for single factor model: nb does NOT include path estimates (these can be shown if we put 'par' rather than 'diagram', but it gets messy, and the parameters are shown instead in a table)
#lots of details of this here
#https://www.rdocumentation.org/packages/semPlot/versions/1.1.2/topics/semPaths

pathfigname<-paste0(mydir,"/03-graphic-outputs/pathfigF1")
semPaths(fit1, "diagram", weighted = FALSE,  shapeMan = "rectangle", sizeMan = 8, 
    sizeMan2 = 5,filetype='jpg',filename=pathfigname,width=3,height=2) #draws a path diagram

fit2 <- cfa(model.2F, estimator="WLSMV",data=ddati)
sfit2 <- makeSEMtab(fit2)
bigsummary<- addmodel(bigsummary,fit2,fit1,'Model.2F',writecol=3) #summary from this model written to col 3 of bigsummary

#lavResiduals(fit2)
pathfigname<-paste0(mydir,"/03-graphic-outputs/pathfigF2")
semPaths(fit2, "diagram", weighted = FALSE,  shapeMan = "rectangle", sizeMan = 8, 
    sizeMan2 = 5,filetype='jpg',filename=pathfigname,width=3,height=2) #draws a path diagram
#anova(fit1,fit2) #compares model fit -if significant, means 2nd model is better fit than 1st. This information is now included in bigsummary.


#I checked whether the one-factor model would fit better if we specified a 2-group solution with handedness groups; it did not
#measurementInvariance(model=model.1F,estimator="WLSMV",data=ddati,group="Handed")
#CFI very low.

#Also true with the 2F model: fit is better but still well below acceptable on CFI and RMSEA/ Also gives negative variance estimates.
#measurementInvariance(model=model.2F,estimator="WLSMV",data=ddati,group="Handed")

```

```{r trysemproducible,echo=F}
# semproducible is intended to give you output in a format suitable for adding to journal.
# Currently not used. 
# Reproduce lavaan model.
#code <- semproducible(fit1, ,model.1F)
 #cat(code)

```

We used the lavaan() package to perform the pre-registered model comparison. To take into account non-normality of some variables, the WLSMV estimator was specified; this uses weighted least squares with robust standard errors and a mean- and variance adjusted test statistic, and makes no distributional assumptions about the observed variables. Table z summarises the main output of the model-fitting. The fit of both the one-factor and the two-factor model is poor. 

Therefore, as planned we divided the sample into two random subsamples, 1 and 2. The first subsample is used in an exploratory analysis. We start with tasks A-E linked to Factor 1 and tasks B-F linked to Factor 2, and then drop non-significant paths. This gives a revised model with considerably improved fit, as shown in Figure x, where tasks A, B, C, and E load on Factor 1, and tasks C, D, E and F load on Factor 2. 

This model is then run using data from subsample 2. Table x shows that the fit is actually better for subsample 2 than subsample 1, but it also indicates how model fit can vary with samples of this size. Table x also shows the fit of the revised 2-factor model for the full sample. The model fit falls short of meeting criteria for a good fit, though it is much better than for the previous model. Given the limitations of the dataset, we concluded this is likely to be the optimal model that can be achieved, and so we proceeded to test whether the same model was appropriate for left- and right-handers.  




```{r newmodel,echo=F,include=F}
#We explore for best model using just half the data (randgroup = 1)
mygroup <-1
mydat<- ddati[ddati$randgroup==mygroup,]
model.2Fall <- '
f1 =~  A_P1 + B_P2+C_P3+D_R1+ E_R2
f2 =~  F_R3 + B_P2+ C_P3 +D_R1+ E_R2 
'
fit.2Fall.1 <- cfa(model.2Fall,estimator="WLSMV", data=mydat)
tab.2Fall <- makeSEMtab(fit.2Fall.1)
wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2Fall.1,wantfits)

#This gives impossible values with negative variance estimate

#Model that is based on Woodhead et al
model.2FZW <- '
f1 =~  B_P2+A_P1 +C_P3+D_R1+ E_R2+F_R3
f2 =~  NA*C_P3 +D_R1+ E_R2 +F_R3+A_P1
f1~~1*f1
'

fit.2FZW.1 <- cfa(model.2FZW,estimator="WLSMV", data=mydat)
tab.2FZW <- makeSEMtab(fit.2FZW.1)
wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2FZW.1,wantfits)
#This gives nonfunctional model: cannot invert matrix plus negative variance estimates.

#make oddeven file so can try same model as Woodhead et al
wantcols <- c("ID","male","Rhanded","A_mean_odd","A_mean_even","B_mean_odd","B_mean_even","C_mean_odd","C_mean_even","D_mean_odd","D_mean_even","E_mean_odd","E_mean_even","F_mean_odd","F_mean_even","DopExclude")
ddat_OE <- combdat[combdat$ftcd==1,wantcols]
ddat_OE<-ddat_OE[ddat_OE$DopExclude==0,]

#impute missing values using mice package
nunames<-wantcols[4:15]
thisdat.j <- mice(ddat_OE[,nunames], m=1, maxit = 50, method = 'pmm', seed = 500,print=FALSE)
ddat_OE <-cbind(ddat_OE[,1:3],complete(thisdat.j,1))

colnames(ddat_OE)<-c("ID","male","Rhanded","A_o","A_e","B_o","B_e","C_o","C_e","D_o","D_e","E_o","E_e","F_o","F_e")
set.seed(50)

ddat_OE$randgroup<-1+rbinom(nrow(ddat_OE),1,.5) #create random group 1 or 2 for later split
mygroup <-1
mydat<- ddat_OE[ddat_OE$randgroup==mygroup,]

#Model that is same as Woodhead et al
model.2FZW2 <- '
f1 =~  1*B_o+equal("f1=~B_o")*B_e+  #same path value for B_o and B_e
       a*A_o+a*A_e +
       c*C_o+c*C_e+
       d*D_o+d*D_e+
       e*E_o+e*E_e+
       f*F_o+f*F_e
f2 =~  NA*D_o+equal("f2=~D_o")*D_e+ #same path value for D_o and D_e
       e2*E_o+e2*E_e+
       f2*F_o+f2*F_e+
       a2*A_o+a2*A_e +
       c2*C_o+c2*C_e  #only B is omitted from f2

f2~~1*f2
A_o~~av*A_o #equate variances for odds and evens
A_e~~av*A_e
B_o~~bv*B_o
B_e~~bv*B_e
C_o~~cv*C_o
C_e~~cv*C_e
D_o~~dv*D_o
D_e~~dv*D_e
E_o~~ev*E_o
E_e~~ev*E_e
F_o~~fv*F_o
F_e~~fv*F_e
'

fit.2FZW2.1 <- cfa(model.2FZW2,estimator="WLSMV", data=mydat)
tab.2FZW2 <- makeSEMtab(fit.2FZW2.1)
wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2FZW2.1,wantfits)
#This gives matrix not positive definite

#Paths suggest dropping nonsig paths from f1 to D and F 
# and A from F2 (B is already missing as was excluded from model)
model.2FZW2a <- '
f1 =~  NA*B_o+equal("f1=~B_o")*B_e+
       a*A_o+a*A_e +
       c*C_o+c*C_e+
       e*E_o+e*E_e
f2 =~  NA*D_o+equal("f2=~D_o")*D_e+ 
       c*C_o+c*C_e+
       e2*E_o+e2*E_e+
       f2*F_o+f2*F_e
f1~~1*f1
f2~~1*f2
A_o~~av*A_o #equate variances for odds and evens
A_e~~av*A_e
B_o~~bv*B_o
B_e~~bv*B_e
C_o~~cv*C_o
C_e~~cv*C_e
D_o~~dv*D_o
D_e~~dv*D_e
E_o~~ev*E_o
E_e~~ev*E_e
F_o~~fv*F_o
F_e~~fv*F_e
'

fit.2FZW2a.1 <- cfa(model.2FZW2a,estimator="WLSMV", data=mydat)
tab.2FZW2a <- makeSEMtab(fit.2FZW2a.1)
wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2FZW2a.1,wantfits)
#Converges nicely

#More sense to make changes to model based on observation of heatmap for group 1.
#Low correlations between D and F & Fac1 and path A and B & Fac2 (v similar to heatmap for whole sample). We can drop these

#After checking model from group 1, now try running with 2nd group and with full sample

model.2Fn <- '
f1 =~  A_P1 + B_P2+C_P3+ E_R2
f2 =~  F_R3 + C_P3+D_R1+ E_R2   #2 factor model: no constraint on covariance
'

fit.1F.1 <-cfa(model.1F, estimator="WLSMV",data=mydat)
fit.2Fn.1 <- cfa(model.2Fn, estimator="WLSMV",data=mydat)
tab.2Fn.1 <- makeSEMtab(fit.2Fn.1)
fitmeasures(fit.2Fn.1,wantfits)

bigsummary<- addmodel(bigsummary,fit.2Fn.1,fit.1F.1 ,'Model.2Fn.1',writecol=4) #summary from this model written to col 4 of bigsummary - becauase different subsample, can't compare with prior model in bigsummary


#model.2Fn is best-fitting model from exploration with group 1.


#After checking model from group 1, now try running with 2nd group and with full sample
#It's less good with group 2 but not disastrous, but it does give negative variance estimates for B for group 2, which I can't get to the bottom of.

mydat<- ddati[ddati$randgroup==2,] #explore model just with group 1
fit.2Fn.2 <- cfa(model.2Fn, estimator="WLSMV",data=mydat)
fit.1F.2 <- cfa(model.1F,estimator="WLSMV", data=mydat)
bigsummary<- addmodel(bigsummary,fit.2Fn.2,fit.1F.2 ,'Model.2Fn.2',writecol=5)

mydat<-ddati #full sample
fit.2Fn <- cfa(model.2Fn, data=mydat)
bigsummary<- addmodel(bigsummary,fit.2Fn,fit1,'Model.2Fn',writecol=6) #summary from this m


pathfigname<-paste0(mydir,"/03-graphic-outputs/pathfigF2_revised")
semPaths(fit.2Fn, "diagram", weighted = FALSE,  shapeMan = "rectangle", sizeMan = 8, 
    sizeMan2 = 5,filetype='jpg',filename=pathfigname,width=3,height=2) #draws a path diagram

#look at residuals for diagnostics
res<-resid(fit.2Fn, type = "cor") #large abs values (> .1) indicate relationships not well accounted for

#Now with L and R hander
mydat<- ddati[ddati$Handed=='Left',] #explore model just with group 1
fit.2Fn.L <- cfa(model.2Fn, estimator="WLSMV",data=mydat)
fit.1F.L <- cfa(model.1F, estimator="WLSMV",data=mydat)
bigsummary<- addmodel(bigsummary,fit.2Fn.L,fit.1F.L,'Model.2Fn.L',writecol=7) #summary from this m

mydat<- ddati[ddati$Handed=='Right',] #explore model just with group 1
fit.2Fn.R <- cfa(model.2Fn, estimator="WLSMV",data=mydat)
fit.1F.R <- cfa(model.1F, estimator="WLSMV",data=mydat)
bigsummary<- addmodel(bigsummary,fit.2Fn.R,fit.1F.R,'Model.2Fn.R',writecol=8) #summary from this m


#Can't get 2nd col to reformat - need to convert to character - then OK
bigsummary[,2]<-as.character(bigsummary[,2])
flextable(bigsummary)

```

```{r makefacscatter,echo=F}
myscatter <- function(myx,myy,xlabel,ylabel,thisdat){
thisscatter<-ggplot(thisdat, aes(x = myx, y = myy,color=Handed)) + 
  geom_point(shape = 4,  size = 1)+
 scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
    xlab(xlabel)+
    ylab(ylabel)+
  geom_hline(yintercept=0, linetype="dashed")+
    geom_vline(xintercept=0, linetype="dashed")
 #NB scale of factors is NOT centred on LI of zero!
  return(thisscatter)
}

```




 


```{r plotfacs, echo=F}
#Creates a plot and saves it; also adds factor scores to ddati
ddati <- makefactorplot(cfa(model.2Fn, estimator="WLSMV",data=ddati),'fit.2Fn',ddati)


```




## Model equivalency for left- and right-handers  
<!---Level of measurement equivalency are assessed through model fit of a series of nested multiple group models.  
Substantial decrease in goodness of fit indicates non-invariance
Xu: It is a good practice to look at several model fit indices rather than relying on a single one
• Δχ2
• ΔRMSEA
• ΔCFI
• ΔTLI
• ΔBIC
• ΔAIC  


Step 1: Configural invariance
  Same factor structure in each group
  First, fit model separately in each group
  Second, fit model in multiple group but let all parameters vary freely in each group
  No latent mean difference is estimated
  
  
Step 2: Weak/metric invariance
  Constrain factor loadings equal across groups
  This shows that the construct has the same meaning across groups
  No latent mean difference is estimated
  
Step 3: Strong/scalar invariance
  Constrain item intercepts equal across groups
  Constrain factor loadings
  This is important for assessing mean difference of the latent variable across groups
  Latent mean difference is estimated
  
Step 4: Strict invariance
  Constrain item residual variances to be equal across groups
  Constrain item factor loadings and intercepts equal across groups. 
  Strict invariance is important for group comparisons based on the sum of observed item scores, because observed variance is a combination of true score variance and residual variance
  Latent mean difference is estimated --->
  


```{r measurementinvariance,echo=F,include=F}
#https://towardsdatascience.com/measurement-invariance-definition-and-example-in-r-15b4efcab351


#library(semTools) fits increasingly restrictive models in one command

#same model in different syntax to make it easier to interpret

model.2Fn <- '
f1 =~  A_P1 + B_P2+C_P3+ E_R2
f2 =~  F_R3 + C_P3+D_R1+ E_R2   #2 factor model: no constraint on covariance
'


measurementInvariance(model=model.2Fn,estimator="WLSMV",data=ddati,group="Handed")
#This gives message to say command is deprecated.
#It gives substantial change to model fit for the final fit.means model.


#NB makeSEMtab2 is a function defined at top of script - just makes it easier to compare parameters across groups

fitM0 <- cfa(model.2Fn,estimator="WLSMV", data = ddati,meanstructure=T)
tabM0 <- makeSEMtab(fitM0)  #

# configural invariance
fitM1 <- cfa(model.2Fn, estimator="WLSMV",data = ddati,meanstructure=T, group = "Handed")

tabM1 <- makeSEMtab2(fitM1,c('_L','_R'))  #in tab1, everything varies for the 2 groups

# weak invariance
fitM2 <- cfa(model.2Fn, ddati, estimator="WLSMV",meanstructure=T,group = "Handed",
            group.equal = "loadings")
tabM2 <- makeSEMtab2(fitM2,c('_L','_R'))
#in tab2, the factor loadings are same for the 2 groups, but everything else can vary


# strong invariance
fitM3 <- cfa(model.2Fn, ddati, estimator="WLSMV",meanstructure=T,group = "Handed",
            group.equal = c("intercepts", "loadings"))
tabM3 <- makeSEMtab2(fitM3,c('_L','_R'))
#THis does have same intercepts and loadings, but the correlation between f1/f2 and the variances of each factor can differ, as well as the residuals.

#strong invariance with covariance between factors too
fitM3a <- cfa(model.2Fn, ddati,estimator="WLSMV",meanstructure=T, group = "Handed",
            group.equal = c("intercepts","loadings","lv.covariances"))
tabM3a <- makeSEMtab2(fitM3a,c('_L','_R'))
# This is not included in usual hierarchy of models but seems crucial for our hypothesis, which assumes the two factors have equivalent correlation in L and R handers. NB including this at step 2 gives error re standardized measures

fitM4 <- cfa(model.2Fn, ddati, estimator="WLSMV",meanstructure=T,group = "Handed",
 group.equal = c("loadings","intercepts","means","lv.covariances"))

tabM4 <- makeSEMtab2(fitM4,c('_L','_R'))

# model comparison tests
mylavtest<-lavTestLRT(fitM1, fitM2, fitM3,fitM3a, fitM4)

#see https://users.ugent.be/~yrosseel/lavaan/multiplegroup6Dec2012.pdf

```

Table x shows the fit of the revised 2-factor model in each handedness group separately. It can be seen that the model fit is good in both groups, but the size of the estimates shows moderate variability. Our next preplanned analysis was designed to test whether such variability in model parameters, or whether it could be explained by sampling error. 

The approach we adopted is a standard one used when structural equation modeling (SEM) is applied to evaluation of measurement models in other domains, where it is described as a test of measurement invariance. Essentially, the data from left- and right-handers are analysed together in a series of nested models; these pose increasingly stringent constraints on which parameters of the model are allowed to vary. Initially, a model is fit in which all the paths, covariances, and intercepts are free to differ between left- and right-handers (as shown in the last two columns of Table x, where data from the two groups were fitted separately). This model is tested against a model of 'metric invariance', which sets the loadings from each observed variable to the factors to be the same for the two groups. If the fit of the model does not worsen, we can assume the basic model structure is equivalent for the two groups. This test of equivalence was passed (see Table x2).

At the next step, (scalar invariance), the item intercepts are set to be the same across groups. Once again, the model fit did not worsen (see Table x3).

In a final step (strict invariance), we constrain item residual variances as well as factor loadings and intercepts to be equal across groups. Here we obtained a substantial worsening of model fit, indicating that the mean difference between groups on the latent factors is not the same. This is as we would expect, given the substantial differences seen in mean factor scores of left- and right-handers (Figure x).

In sum, results from the measurement invariance test show that the same underlying structural model can be assumed to apply for both left- and right-handedness, with the differences between handedness groups being explained solely in terms of differences in means, rather than in covariances between the six LIs. 

# Prediction 4
__On categorical analysis, individuals who depart from left-brained laterality on one or more tasks will be more likely to be left-handed than those who are consistently left-lateralised.__

The analysis so far has treated laterality as a continuum, but this continuum does have a zero-point, and negative scores indicate right-lateralisation and positive scores left-lateralisation. There are theoretical reasons to suppose that brain function might be influenced more by consistency in direction of lateralisation, than by degree. Thus, regardless of how strong or weak a laterality index is, brain functioning might be more efficient if all language functions are represented in the same hemisphere. 

As stated in our preregistration, we first adopted the simple approach of dichotomising laterality at a cutoff of zero for each task, and then performed a chi square analysis to test for association with handedness. For 6 measures, we adopted a Bonferroni-corrected alpha level of .02/6 = .003. 

```{r categorical-assignment, echo=F}
mycols<-paste0(LETTERS[1:6],'_mean_LI')
for (i in 1:6){
  c<-which(colnames(ddat)==mycols[i])
  myvector<-rep(NA, nrow(ddat))
  w<-which(ddat[,c]>0.000000001)
  myvector[w]<-1
  w<-which(ddat[,c]<0)
  myvector[w]<-0
  ddat[,(1+ncol(ddat))]<-myvector
  colnames(ddat)[ncol(ddat)] <- paste0(LETTERS[i],'_catlatL')
}
```

```{r category-analysis,echo=F}
chidf <- data.frame(matrix(NA,nrow=6,ncol=5))
colnames(chidf) <- c('Task','pL_Lhander','pL_Rhander','chisq','p')
nucols <- paste0(LETTERS[1:6],'_catlatL')
  myL <- filter(ddat,Handed=='Left')
  myR <- filter(ddat,Handed=='Right')
for (i in 1:6){
    c<-which(colnames(ddat)==nucols[i])
 
  myt<-table(ddat$Handed,ddat[,c])
  chitab<-chisq.test(myt)
  chidf[i,1]<-LETTERS[i]
  chidf[i,2]<-round(sum(myL[,c],na.rm=T)/nrow(myL),3)
  chidf[i,3]<-round(sum(myR[,c],na.rm=T)/nrow(myR),3)
  chidf[i,4]<-round(chitab$statistic,2)
  chidf[i,5]<-pformat2(chitab$p.value)

}
  ft<-flextable(chidf)
  autofit(ft)
  
  chidf$diffp<-chidf$pL_Rhander-chidf$pL_Lhander
```
Results are shown in Table `r tabnumber`.  The trend is similar for all six tasks, with the proportion who are left-lateralised averaging at 16% lower in left-handers than in right-handers, regardless of the mean LI for the task. The difference ranged from 11% for Syntactic Decision to 24% for Syntactic Comprehension, but met our prespecified significance criterion only for Syntactic Comprehension.

After testing associations for individual measures, we categorised individuals as either consistently left-lateralised on all tests, or right-lateralised on one or more tests. The proportions of left- and right-handers who are left-lateralised on between 0 and 6 tests is shown in table `tabnumber`. 

__Table `tabnumber`: Proportions of left- and right-handers with between 0 and 6 tasks left-lateralised on fTCD.__    




```{r ntestL,echo=F}
ddat$Nleft <- ddat$A_catlatL+ ddat$B_catlatL+ddat$C_catlatL+ddat$D_catlatL+ddat$E_catlatL+ddat$F_catlatL

tabN <- table(ddat$Nleft,ddat$Handed)
tabNp<-round(prop.table(tabN,2),3)
ntestL <- as.data.frame(cbind(0:6,tabNp[,1],tabNp[,2])) #N tests that are L lateralised
colnames(ntestL)<-c('N tasks L lateralised','L-handers','R-handers')

ft<-autofit(flextable(ntestL))
ft
tabnumber<-tabnumber+1

chitab<-matrix(c(sum(tabN[1:6,1]),sum(tabN[1:6,2]),tabN[7,1],tabN[7,2]),nrow=2)
mychi<-chisq.test(chitab)



#Some exploratory analyses folllow
#repeating just for factor 1
ddat$NleftABCE <- ddat$A_catlatL+ ddat$B_catlatL+ddat$C_catlatL+ddat$E_catlatL

tabN4 <- table(ddat$NleftABCE,ddat$Handed)

ntestL_F1 <- round(prop.table(tabN4,2),3)
chitab4<-matrix(c(sum(tabN4[1:4,1]),sum(tabN4[1:4,2]),tabN4[5,1],tabN4[5,2]),nrow=2)
mychi4<-chisq.test(chitab4)



#repeating with L handers subdivided into extreme or moderate 
ddat$hand3cat <- ddat$Rhanded
ddat$hand3cat[ddat$EHI.LI<(-90)]<--1

tabN4a <- table(ddat$NleftABCE,ddat$hand3cat)
ntestL_F1_LHs<-round(prop.table(tabN4a,2),3)



```
It is evident from Table `r tabnumber` that a minority of individuals are consistently left-lateralised on all six tasks, regardless of handedness. Although the trend is for more right-handers to show this pattern than left-handers, the difference is not significant on chi square test, chisquare = `r round(mychi$statistic,2)`, `r pformat(mychi$p.value)`.  However, two of the tests included in this analysis, Word Comprehension and Syntactic Decision, were not left-lateralised at the population level, and it could be argued they would just add noise to the analysis, which was intended to identify those who departed from the typical pattern of left-lateralisation. We therefore added an exploratory analysis, in which we excluded these two tests. When only Word Production, Sentence Production, Phonological Decision and Syntactic Comprehension were considered, `r round(100*tabN4[5,1]/sum(tabN4[1:5,1]),1)`% of left-handers and `r round(100*tabN4[5,2]/sum(tabN4[1:5,2]),1)`% of right-handers were consistently left-lateralised. 

It is noteworthy that this more categorical analysis finds rates of "atypical", i.e. non-left, lateralisation on language tasks that are on the one hand task-dependent, but on the other hand lower than typically observed when methods such as Wada test or fMRI are used. This is the case even for the most lateralised task, Sentence Generation, where 91% of right-handers vs 77% of left-handers were left-lateralised. The implications of this observation will be considered further in the Discussion.

# Are online behavioural measures comparable to direct measures of cerebral blood flow in indexing language laterality?

A preliminary inspection of correlations between online and fTCD laterality indices showed very little relationsihp between the two, even for the two measures, Word Comprehension and Rhyme Decision, that have analogues in fTCD (tasks D and C respectively).


```{r preparedata,echo=F}
ncol<-ncol(ddati)
for (i in 1:nrow(ddati)){
  mysub <- ddati$ID[i]
  w<-which(combdat$ID==mysub)
  ddati$DL_z[i] <- combdat$DL.zlat.ex[w]
   ddati$RDT_z[i] <- combdat$RDT.zlat.ex[w]
   ddati$WC_z[i] <- combdat$WC.zlat.ex[w]
}

#impute missing values using mice package
nunames<-c('DL_z','RDT_z','WC_z')
thisdat.j <- mice(ddati[,nunames], m=1, maxit = 50, method = 'pmm', seed = 500,print=FALSE)
ddati <-cbind(ddati[,1:12],complete(thisdat.j,1))
```



```{r bigheatmap,echo=F}

 #need to ensure online LIs from excluded are not included
mycolsb<-c("DL_z","RDT_z","WC_z",  "A_P1", "B_P2","C_P3", "D_R1","E_R2", "F_R3")
bigheatmap <- makeheatmap(ddati,mycolsb)
bigheatmap

```
__Need to double check that all appropriate exclusions have been made here__.

We had pre-registered two data checks: 1) Online measures that have split-half reliability below .6 will be excluded from further analysis. 2) Online measures of word comprehension and rhyme decision will only be taken forward to the next stage of analysis if they have a correlation of at least .11 with the counterpart measure from fTCD (word comprehension and phonological decision respectively).

The online Word Comprehension measure failed both checks: split-half reliability was below .6, and the correlation with the fTCD Word Comprehension laterality index was close to zero.  Rhyme Decision narrowly passed both checks: split-half reliability was `r onlinesummary[7,4]` (though test-retest reliability was only`r onlinesummary[8,4]`).  The correlation between the online Rhyme Decision and fTCD Phonological Decision was `r round(cor(ddati$RDT_z,ddati$C_P3,use='complete.obs'),3)`. Accordingly, we proceeded with the next step of analysis only with Dichotic Listening (which had good reliability, but no counterpart in the fTCD battery) and Rhyme Decision.

We had predicted that Dichotic Listening, as a receptive task, should load on the same factor as the Word Comprehension, Sentence Comprehension and Syntactic Decision, but it is evident from the heatmap that, insofar as it correlates with the fTCD tasks, the strongest association is with production tasks. 



```{r newmodelx,echo=F,include=F}

model.2Fnbase <- '
f1 =~  A_P1+B_P2+C_P3+E_R2+0*RDT_z
f2 =~  F_R3+D_R1+E_R2+C_P3+0*+DL_z
'
fit.2Fnbase <- cfa(model.2Fnbase,estimator="WLSMV", data=ddati)
summary(fit.2Fnbase)
wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2Fn,wantfits)

model.2Fnx <- '
f1 =~  A_P1+B_P2+C_P3+E_R2+RDT_z
f2 =~  F_R3+D_R1+E_R2+C_P3+DL_z
'
fit.2Fnx <- cfa(model.2Fnx,estimator="WLSMV", data=ddati)
summary(fit.2Fnx)
wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2Fnx,wantfits)
measurementInvariance(model=model.2Fnx,estimator="WLSMV",meanstructure=T,data=ddati,group="Handed")
anova(fit.2Fnbase,fit.2Fnx)

model.2Fny <- '
f1 =~  A_P1+B_P2+C_P3+E_R2+RDT_z+DL_z
f2 =~  F_R3+D_R1+E_R2+C_P3
'
fit.2Fny <- cfa(model.2Fny,estimator="WLSMV", data=ddati)
summary(fit.2Fny)
wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2Fny,wantfits)
measurementInvariance(model=model.2Fny,estimator="WLSMV",data=ddati,meanstructure=T,group="Handed")
anova(fit.2Fnbase,fit.2Fny)

tab.2Fnbase <- makeSEMtab(fit.2Fnbase)
tab.2Fnx <- makeSEMtab(fit.2Fnx)
tab.2Fny <- makeSEMtab(fit.2Fny)

model.2Fnz <- '
f1 =~  A_P1+B_P2+C_P3+E_R2+DL_z+0*RDT_z
f2 =~  F_R3+D_R1+E_R2+C_P3
'
fit.2Fnz <- cfa(model.2Fnz,estimator="WLSMV", data=ddati)
summary(fit.2Fnz)
wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2Fnz,wantfits)
measurementInvariance(model=model.2Fnz,estimator="WLSMV",data=ddati,meanstructure=T,group="Handed")
anova(fit.2Fnbase,fit.2Fnz)


pathfigname<-paste0(mydir,"/03-graphic-outputs/pathfigF2_bigA")
semPaths(fit.2Fnx, "diagram", weighted = FALSE,  shapeMan = "rectangle", sizeMan = 8, 
    sizeMan2 = 5,filetype='jpg',filename=pathfigname,width=3,height=2) #draws a path diagram

pathfigname<-paste0(mydir,"/03-graphic-outputs/pathfigF2_bigB")
semPaths(fit.2Fny, "diagram", weighted = FALSE,  shapeMan = "rectangle", sizeMan = 8, 
    sizeMan2 = 5,filetype='jpg',filename=pathfigname,width=3,height=2) #draws a path diagram


#Looking at residuals : dichotic has high variability and big residuals - may be worth trying to scale it - or possibly just censor at a less extreme value
```
 In practice, the model including a link from Rhyme Decision to Factor 1, and Dichotic Listening to Factor 2 gave a better fit than a model with both paths set to zero. Guided by the data, we ran an alternative model (not pre-registered) with both Rhyme Judgement and Dichotic Listening loading on Factor 1. This also gave excellent fit, with significant paths from both of these measures, as shown in Figure x. 


# Discussion

To simplify the interpretation of this complex dataset, it helps to focus on two overarching questions addressed by the study. The first question concerns correlations between laterality measures: in brief, is there evidence for a single laterality dimension on which people vary? The second question concerns handedness: does the answer to our first question differ in groups of left- and right-handers?

## Evidence for a single laterality dimension  

Previous attempts to consider the dimensionality of language lateralisation have been obscured by two issues. First, many studies have been conducted with measures whose reliability was not established. If two laterality indices are not correlated, it could just be because they are unreliable, and so it has been easy to dismiss lack of correlation between laterality measures as uninformative. Second, researchers have tended to focus only on measures that show left-lateralisation at the population level, treating unlateralised language measures as uninteresting. 

Considering first the online behavioural data, the most noteworthy observation was that the correlations between laterality indices from the three tasks were generally weak. This could not be attributed solely to poor reliability: although reliabilities of the two new tasks, Rhyme Decision and Word Comprehension, were not impressive (ranging from .54-.55 for test-retest), they were higher than the intercorrelations between measures. Our data was not suitable for a more formal model comparison, but inspection of the pattern of correlations between measures made it clear that our proposed two-factor model, with Dichotic Listening and Word Comprehension being positively correlated and unrelated to Rhyme Decision could not be supported. Indeed, the strongest correlation was found between Word Comprehension and Rhyme Decision, consistent with the idea that task demands (speeded responding to visual pictures) might be a greater determinant of strength of lateralisation than whether receptive or expressive language was involved.  

For the fTCD data, we again found good evidence for reliability of LIs on most tasks, even though some tasks were not left-lateralised. Consistent with Woodhead et al (2021), the LI on Syntactic Decision task had good reliability, and a distinctive pattern of association with other LIs, despite being unlateralised. This observation shows that lack of lateralisation at the population level does not mean that individuals use both hemispheres equally for the task: it seems rather that the population contains a mixture of people, some of whom consistently prefer the left hemisphere, and others the right. The LI from the Word Comprehension task was the least reliable in the battery, yet again showed quite distinctive patterns of selective association with other tasks. In addition, this task showed a trend for right-hemisphere bias, particularly in left-handers. 

With fTCD we were able to subject the single factor model to a stronger test, because we had sufficient tasks for Structural Equation Modeling. Consistent with Woodhead et al (2021), we could reject a single factor model; this gave a poor fit to the data, as it could not account for the fact that the correlations between LIs tended to form clusters. We tested a preregistered alternative two-factor model that involved a division between language production and language reception. This accounted for significantly more variance than the single factor model, but still left a great deal unexplained, and overall the fit was poor. Accordingly, following our preregistration, we divided the sample into two subgroups to explore different models and found one that gave good fit, which was then replicated in the second half of the sample. This again had a two factor structure, but had two of the tasks, Phonological Decision and Syntactic Comprehension, loading on both the factors. 

In a final step of analysis, we considered adding the laterality indices from online tasks to the model. The correlations between LIs from online tasks and fTCD were generally weak, and these measures did not help differentiate models.  Models that included Dichotic Listening and Rhyme Decision gave better fit when non-zero paths were included from these measures, than when they were set to zero, but the best fit was seen for a model where both Dichotic Listening and Rhyme Judgement loaded on Factor 1 (with language production tasks), rather than when Dichotic Listening was regarded as an indicator of Factor 2 (with receptive tasks).

## Left vs Right Handers  
For both online and fTCD LIs, with just one exception, there was a consistent trend for stronger left-hemisphere bias in right-handers than in left-handers. This reached significance on all measures except fTCD Word Comprehension. The exception was online Rhyme Judgement, which was not left-lateralised and where means for left- and right-handers were very similar. 

The SEM analysis allowed us to go beyond simple comparison of means to test whether the association between LIs showed a similar pattern in the two handedness groups. In our previous fTCD study using four of the same measures, we had concluded that there may be more dissociation between factors in left-handers, but the sample size was small for this kind of analysis.  Even with the current sample size, power to detect model invariance is limited. Having noted those limitations, our analysis suggested that there was no reason to postulate different models for left- vs right-handers. The substantial differences between these groups could be entirely accounted for in terms of differences in factor means.  As an analogy, we could say this would be like showing that the relationship between height and weight is similar in males and females, even though females are on average shorter and lighter. 

## Interpreting the two-factor structure    
```{r taskanalysis,echo=F}
tasks <- read.csv(paste0(mydir,"/05-writeup/task details.csv"))
ftasks <- flextable(tasks[,1:7])
ftasks
tabnumber <- tabnumber+1
```
Table `tabnumber` summarises the characteristics of the six fTCD tasks, grouped according to the factors they load on. The online Dichotic Listening task is also shown. The task battery had been designed to include three tasks that involved language generation, and three that involved receptive language. As predicted the former loaded on Factor 1 and the latter on Factor 2, but in addition Phonological Decision loaded on Factor 2, and Sentence Comprehension on Factor 1. Phonological Decision, unlike the other tasks loading on Factor 2, did not involve auditory input, but did have in common the 2-choice response format of other Factor 2 tasks. 

Sentence Comprehension also behaved unexpectedly, in that it had significant loadings on Factor 1, despite being designed as a purely receptive task. While it is possible that participants might have covertly repeated sentences to themselves when doing the task, the fast pace of the task made that unlikely, and furthermore, our previous study suggested that simple production of spoken output was not the key attribute of Factor 1 tasks; we had dropped from the current study a List Generation task used by Woodhead, Rutherford and Bishop (2020) that simply involved repeating overlearned sequences such as days of the week, as it had been only weakly lateralised. The Sentence Comprehension task did, however, involve  using syntactic information to assign semantic roles and build meaning representations, and hence more linguistic computation than the tasks that used single word stimuli. 



## Supplementary materials  

Supplementary Figure 1

![Density plots and scatterplot showing relationship between conventional laterality index and z-LI for dichotic listening.](~/Dropbox/COLA_RR_Analysis/03-graphic-outputs/Afig4_DLdens.png)
Supplementary Figure 2
![Density plots and scatterplot showing z-LI scores for Rhyme Decision and Word Comprehension listening.](~/Dropbox/COLA_RR_Analysis/03-graphic-outputs/Afig5_RDWCdens.png)
```{r saveworkspace,echo=F}
#save.image(file = "my_work_space.RData")
```


```{r anothertryfactors,echo=F}
#Trying to extract factor scores that are not centred on zero.
#Not sure this is legit!
mys<-standardizedSolution(fit.2Fn)
wts1 <- mys$est.std[1:4]
wts2 <-mys$est.std[5:8]
ddati$f1<-wts1[1]*ddati$A_P1+
  wts1[2]*ddati$B_P2+
  wts1[3]*ddati$C_P3+
  wts1[4]*ddati$E_R2

ddati$f2<-wts2[1]*ddati$F_R3+
  wts2[2]*ddati$C_P3+
  wts2[3]*ddati$D_R1+
  wts2[4]*ddati$E_R2

myx<-ddati$f1
myy<-ddati$f2
mygroup< ddati$Handed
mydat<-as.data.frame(cbind(myx,myy,mygroup))
mydat$mygroup<-as.factor(mydat$mygroup)
namex<-'Factor 1'
namey<-"Factor 2"
namegroup <- "Handedness"
grouplabels<- c("Left","Right")
mylines<-2 #horiz and vertical lines showing zero
mytitle <- "Factors derived from standardized weighted \nsum from modified 2 factor model"
mycompositeplot <- dodensity(mydat,myx,myy,mygroup,namex,namey,namegroup,grouplabels,mylines,mytitle)

mycompositeplot
ggsave(paste0(mydir,"/03-graphic-outputs/standfactors.png"),width = 5, height = 4)


```


```{r plotexplore}


latcols<-c("A_P1","B_P2","C_P3","D_R1","E_R2","F_R3")
w<-which(colnames(ddati) %in% latcols)
means<-colMeans(ddati[,w])
nucols<-min(w)+order(means,decreasing=T)-1

for (i in 1:6){
  thiscol<-colnames(ddati)[nucols[i]]
  thisx<-'x'
  thisgroup<-'Handed'
# Plot
ddati %>%
  ggplot( aes_string(x=thiscol, color=thisgroup, fill=thisgroup)) +
  geom_density(alpha=0.6) +
  #scale_fill_viridis(discrete=TRUE) +
  #scale_color_viridis(discrete=TRUE) +
  theme_ipsum() +
  theme(
    legend.position="none"
  ) 

}
#need to set x axis from -6 to 8 to accommodate all. Can then add zero point.
#Also add better task labels and maybe means for L and R

```
