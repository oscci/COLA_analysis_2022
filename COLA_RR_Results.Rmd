---
title: Inconsistent language lateralisation -  testing the dissociable language laterality
  hypothesis using behaviour and lateralised cerebral blood flow
author: "COLA consortium*"
date: "12 May 2022"
output:
  word_document: default
  reference_docx: mystyle.docx
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

```{=html}
<!-- This script will create output files in the working directory. It needs to have these files available in a subdirectory, currently called COLAbits:
task_details.csv  
combined_data.csv  
-->
```
<!-- IF YOU RUN THE SUPPLEMENTARY OUTPUTS-PANDER.RMD SCRIPT BEFORE THIS ONE, YOU MAY GET A CONFLICT BETWEEN PANDER AND FLEXTABLE, AND THIS SCRIPT CRASHES WITH ERROR MESSAGE WHEN FONT SIZE IS SET IN FLEXTABLE.  TO OVERCOME THIS, RESTART R BEFORE YOU RUN THIS SCRIPT-->

\*Adam J. Parker^a12^, Zoe V. J. Woodhead^a1^ , David P. Carey^b^,
Margriet A. Groen^c^, Eva Gutierrez-Sigut^d,e^, Jessica Hodgson^f^, John
Hudson^f^, Emma M. Karlsson^b^, Mair√©ad MacSweeney^e^, Heather Payne^e^,
Nuala Simpson^a^, Paul A. Thompson^a^, Kate E. Watkins^a^, Ciara
Egan^a,b^, Jack H. Grant^a,f^, Sophie Harte^a,e^, Brad T. Hudson^a,c^,
Maria Sablik^a,b^, Nicholas A. Badcock^h^, Dorothy V. M. Bishop^a,3^

^1^ Joint first author\
^2^ Current address: Department of Experimental Psychology, University
College London, 26 Bedford Way, London WC1H 0AP.\
^3^ Corresponding author,
[dorothy.bishop\@psy.ox.ac.uk](mailto:dorothy.bishop@psy.ox.ac.uk){.email}

^a^Department of Experimental Psychology, University of Oxford, UK\
^b^School of Human and Behavioural Sciences, Bangor University, UK\
^c^Department of Psychology, Lancaster University, UK\
^d^Department of Psychology, University of Essex, UK\
^e^Deafness, Cognition and Language Research Centre, University College
London, UK ^f^Lincoln Medical School, University of Lincoln, UK\
^g^School of Psychology, University of Lincoln, UK\
^h^School of Psychological Sciences, University of Western Australia,
Australia

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


# If the package is not installed, install it. If it is installed, load it.
 usePackage <- function(p) {
    if (!is.element(p, installed.packages()[,1]))
         install.packages(p, dep = TRUE)
    require(p, character.only = TRUE)
 }
usePackage("tidyverse")
usePackage('yarrr') #for pirate plots
#usePackage("osfr") #for reading files from OSF
#usePackage("stringr")
usePackage("table1") #useful for making simple tables of demographic etc
usePackage("ggExtra") #for marginal plots
usePackage("ggpubr")
usePackage("patchwork") #for combining plots in ggplot
usePackage("flextable")

usePackage("kableExtra")
#
usePackage("ggstatsplot")
usePackage("MASS") #includes boxcox function
usePackage("MBESS")
usePackage("nlme")
usePackage("semPower")
usePackage("semTools")
usePackage("bookdown")
usePackage("lavaan")
usePackage("semPlot")
#
usePackage("officer")
usePackage("corrr") #added by DB for easy correlations
usePackage("plyr")
usePackage("ggpubr")
usePackage("reshape2")
usePackage("mice")
usePackage("DiagrammeR") #for participant recruitment flowchart
usePackage('DiagrammeRsvg')
#usePackage('magrittr')
usePackage('rsvg')
usePackage("RVAideMemoire") #for bootstrapped CIs around correlations
usePackage("effsize")
usePackage("psych")
usePackage("magick") #used to combine path diagrams to single figure

usePackage("readr")
usePackage("semPlot")
usePackage("sjPlot")
usePackage("here") #for controlling paths

options(scipen=999)

tabnumber <-0 #initialise counter for tables
fignum <-0 # initialise counter for figs
suppfignum<-0 
suppnum <- 0

here::i_am("COLA_RR_Results.rmd") #working directory will be set to this by rmd
#We use 'here' package to ensure results are written and read relative to this
#NB here does not like directories that start with numeric characters etc.
#This is the file structure that you need to run this script smoothly.

#Script itself lives in top level of directory

# Folder 'data' within that directory contains processed files created by Gorilla.Processing.Script_forOSF.rmd:
# "combined_data.csv"  
# "combined_data_origbaseline.csv" 
# "sess1.csv"
# "sess2.csv"
# "ftcd_data.csv"
# "ftcd_data_origbaseline.csv"
# "task_details.csv"  - not really data, but stored here as it is read in by this script

# Additional files will be created as script is run. Note 'ddat-OE.csv' is created so it can be read when creating Supplementary outputs. 

# Folder 'ftcd_task_means' - large folder of means from L and R channels for each task and participant - needed to create timecourse plot, but once it is made, can skip that chunk.

# Folder 'figs' : folder to which figures will be written; also includes predrawn figures WordCompDemo.pdf and tasktimings.pdf



```

<!--- GENERIC FUNCTIONS START -->

```{r numformat,echo=F}
#Format numbers so they have same n decimal places, even if zero at end
#This returns a string

numformat=function(mynum,ndecimals){
  newnum <- format(round(mynum,ndecimals),nsmall=ndecimals)
  return(newnum)
}

```

```{r corformat,echo=F}
#Format correlation so they have same n decimal places,and no initial zero

corformat=function(mynum,ndecimals){
  newnum <- format(round(mynum,ndecimals),nsmall=ndecimals)
  neg<-''
  if(mynum<0){
    neg<-'-'
    mynum<-substring(newnum,2)} #strip off minus sign - will put it back later
  newnum<-substring(newnum,2) #strip off initial zero
  newnum<-paste0(neg,newnum)
  
  return(newnum)
}

```

```{r pformat, echo=F}
#function to format p-values
pformat=function(myp){
  pout <- paste('p =',numformat(myp,3))
  if(myp<.001){pout = 'p < .001'}
  return(pout)
}
```

```{r pformat2, echo=F}
#function to format p-values, without the 'p = ' bit
pformat2=function(myp){
  pout <- numformat(myp,3)
  if(myp<.001){pout ='< .001'}
  return(pout)
}
```

```{r LIdensityplot,echo=F}
#generic density plot, subsetted by handedness or another categorical variable
#includes line showing zero point.
#takes as input allsum; temp and cattemp are dummy columns that are assigned prior to call to the function
doLIplot <- function(myfile,temp,cattemp,mysubsetname,mysubsetlabels,xlabel,xrange){
LI.plot <- ggplot(myfile, aes(x=temp, color=as.factor(cattemp))) +
  geom_density()+
  xlab(xlabel)+
  geom_vline(xintercept = 0,lty=3)+
  xlim(xrange)+
  scale_color_manual(name=mysubsetname,
                       labels=mysubsetlabels,
                       values=c("blue","red"))


return(LI.plot)
}
```

```{r filltable,echo=F,warning=F}
#Generate function to populate summary data frame - same steps for all online tasks
#This function also repurposed for ftcd
populate <- function(mydf,mytask,mysummary,writecolnum,tasktype){
  if(tasktype=='beh'){
w<-which(colnames(mydf)==paste0(mytask,'.zlat'))
x<-which(colnames(mydf)==paste0('exclude',mytask))
odds<-which(colnames(mydf)==paste0(mytask,'.odd.zlat'))
evens<-which(colnames(mydf)==paste0(mytask,'.even.zlat'))
  }
    if(tasktype=='ftcd'){
w<-which(colnames(mydf)==paste0(mytask,'_mean_LI'))
x<-which(colnames(mydf)==paste0(mytask,'_exclude'))
odds<-which(colnames(mydf)==paste0(mytask,'_mean_odd'))
evens<-which(colnames(mydf)==paste0(mytask,'_mean_even'))
  }

thisdat<-mydf[mydf[,x]==0,]
thisdat$thiscol<-thisdat[,w]
normp <-shapiro.test(thisdat$thiscol)$p.value
nL <- nrow(filter(thisdat,Rhanded==0))
nR <- nrow(filter(thisdat,Rhanded==1))
tL<- t.test(thisdat$thiscol[thisdat$Rhanded==0])
tR<- t.test(thisdat$thiscol[thisdat$Rhanded==1])
sdL<-numformat(sd(thisdat$thiscol[thisdat$Rhanded==0],na.rm=T),2)
sdR<-numformat(sd(thisdat$thiscol[thisdat$Rhanded==1],na.rm=T),2)
sdall <-numformat(sd(thisdat$thiscol,na.rm=T),2) 
tcompare <- t.test(thisdat$thiscol~thisdat$Rhanded,alternative='less')
#write N for L and R in row 1 of online summary
mysummary[1,writecolnum]<-paste0(nL,' LH + ',nR,' RH')
mysummary[2,writecolnum]<-paste0(numformat(mean(thisdat$thiscol,na.rm=T),2)," (",sdall,")")
mysummary[3,writecolnum]<-paste0(numformat(semTools::skew(thisdat$thiscol)[1],2),' (',pformat(semTools::skew(thisdat$thiscol)[4]),')')
mysummary[4,writecolnum]<-paste0(numformat(semTools::kurtosis(thisdat$thiscol)[1],2),' (',pformat(semTools::kurtosis(thisdat$thiscol)[4]),')')
mysummary[5,writecolnum]<-pformat(normp)
mysummary[6,writecolnum]<-paste0(numformat(tL$estimate,2)," (",sdL,")")
mysummary[7,writecolnum]<-paste0(numformat(tR$estimate,2)," (",sdR,")")
mysummary[8,writecolnum]<- paste0('t = ',numformat(tL$statistic,1),'; ', pformat(tL$p.value))
mysummary[9,writecolnum]<- paste0('t = ',numformat(tR$statistic,1),'; ', pformat(tR$p.value))
mysummary[10,writecolnum]<- paste0('t = ',numformat(-tcompare$statistic,1),'; ', pformat(tcompare$p.value))
cor.eo <-spearman.ci(thisdat[,odds],thisdat[,evens], nrep = 1000, conf.level = 0.95)
mysummary[11,writecolnum]<-paste0(numformat(cor.eo$estimate,2),' [',numformat(cor.eo$conf.int[1],2),', ',numformat(cor.eo$conf.int[2],2),']')




return(mysummary)
}
```

```{r trianglefunction,echo=F}
#Gets upper (or lower if specified) triangle of a correlation matrix
 get_tri <- function(cormat){
    cormat[upper.tri(cormat,diag=F)]<- NA
    return(cormat)
  }
```

```{r heatmapfunction,echo=F}
#Make a heatmap
# http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization
makeheatmap <- function(mydf,mycols,pearsons){
cormat <- cor(mydf[,mycols],use="complete.obs")
for (i in 1:length(mycols)){
    cormat[i,i]<-pearson_splits[i,2] #put split half correlation on diagonal
}
melted_cormat <- melt(cormat) #whole correlation matrix in a table
head(melted_cormat)

mytri <- get_tri(cormat)
# Melt the correlation matrix

melted_cormat <- melt(mytri, na.rm = TRUE)

melted_cormat$mycolour<-'NA'  #colour used to set diagonal values to have  frame
melted_cormat$corlabel<-'' #formatting correlation matrix to 2 decimals

for (r in 1:nrow(melted_cormat)){
  if(melted_cormat$Var1[r]==melted_cormat$Var2[r]){
    melted_cormat$mycolour[r]<-'grey'
  }
    melted_cormat$corlabel[r]<-corformat(melted_cormat$value[r],2)
}


# Heatmap

#original heatmap
ggheatmap <- ggplot(data = melted_cormat, aes(Var1, Var2, fill = value))+
 geom_tile(color = melted_cormat$mycolour,size=1.5)+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-0.2,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_linedraw()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
theme(axis.text.y = element_text(
    size = 10, hjust = 1))+
  scale_y_discrete(limits=rev)+ #gets tasks in same order for x and y axis!
 coord_fixed()

#add some formatting
myheatmap <- ggheatmap + 
geom_text(aes(Var1, Var2, label = corlabel), color = "black", size = 4) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.5, 0.2),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))

return(myheatmap)}
```


```{r heatmapLR,echo=F}
#Make a heatmap with L and R below and above diagonal
#Pearson split half reliabilities are read in and added to diagonal
# http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization
makeLRheatmap <- function(mydf,mycols,pearson_splits,shortnames){
#if shortnames is TRUE, change to short form of names
shortcolnames<-c('X.Dichotic','Y.Rhyme Dec','Z.Word Comp','A.Word Gen','B.Sent Gen','C.Phon Dec','D.Word Dec','E.Sent Dec','F.Synt Dec')
if (shortnames==T){
  for (i in 1:length(mycols)){
    thiscol<-which(colnames(mydf)==mycols[i])
    colnames(mydf)[thiscol]<-shortcolnames[i]
  }
  mycols<-shortcolnames
}
  
corL <- cor(mydf[mydf$Handed=='Left',mycols],use="complete.obs")
corR <- cor(mydf[mydf$Handed=='Right',mycols],use="complete.obs")
corboth<-corL #we copy the correlation matrix for L, then substitute R values in upper triangel
for(i in 1:length(mycols)){
  for (j in i:length(mycols)){
    corboth[i,j]<-corR[i,j]
  }
}
for (i in 1:length(mycols)){
    corboth[i,i]<-pearson_splits[i,2] #put split half correlation on diagonal
}
melted_cormat <- melt(corboth) #whole correlation matrix in a table suitable for heatmap plot
melted_cormat$mycolour<-'NA'  #colour used to set diagonal values to have  frame
melted_cormat$corlabel<-'' #formatting correlation matrix to 2 decimals

for (r in 1:nrow(melted_cormat)){
  if(melted_cormat$Var1[r]==melted_cormat$Var2[r]){
    melted_cormat$mycolour[r]<-'grey'
  }
    melted_cormat$corlabel[r]<-corformat(melted_cormat$value[r],2)
}


#original heatmap
ggheatmap <- ggplot(data = melted_cormat, aes(Var1, Var2, fill = value))+
 geom_tile(color = melted_cormat$mycolour,size=1.5)+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-0.2,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_linedraw()+  #original had theme_minimal which made labels faint
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 10, hjust = 1))+
theme(axis.text.y = element_text(
    size = 10, hjust = 1))+
  scale_y_discrete(limits=rev)+ #gets tasks in same order for x and y axis!
 coord_fixed()

#add some formatting
#the corformat function should strip off initial zero but it doesn't, so substring used here to ensure it happens
myheatmap <- ggheatmap + 
geom_text(aes(Var1, Var2, label =corlabel), color = "black", size = 4)+
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank())+
  guides(fill = guide_colorbar(barwidth = 1, barheight = 5,
               title.position = "top", title.hjust = 0.5))
 

return(myheatmap)}
```

```{r bivplotfunction2,echo=F,warning=F}
#Function to make a bivariate plot colour coded by handedness, with spearman correlation in plot

#When we call this function we have already created temporary x and y cols (tempx and tempy) to be used in this function
bivplot2<-function(bivdat,name1,name2){

#correlations for each group

var1<-bivdat$tempx
var2<-bivdat$tempy
cor1<-spearman.ci(var1, var2, nrep = 1000, conf.level = 0.95)





lab1<- paste0("r[s] == ",round(cor1$estimate,3)) #correlation with subscript (need parse=TRUE below)
lab2 <- paste0("[",numformat(cor1$conf.int[1],3),", " ,numformat(cor1$conf.int[2],3),"]")

#lab1<- paste0("L-handers: rs = ",round(cor1$estimate,2))
#lab2<- paste0("R-handers: rs = ",round(cor2$estimate,2))   

myplot <- ggplot(bivdat, aes(x=tempx, y=tempy, color=Handedness)) +
  xlab(name1)+
   ylab(name2)+
  xlim(-10,10)+
  ylim(-10,10)+
   geom_point(shape=1, size=1)+
  scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
  geom_hline(yintercept=0,linetype="dashed")+
   geom_vline(xintercept=0,linetype="dashed")+
  annotate("text", x=-7, y=9.5, label= lab1,size=3,parse=TRUE) +
  annotate("text", x=-7, y=8.5, label= lab2,size=2.8) +
  theme(text = element_text(size = 8)) 
  return(myplot)
}


```

```{r scatterplusfunction,echo=F}
#Function for doing scatterplot with marginal density distributions
doscatterplus <- function(myfile, taskname,mycolnames,myrange){ #myfile contains group in col 1 and the 2 cols to plot in cols 2-3
colnames(myfile)[2:4]<- mycolnames

r <- round(cor(myfile$Odd,myfile$Even,use="complete.obs",method="spearman"),3)
myt<-t.test(myfile$All~myfile$Handed)
myt2 <- t.test(myfile$All) #single group t-test
text1 <- paste0('Mean diff from zero: \nt = ',round(myt2$statistic,2),"; ", pformat(myt2$p.value))
text2 <- paste0('L vs. R handers \n(all trials):\n t = ',round(myt$statistic,2),"; ", pformat(myt$p.value))
text3 <-paste0('Spearman $r_s$: ',r)
p <- ggplot(myfile, aes_string('Odd','Even')) +
  aes_string(colour = 'Handed') +
  geom_point() + theme_bw(15)+
  annotate("text", x = 3.2, y = -3,label=text3,size=3)+
  annotate("text", x = -2.5, y = 5.5,label=text1,size=3)+
  annotate("text", x = -2.5, y = 3.5,label=text2,size=3)+
  geom_hline(yintercept=0,linetype="dashed",colour="grey")+
  geom_vline(xintercept=0,linetype="dashed",colour="grey")+
  xlim(myrange)+
  ylim(myrange)+
  ggtitle(taskname)

p2 <- ggExtra::ggMarginal(
  p,
  type = 'density',
  margins = 'both',
  size = 5,
  groupColour = TRUE,
  groupFill = TRUE
)
return(p2)

}

```

```{r makeSEMtabfunction,echo=F}
#Function to make tidy table for SEM output for model with one group
#This is now largely superseded by bigsummary, though SEMtab has more complete information
makeSEMtab <- function(myfit){
ss<- standardizedsolution(myfit)
 srow<-nrow(ss)
 scol<-ncol(ss)
mySEMout <-ss[,-4]
mySEMout[,4:7]<-round(mySEMout[,4:7],3)

#add fit measures
myfitmeasures<-c('CFI','TLI','SRMR','rmsea','chisq','DF') #can modify this if need be
fm<-fitmeasures(myfit,myfitmeasures)
for(i in 1:length(myfitmeasures)){
mySEMout[(i+srow),1]<-myfitmeasures[i] #write name of fit measure to col 1
mySEMout[(i+srow),2]<-round(fm[i],3) #write value to col 2
}
return(mySEMout)
}
```

```{r makeSEMtab2function,echo=F}
#Table for SEM output side by side for 2 groups
makeSEMtab2 <- function(myfit,mygroups){
ss<- summary(myfit)$PE   #! unstandardized estimates used here
wantcols<-c("lhs", "op","rhs" ,  "est" ,"se" )
ss<-ss[,wantcols]
ss[,4:5]<-numformat(ss[,4:5],2)
for (i in 1:nrow(ss)){
ss[i,3]<-paste0(ss[i,1],ss[i,2],ss[i,3])
}
ss<-ss[,3:5]

 srow<-nrow(ss)
 scol<-ncol(ss)
 sshort<-srow/2
mySEMout <- cbind(ss[1:sshort,],ss[(1+sshort):srow,2:3])


myfitmeasures <- c('CFI','TLI','SRMR','rmsea','rmsea.ci.lower','rmsea.ci.upper','chisq','df','pvalue','chisq.scaled','pvalue.scaled') #can modify this if need be
lfm<-length(myfitmeasures)

mySEMout[(sshort+1):(sshort+(1+lfm)),]<-NA #additional rows for fit measures


#add fit measures

mySEMout[(1+sshort),]<-c('Fit measures','','','','')
fm<-fitmeasures(myfit,myfitmeasures)
for(i in 1:length(myfitmeasures)){
mySEMout[(1+i+sshort),1]<-myfitmeasures[i] #write name of fit measure to col 1
mySEMout[(1+i+sshort),2]<-round(fm[i],3) #write value to col 2
}

mySEMout[(sshort+1):nrow(mySEMout),3:5]<-''

#make different col headers for the 2 groups
colnames(mySEMout)[2:3]<-paste0(colnames(mySEMout)[2:3],mygroups[1])
colnames(mySEMout)[4:5]<-paste0(colnames(mySEMout)[4:5],mygroups[2])
colnames(mySEMout)[1]<-'Estimates'
mySEMout<-rbind(mySEMout[1,],mySEMout) #add a row at the top
mySEMout[1,]<-c('Parameter Estimates','','','','')

return(mySEMout)
}
```

```{r add-to-bigsummaryfunction,echo=F}
#This function adds to the bigsummary data frame - it takes the paths and a few diagnostic stats from myfit and writes to writecol in bigsummary. If comparisonfit is specified, it will also do a $\chi$^2^  comparison with that model and put p value in final row.
addmodel <- function(bigsummary,myfit,comparisonfit,myfitname,writecol){
  colnames(bigsummary)[writecol]<-myfitname #name of the model as column name
ss<- summary(myfit,standardized=TRUE)$PE #get standardized coefficients from current model
bigsummary[1,writecol]<-lavInspect(myfit,'nobs') #sample size goes in 1st row
thisrow<-2 #initialise counter for factor loadings; starts row 3, after title
#if a relevant path exists, put its estimate in correct row; first Factor1, then Factor2
#If  no path in the model, it just skips it
for (f in 1:2){
for (m in 1:6){
  thisrow<-thisrow+1
  w<-which(ss$lhs==paste0('f',f))
  x<-which(substr(ss$rhs,1,1) == LETTERS[m])
  myrow<-intersect(w,x)[1]
  thisstring <- paste0(numformat(ss$est[myrow],2)," [",numformat(ss$se[myrow],2),"]")  #we use numformat function defined earlier to give tidy cols
  bigsummary[thisrow,writecol]<-thisstring
}
}
#Correlation between factors is added (if it exists)
thisrow<-thisrow+1
myrow<-intersect(which(ss$lhs=='f1'),which(ss$rhs=='f2'))[1]
if(length(myrow)>0){
  thisstring <- paste0(numformat(ss$est[myrow],2)," [",numformat(ss$se[myrow],2),"]")
  bigsummary[thisrow,writecol]<-thisstring
}

wantfits <- c('CFI','TLI','SRMR','rmsea','chisq','chisq.scaled','df','pvalue','pvalue.scaled','rmsea.ci.lower','rmsea.ci.upper') #fit indices to include
#NB there are many other fit indices we could add, but if we do, would need to modify bigsummary.
#Currently script assumes we will have these fit indices 

#fm<-as.character(round(fitmeasures(myfit,wantfits),2))
fm <- numformat(fitmeasures(myfit,wantfits),2)
w<-which(wantfits=='rmsea')
#need to bolt upper and lower ci to this value. These are set to be last 2 indices
y<-which(wantfits=='rmsea.ci.lower')
fm[w]<-paste0(fm[w], " [",fm[y],", ",fm[(y+1)],"]")

w<-which(wantfits=='chisq')
#need to bolt pvalue to this value. 
y<-which(wantfits=='pvalue')
fm[w]<-paste0(fm[w],", ",pformat(as.numeric(fm[y]))) #pformat defined in function at top of script

w<-which(wantfits=='chisq.scaled')
#need to bolt pvalue to this value. 
y<-which(wantfits=='pvalue.scaled')
fm[w]<-paste0(fm[w],", ",pformat(as.numeric(fm[y])))

#find first row to write to
r<-which(bigsummary[,1]==wantfits[1]) #find row corresponding to first fit index
writeN <- length(wantfits)-4 #we don't write the pvalues or CI for RMSEA as they are incorporated with chisqs and RMSEA
bigsummary[r:(r+writeN-1),writecol]<-fm[1:writeN] #write the fit indices in successive rows, starting with r


bigsummary[3:15,1]<-
  c('Factor 1 -> Word Generation',
'Factor 1 -> Sentence Generation',
'Factor 1 -> Phonological Decision',
'Factor 1 -> Word Decision',
'Factor 1 -> Sentence Decision',
'Factor 1 -> Syntactic Decision',
'Factor 2 -> Word Generation',
'Factor 2 -> Sentence Generation',
'Factor 2 -> Phonological Decision',
'Factor 2 -> Word Decision',
'Factor 2 -> Sentence Decision',
'Factor 2 -> Syntactic Decision',
'Factor 1 <-> Factor 2')



return(bigsummary)
}
```

```{r factorscores,echo=F}
#FUnction to extract factor scores and plot them and save plot
makefactorplot <- function(myfit,fitname,thisdat){
lastc <- ncol(thisdat)
myfacs<-predict(myfit)
myfacs<-as.data.frame(myfacs)
colnames(myfacs)<-c("Factor1","Factor2")
w<-which(colnames(ddati)=='Factor1') #check if we already have col for factors
if(length(w)==0){
thisdat<-cbind(thisdat,as.data.frame(myfacs))
}
if(length(w)>0){
  thisdat$Factor1 <- as.data.frame(myfacs[,1])
  thisdat$Factor2 <- as.data.frame(myfacs[,2])
}

plotf1f2 <- myscatter(thisdat$Factor1,thisdat$Factor2,xlabel='Factor1',ylabel='Factor2',thisdat)

plotname <- paste0("/factors_",fitname,".png")

ggsave(plotname,width = 5, height = 3)
return(thisdat)
}

```

```{r densplotsfunction,echo=F}
#overlapping density plots 
densplots<-function(mydat,thiscol,thisgroup,mylab){
  myplot <- ggplot(data=ddati, aes_string(x=thiscol, color=thisgroup, fill=thisgroup)) +
  geom_density(alpha=0.5,kernel="gaussian") +
  annotate(geom="text", x=-6, y=.3, label=mylab,hjust=0,size=4)+
  #scale_fill_viridis(discrete=TRUE) +
  #scale_color_viridis(discrete=TRUE) +
  xlim(-6,8)+
    ylim(0,.36)+
  geom_vline(xintercept = 0,linetype="dotted")+
  theme(axis.title.x=element_blank(),
        axis.title.y=element_blank(),legend.position="none")
  return(myplot)
}
```

<!--- GENERIC FUNCTIONS END -->

```{r readcombined,echo=F}
#Script is set up so you can run the whole script with different subsets of participants. If you want to do that, you need to make a folder within your working directory called 'supplementary' where alternative outputs will be stored. Note that this is now superseded by separate script for creating suppolementary document.

wdir<-here("COLA_RR_Results.rmd") 
mydir <- paste0(wdir,"/")
alldir <- c(mydir,paste0(mydir,"supplementary"))

dir2 <-"02-data/" 

useorigbaseline <- 0 #should be set to zero unless creating alternative file for supplement. This will undergo usual processing and then be saved as ddati_origbaseline file which can be read in for the supplement.
readfile<-"data/combined_data.csv" #we use 'here' below to ensure this is specified relative to top level file, set in initial chunk
if(useorigbaseline==1){
  readfile<-"data/combined_data_origbaseline.csv" 
}
combdat <- read.csv(here(readfile))

#Create a code for handedness based on EHI
combdat$selfrep_Rhand<-combdat$Rhanded #this is self-report
combdat$Rhanded<-0
combdat$Rhanded[combdat$EHI.LI>0]<-1

w<-which(combdat$No_signal==1)
combdat$ftcd[w]<-9 #cases marked 9 were seen but did not have a signal

supplementary <- 0 #set this to zero for the main paper. With other values we can change the selection criteria to look at implications of using original language exclusion cutoff. When supplementary=1, we include only those shown in flowchart as group 1 (native and nonnative)

#NB the supplementary results are now shown in Supplementary files. 

if(supplementary==1){
  mydir <- alldir[2]
}

#We will create a code that divides people into 4 groups. 
#1A are native English speakers (1)
#1B are non-native speakers who pass the preregistered exclusion criterion (2)
#2A are non-native speakers who failed the preregistered exclusion criterion, but passed the modified criterion (3)
#2B are non-native speakers who failed the modified criterion. (4)

#We first create a code that reflects whether strict exclusion criteria are met for language. This is 1 for lextale < 80, 10 for gram12err >1, and 11 if both are met
combdat$lang2exclude <- 0
w<- which(combdat$gramerr12>1)
combdat$lang2exclude[w]<-10
if(supplementary==1){
  combdat$lang2exclude <- 0
  w<- which(combdat$gramerrs>3)  #gramerrs has all items and here we use cutoff of 3
combdat$lang2exclude[w]<-10
}
w<- which(combdat$lexTALE<80)
combdat$lang2exclude[w]<-combdat$lang2exclude[w]+1

#We now use langgroup to determine specific exclusions, depending on setting of supplementary

combdat$langpreregexclude<-0
w<- which(combdat$gramerrs>3)
combdat$langpreregexclude[w]<-1

combdat$langgroup <-1
w<-intersect(which(combdat$langpreregexclude==0),which(combdat$nativeEnglish==0))
combdat$langgroup[w] <-2

w<-intersect(which(combdat$langpreregexclude==1),which(combdat$nativeEnglish==0))
combdat$langgroup[w] <-3

w<-intersect(which(combdat$lang2exclude==1),which(combdat$nativeEnglish==0))
combdat$langgroup[w] <-4


if(supplementary==1){
  w<-which(combdat$langgroup<3) #supplementary 1 includes only groups 1A and 1B
  combdat<-combdat[w,]
}

testtab <- table(combdat$Rhanded,combdat$ftcd)
colnames(testtab)<-c('-FTCD','+FTCD','no signal')
rownames(testtab)<-c('Left-handed','Right-handed')


ftcd.dat <- filter(combdat,ftcd==1) #has demog, behav and ftcd data for those who did ftcd
langtab <- table(ftcd.dat$Rhanded, ftcd.dat$langgroup)
#Langtab cells can be accessed with 3 indices corresponding to Rhanded,langexclude and native English status).

myftcd <- filter(ftcd.dat) #we'll do analysis on myftcd
#can add here criteria for exclusion


```

```{r dataqual, echo=FALSE, warning=FALSE}
#As preregistered, we identify cases who have high values of SE for the LI, as this indicates the measurements are unusually variable from trial to trial
#We compute Qlimit based on Hoaglan-Iglewicz formula and exclude cases that exceed this value.
# Identify outliers
for (t in 1:6){
  SEcol <- which(colnames(combdat) == paste0(LETTERS[t], '_mean_se'))
  Q3<-quantile(combdat[ , SEcol],.75,na.rm=TRUE)
  Q1<-quantile(combdat[ , SEcol],.25,na.rm=TRUE)
  Qlimit<-Q3+2.2*(Q3-Q1)
  
# If there are at least 10 trials, include the data
  excludecol = which(colnames(combdat) == paste0(LETTERS[t], '_exclude'))
  trialscol = which(colnames(combdat) == paste0(LETTERS[t], '_N'))
  combdat[,excludecol] <- NA #initialise with NA
  combdat[which(combdat[ , trialscol] > 9), excludecol] <- 0
  combdat[which(combdat[ , trialscol] < 10), excludecol] <- 1
  
  # If the SE is too high, exclude the datatrials
  combdat[which(combdat[ , SEcol] > Qlimit) , excludecol] <- 1
}

# Count number of missing or excluded datapoints per task. 


# First set to NA those who did not do ftcd
n_excludeLI = matrix(data=NA, nrow=1, ncol=6)
for (t in 1:6){
  excludecol = which(colnames(combdat) == paste0(LETTERS[t], '_exclude'))
  n_excludeLI[t] = length(which(combdat[ , excludecol] > 0)) 
}

# Should any participants be excluded? If a participant has more than one excluded task, the participant is excluded entirely
combdat$DopExclude <- 0
tmp <- combdat$A_exclude + combdat$B_exclude + combdat$C_exclude + combdat$D_exclude + combdat$E_exclude + combdat$F_exclude
combdat$DopExclude[which(tmp > 1)] = 1
n_excluded = length(which(combdat$DopExclude == 1))

ddat<- filter(combdat,DopExclude==0,ftcd==1) #not really necessary to create this, but done for consistency with previous script : this has just those who did ftcd and who were not excluded on demographics or ftcd.

# We now count how many included participants omitted a single task
n_excludeLI = matrix(data=NA, nrow=1, ncol=6)
for (t in 1:6){
  excludecol = which(colnames(ddat) == paste0(LETTERS[t], '_exclude'))
  n_excludeLI[t] = length(which(ddat[ , excludecol] > 0)) 
}

```

# Abstract

<!--- Numbers with ftcd: admitted defeat and hard-coded these - they come from testtab2, but that is computed later on and moving it created havoc. Numbers here agree with testtab2 values in flowchart--->

*Background*: Most people have strong left-brain lateralisation for
language, with a minority showing right- or bilateral language
representation. On some receptive language tasks, however,
lateralisation appears to be reduced or absent. This contrasting pattern
raises the question of whether and how language laterality may
fractionate within individuals. Building on our prior work, we
postulated (a) that there can be dissociations in lateralisation of
different components of language, and (b) these would be more common in
left-handers. A subsidiary hypothesis was that laterality indices will
cluster according to two underlying factors corresponding to whether
they involve generation of words or sentences, vs. receptive language.
*Methods*: We tested these predictions in two stages: At Step 1 an
online laterality battery (Dichotic listening, Rhyme Decision and Word
Comprehension) was given to `r sum(testtab)` individuals
(`r round(100*sum(testtab[1,]/sum(testtab),0))`% left-handers); At Step
2, functional transcranial Doppler ultrasound (fTCD) was used with 230
of these individuals (51% left-handers). 108 left-handers and 101
right-handers gave usable data on a battery of three language generation and
three receptive language tasks. *Results*: Neither the online nor fTCD
measures supported the notion of a single language laterality factor. In
general, for both online and fTCD measures, tests of language generation
were left-lateralised. In contrast, the receptive tasks were at best
weakly left-lateralised or, in the case of Word Comprehension, slightly
right-lateralised. The online measures were only weakly correlated, if
at all, with fTCD measures. Most of the fTCD measures had split-half
reliabilities of at least .7, and showed a distinctive pattern of
intercorrelation, supporting a modified two-factor model in which
Phonological Decision (generation) and Sentence Decision (reception)
loaded on both factors. The same factor structure fitted data from left-
and right-handers, but mean scores on the two factors were lower (less
left-lateralised) in left-handers. *Conclusions*: There are at least two
factors influencing language lateralization in individuals, but they do
not correspond neatly to language generation and comprehension. Future
fMRI studies could help clarify how far they reflect activity in
specific brain regions.

KEYWORDS: language laterality, handedness, dichotic listening, visual
half-field,\
functional transcranial Doppler ultrasound

Abbreviations: CBFV, cerebral blood flow velocity; fTCD, functional
transcranial Doppler ultrasound; LIz score, laterality index expressed
as individualised z-score

*In our Stage 1 Report (registered at <https://osf.io/p8k2b>) we gave
the background for our preregistered hypotheses and analysis plan for
this study. In the current paper, with the exception of the Abstract,
material prior to Participants has the same wording as the Stage 1
Report, except for minor typographical changes (e.g. verb tense), plus
some sentences where departures to preregistration are made explicit.
The latter are in italic font. Some preregistered material has been
moved after the Results section to improve logical flow; this is shown
in bold italic.*

# 1. Introduction

Cerebral lateralisation for language has been studied both in
populations and within individuals. At the population level, it is
well-established that, for most people, language generation is
predominantly controlled by the left hemisphere of the brain. There is
individual variation, and a minority of people have right hemisphere
language or do not show clear bias to one side. Numerous sources of
evidence converge to show that atypical language laterality is more
common in left-handers (around 70% left-lateralised) than right-handers
(around 95% left-lateralised; Carey & Johnstone, 2014; Knecht, Deppe,
Dr√§ger, Bobe, Lohmann et al., 2000; Rasmussen & Milner, 1976, 1977;
Vingerhoets, 2019). Strong left-lateralisation is not seen for all
aspects of language, however. As will be reviewed below, functional
brain imaging has shown that on some language tasks left-lateralisation
at the population level is either reduced or absent. If we regard
language lateralisation as a single dimension it may be tempting to
conclude that a language task that is not strongly lateralised at the
population level is likely to be a noisy or invalid measure (cf.
Bethmann, Tempelmann, De Bleser, Scheich, & Brechmann, 2007, S√∏rensen &
Westerhausen, 2020). In practice, little attention has been given to
individual differences in language functions that are not strongly
lateralised. Vingerhoets (2019) noted that to date few studies have
distinguished between left-lateralised, right-lateralised and bilateral
phenotypes within a single lateralised function, and few have compared
laterality across different functions.

Tasks that do not show a population bias to left or right may
nevertheless be lateralised within individuals -- but with absent or
reduced population bias to left-sided functioning. For example, if we
have a language task where there is a 50:50 mixture of left- and
right-lateralised individuals, the population mean will indicate no
lateralisation, suggesting that both hemispheres participate in the task
in individuals. However, a subset of individuals may nevertheless be
reliably lateralised to one side or the other, but with equal
proportions being left-lateralised or right-lateralised. In that case,
we should find people who have different language functions mediated by
opposite hemispheres. This notion is compatible with suggestive evidence
that some people have discrepant language lateralisation for different
tasks measured using fMRI (e.g. Ramsey, Sommer, Rutten, & Kahn, 2001;
Lee, Swanson, Sabsevitz, Hammeke, Scott, Possing, & Binder, 2008) or for
different brain regions when performing a single task (Bethmann et al.,
2007). It has, however, been difficult to draw firm conclusions due to a
lack of reliable and validated laterality measures and well-powered
studies in this area. An additional problem is the lack of available
data on individual participants in the majority of studies that have
used fMRI, electrophysiology or behavioural methods to assess
laterality.

It is important to clarify the nature of atypical lateralisation,
because cerebral asymmetry has clinical implications for such issues as
epilepsy surgery and recovery from aphasia, as well as informing our
understanding of brain-behaviour associations, and the neurobiological
basis of the human language capacity. For these reasons we designed a
multi-centre study to quantify several measures of cerebral asymmetry
using behavioural and physiological measures, in a sample enriched with
left-handed participants. We acquired a large dataset of reliable
language laterality measures, including behavioural methods and
functional transcranial Doppler ultrasound (fTCD), which is shared
following the principles of open research.

## 1.1 Individual Differences in Language Lateralisation

Woodhead, Bradshaw, Wilson, Thompson and Bishop (2019) and Woodhead,
Thompson, Karlsson and Bishop (2020) compared language lateralisation
for a range of tasks designed to engage different aspects of language
functioning. These studies used fTCD, which quantifies lateralisation by
directly comparing blood flow in left and right middle cerebral arteries
during performance of an activation task. A group of 31 left-handers and
43 right-handers performed six language tasks twice on two occasions.
Tasks were selected based on the dual-stream model of Hickok and Poeppel
(2007), which postulates that language functioning engages two parallel
processing streams. The dorsal pathway feeds forward to the inferior
frontal gyrus where phonological representations are translated into
articulation and is strongly left-lateralised. The ventral pathway,
involving the middle and inferior temporal gyri, maps phonological
representations onto lexical conceptual representations, and is thought
to have weak or absent left-sided dominance. The two streams are not
independent; they are both connected to a left-lateralised combinatorial
network. Woodhead et al. (2019, 2020) selected tasks involving List
Generation (covertly reciting overlearned sequences such as days of the
week, months of the year) and Phonological Decision (judging if pictured
words rhymed) to index dorsal stream activity. To engage the ventral
pathway they selected tasks involving Semantic Decision (judging if
pictured words belonged in the same semantic category) and Syntactic
Judgement (judging whether a series of nonsense words such as "The
tarben yipped a lev near the kruss" had grammatical structure). Tests of
Sentence Generation (covertly describing a picture) and Sentence
Decision (selecting a picture to match the meaning of a spoken sentence)
were predicted to involve both pathways. Woodhead et al. (2019, 2020)
found that a bifactor model did better than a single factor model at
accounting for covariances between laterality indices, but the two
factors did not divide neatly according to ventral/dorsal stream
predictions. Contrary to prediction, the List Generation task did not
show high loadings on either factor, despite taxing articulatory
processes, though it should be noted this task also had much lower
test-retest reliability than other tasks ($r_s = .33$). The strongest
laterality was seen for the Sentence Generation task, which indexed the
first factor. This factor had significant loadings from Phonological
Decision, Semantic Decision and Sentence Decision. All of these tasks
were left-lateralised overall, though to varying extents. The second
factor had loadings from Sentence Decision (which was left-lateralised)
and Syntactic Decision (which was not lateralised). The two factors were
highly correlated in right-handers, but in left-handers they were less
well correlated. The tentative interpretation of these results was that
in some individuals (primarily left-handers) there can be a dissociation
between laterality for language generation (factor 1) and comprehension
(factor 2). The pattern of results also suggested that word retrieval
may be the key process characterising factor 1, rather than the
articulatory aspect of speech production. Accordingly, we refer to this as 'language generation' rather than 'production'. It was noteworthy that the
tasks loading strongly on factor 2 were the only two tasks that involved
auditory presentation of stimuli.

An important feature of the data was that test-retest reliability of the
laterality index was as high for tasks that were poorly lateralised as
for those that were strongly lateralised (Woodhead et al., 2019, 2020).
This challenges the interpretation that these are tasks where both
hemispheres are equally involved. If that were the case, the true
laterality index would be zero for all people, and any individual
variation would just be noise, so test-retest reliability would be low.
Instead, there were stable individual differences for "bilateral" tasks,
but with equal probability of bias to left or right. This does not
preclude the possibility that some people have true "bilateral
language", i.e., equal involvement of both hemispheres, but it does
challenge the idea that tasks that show no bias at the population level
invariably mean there is no bias in individuals. Further evidence comes
from a fTCD study by Woodhead, Rutherford and Bishop (2018), which
included a List Generation task that was not significantly lateralised.
The LI from this task was nevertheless significantly correlated with
laterality indices for word and sentence generation, both of which
showed the usual left-hemisphere bias. Overall, these observations
suggest that there are meaningful, stable, individual differences in
degree of lateralisation, even for tasks that show no bias at the
population level; this is consistent with the notion that different
language functions may be primarily mediated by opposite hemispheres in
some individuals (Bishop, 2013).

The primary goal of the current study is to consolidate the findings of
Woodhead et al. (2019, 2020) by replicating and extending the findings
of dissociated language functions, using some new tasks. We studied a
large sample using online behavioural laterality assessment, plus a
smaller subset of these individuals assessed with fTCD.

## 1.2 Methodological Considerations

Most contemporary studies of brain lateralisation use fMRI, which
provides information about localisation as well as laterality of brain
activation. Estimates of lateralisation from fMRI are dependent on the
experimental task, the specific brain region, and the selection of a
baseline task. Decisions about how to quantify laterality, and selection
of statistical thresholds can lead to different estimates of group and
individual asymmetry (Bradshaw et al., 2017; Seghier, 2008; Wilke &
Lidzba, 2007). In addition, fMRI has poor temporal resolution and is
expensive enough to preclude routine studies using large numbers of
participants. In contrast, fTCD---the method used by Woodhead et al.
(2019, 2020)---allows for direct comparison of blood flow in the left
and right middle cerebral arteries, and has good temporal resolution.
Although fTCD cannot provide information about which brain regions are
active, it is considerably less expensive than fMRI and is portable.
FTCD has also been validated using the gold standard Wada test of
language lateralisation (Knecht, Deppe, Ebner, Henningsen, Huber et al.,
1998). A third approach to the study of functional asymmetry, which
dates back to the 1960s, involves inferring which hemisphere is more
engaged in behavioural tasks when visual or auditory stimuli are
presented in a way that preferentially engages one hemisphere (reviewed
in Bryden, 1982). Accuracy or response time measures can be used to
provide an indication of left- or right-sided bias, both at the group
level and in individuals. We have recently shown that good quality data
can be obtained with some behavioural tasks using online administration,
which makes it feasible to assess very large samples (Parker, Woodhead,
Thompson, & Bishop, 2020).

In principal, it would be of value to use all three approaches with the
same set of participants, to obtain convergent evidence from methods
that make different assumptions and use different approaches to assess
laterality. Ultimately, we aim to adopt that approach: here we made a
start on that goal with a study of individual differences in language
lateralisation that uses the last two of these methods: behavioural
testing in a large sample, followed by fTCD with a subset of the same
participants. We first describe the rationale for selecting specific
measures: the tasks are described in greater detail under Methods.

## 1.3 Behavioural Measures

Our selection of behavioural measures was guided by methodological and
theoretical considerations. In terms of methodology, we have been
exploring the use of online administration for behavioural laterality
tasks (Parker et al., 2020). Given our interest in the nature of
bilateral language, we do not regard it as important that a task shows a
population bias in lateralisation, provided that test reliability is
strong, indicating stable individual differences (see Positive Controls,
below). From a theoretical perspective, we have a particular interest in
contrasting tasks that involve language generation vs. receptive
language, while noting that this distinction is not always clearcut, as
many receptive tasks can involve covert language generation.

One of the first behavioural tasks used to study language lateralisation
is dichotic listening, where a person hears simultaneous streams of
words or speech sounds in left and right ears. Under binaural
presentation, the contralateral auditory pathways take precedence and
the ipsilateral pathways are suppressed. Thus sounds presented to the
right ear have a more direct access to the left hemisphere speech
systems than those played to the left ear, with a strong bias to report
items from the right ear being present at a population-level. Good
reliability (above r = .75) was found for a dichotic listening task
administered via a mobile app (Bless et al., 2013) and we found
similarly high levels of reliability for this task using online
presentation (Parker et al., 2020). Dichotic listening laterality is
not, however, strongly predictive of language laterality as measured by
fMRI (Bethmann et al., 2007; although see S√∏renson & Westerhausen, 2020,
for a reappraisal). This lack of specificity could mean that factors
such as attentional bias affect performance, invalidating the test as a
measure of language laterality, particularly in individual participants.
However, another possibility is that dichotic listening is a good
measure of lateralisation of receptive language, but it may be
dissociable from laterality for language generation.

Another type of behavioural method to assess lateralisation involves
visual presentation. Stimuli are briefly placed in the left or right
visual half-fields, which project primarily to the contralateral
hemisphere. This method has long been used to assess language
laterality, either using written words or pictures as stimuli (Bryden,
1982). Laterality indices show a right visual field advantage (VFA) at
the population level, but results depend crucially on specific aspects
of task design. Laterality indices from such tasks do not, however,
necessarily correlate highly with dichotic listening (Voyer, 1998).
Hunter and Brysbaert (2008) argued that one needs a visual half-field
task involving speech production to obtain good prediction of language
laterality as measured by fMRI. This hypothesis fits with our
theoretical perspective: a visual half-field task that involves language
generation would be expected to be better than dichotic listening for
predicting lateralisation of word generation as measured by fTCD. Van
der Hagen and Brysbaert (2018) reported reliability for three visual
laterality tasks in a sample of 50 left-handers tested on two occasions,
with test-retest correlations ranging from .49 (optimal viewing
position - OVP- for written words), .77 (visual half-field with
pictures) to .83 (visual half-field with words). Parker et al. (2020)
developed a new Rhyme Decision task that involved covert naming, but
reliability was below a prespecified cutoff for acceptability of .65
(Spearman $r_s = .63$). We subsequently gathered pilot data on a
modified version of the task for 15 left-handers and 15 right-handers,
and obtained split-half reliability of 0.74. Contrary to our
expectation, task performance was not significantly lateralised in
either left-handers or right-handers, but the good reliability indicates
it measures a stable individual difference.

The pilot study also gathered data on two further tasks designed to tap
into more receptive aspects of language: the OVP task, and a new Word
Comprehension task, that simply involved selecting which of two
laterally-presented pictures matched a spoken word. Reliability of the
OVP task was relatively poor, but the laterality index from the Word
Comprehension task had split-half reliability of 0.74, again suggesting
there are stable individual differences in lateralisation. In our pilot
data, the Word Comprehension task was significantly lateralised, but in
the opposite direction to prediction, i.e., with better performance for
pictures shown in the left compared to the right visual field. Given our
goal of using reliable tasks that involve language generation or receptive
language, regardless of lateral bias shown on the tasks, we decided to
focus on dichotic listening, Rhyme Decision and Word Comprehension (see
Methods). In practice, our chosen language tasks differ in ways other
than the generative/receptive distinction: one visual half-field task,
for instance, involves written rather than spoken language, and some
tasks require accessing meaning whereas others do not. It would not be
possible to design a battery that completely controlled for all task
variables; rather we planned to administer tasks with diverse
characteristics, predicting that generation vs. reception of language
will determine which laterality indices form a common factor.

## 1.4 Functional Transcranial Doppler Ultrasound Tasks

The fTCD tasks included identical or closely similar versions of four of
the tasks previously used by Woodhead et al. (2019, 2020), all of which
had good test-retest reliability. In that study, two tasks (Sentence
Generation and Phonological Decision) loaded primarily on factor 1
(language generation), and two tasks (Sentence Decision and Syntactic
Decision) loaded primarily on factor 2 (receptive language). For each
factor, one additional task was used: the gold standard Word Generation
task with letter stimuli for language production (Knecht et al., 1998),
and a new Word Decision task for receptive language. The latter task
used the same materials as the online Word Comprehension task described
above; although the two tasks were closely similar, we gave them
different names to make it easier to distinguish the behavioural and
fTCD versions.

## 1.5 Positive Controls

If there are no significant correlations between different tasks it is
important to demonstrate that this is not simply due to use of
inadequate measures or impact of uncontrolled unwanted variables.
Demonstration of good reliability of laterality indices in effect
provides a positive control. Parker et al. (2020) found very weak
intercorrelations between a set of online tasks, despite test-retest
reliability for individual tasks of .7 or above. Woodhead et al. (2019,
2020) showed that for the six tasks used in their fTCD study,
dissociations between LIs for different tasks could not be attributed to
weak reliability, as all laterality indices (LIs) except list generation
showed test-retest reliability (rs) of 0.6 or more). In the current
study, we did not have resources to test all participants twice, but
planned to repeat the online tests for a subset of 50 individuals (50%
left-handers). In addition, split-half reliability using alternate items
was assessed for all measures.

## 1.6 Sampling Approach

One reason for uncertainty about the phenomenon of dissociated language
functions is that laterality measures follow a strongly skewed
distribution, and people with dissociated or atypical lateralisation
are, by definition, rare (Mazoyer et al., 2014; Johnstone et al., 2020).
Some researchers with an interest in atypical lateralisation have
focussed exclusively on left-handers, which gives a higher yield of such
individuals (Van der Haegen & Brysbaert, 2018; Gerrits, Verhelst &
Vingerhoets, 2020). Given our findings that handedness may influence
patterns of association and dissociation of lateralised language
functions, we planned to recruit both left- and right-handers in a 2:1
ratio, to give adequate power to detect such differences.

A further complication is that left-handers do not form a uniform group.
Over many years, various suggestions have been made about possible
subdivisions between types of left-handers: in particular it has been
proposed that right-sided language lateralisation is associated with
extreme left-handedness (Knecht et al., 2000; Mazoyer et al., 2014).
Another common idea is that familial left-handedness distinguishes
between subtypes of left-handers, or may identify a genetic
predisposition to left-handedness in right-handers (McKeever &
VanDeventer, 1977). This notion remains popular, although empirical
support is weak (Orsini, Satz, Soper & Light, 1985). Indeed, it has been
criticised for making no sense in relation to genetic models of
handedness (Bishop, 1980; 1990), which attribute a relatively minor
causal role to genes and a high contribution from chance factors. We did
not make strong predictions about variation within left-handers, but we
gathered data on strength and familiality of handedness that will allow
for exploratory analyses of this topic.

We describe below the rationale for sample size determination. With
online testing, we gathered a large number of participants, which is the
basis for a preliminary test of the 'dissociable language laterality'
hypothesis. The initial sample was recruited according to handedness,
with the goal of having 300 left-handers and 150 right-handers.

In the second phase of the study, we compared findings from the online
measures with those obtained using direct measures of brain
lateralisation from fTCD on a subset of the initial sample. We aimed to
test around half the sample on fTCD as well as online methods, as
simulations indicated that a sample with 112 left-handers and 112
right-handers would be adequately powered to test our hypotheses (see
Sampling and Analysis plan, below). Note that online test results were
not used to select individuals for phase 2: the aim was to test all
available participants until our quota of left- and right-handers was
met.

## 1.7 Research Questions

The overarching question is whether there are cross-hemispheric
dissociations in lateralisation of different language functions, and if
so whether there are separable dimensions of laterality for tasks that
primarily implicate language generation and receptive language. A
positive answer to this question would challenge the conventional
conceptualisation of language lateralisation as a unitary dimension, and
support instead the dissociable language laterality hypothesis.

A subsidiary question is whether dissociation between laterality
dimensions is more characteristic of left- than right-handers.

A final question is whether online behavioural measures are comparable
to direct measures of cerebral blood flow in indexing language
laterality. It is generally assumed that both types of laterality
measurement are indexing the same underlying bias, but the nature of
what is measured is very different: facilitation of processing material
on one side for behavioural measures, and lateralised increase in blood
flow through the middle cerebral artery in the other.

## 1.8 Hypotheses and Predictions

### 1.8.1 Online Behavioural Measures

1.  It is predicted that the pattern of correlation between laterality
    indices from online measures will reflect the extent to which they
    involve language generation, rather than whether they involve spoken
    or written language. Thus we anticipated dissociation between the
    Rhyme Decision task, which requires covert speech production, and
    the Word Comprehension task and Dichotic Listening tasks, which do
    not. We further anticipated that dissociations between tasks are not
    accountable for in terms of low reliability of measures -- i.e.,
    correlations of laterality indices between tasks will be lower than
    split-half reliability of the measures.

### 1.8.2 FTCD Measures

2.  The same hypothesis predicts that the fTCD data will fit a model
    where 'language generation' tasks cluster together on one factor,
    and 'receptive' language tasks on a second factor. The factors will
    be correlated, but the fit of a two-factor model will be superior to
    a single-factor model.\
3.  From our hypothesis that handedness affects language laterality,
    following Woodhead et al. (2020), we predicted that better model fit
    will be obtained when different parameters are estimated for left-
    vs. right-handers, compared with when all parameters are equated for
    the two handedness groups.\
4.  The same hypothesis leads to the further prediction that on
    categorical analysis, individuals who depart from left-brained
    laterality on one or more tasks will be more likely to be
    left-handed than those who are consistently left-lateralised.

### 1.8.3 Relationship Between fTCD and Behavioural Laterality Indices

5.  Our predictions depend on online and fTCD measures indexing the same
    lateralisation processes. On this basis we predict that the
    laterality profile obtained with the online language battery will be
    significantly associated with the profile seen with the direct
    measurement of cerebral blood flow using fTCD, with laterality on
    dichotic listening and Word Comprehension relating more strongly to
    receptive language tasks, and Rhyme Decision to language generation
    tasks.

# 2. Methods

## 2.1 Criteria for Participants

Our original stage 1 flowchart for participant recruitment is now
presented as Figure `r fignum+3` below, showing both the original
planned sample size and the obtained sample size.

The inclusion criteria were as follows:\
‚óè Aged 16-50 years. The younger age limit avoids developmental change in
language skills affecting performance, and the upper limit makes it less
likely that bone density will make it difficult to find a Doppler
signal.\
‚óè Normal or corrected-to-normal vision and normal hearing\
‚óè Native English proficiency\
‚óè Access to a laptop or desktop computer with stereo headphones for use
in the online testing. N.B. it is not possible to do the online tests on
a tablet or phone.\
The exclusion criteria were as follows:\
‚óè A history of psychiatric or neurological illness\
‚óè A history of developmental language disorder, dyslexia or autism.\
‚óè A history of dyspraxia\
‚óè Unwillingness to travel to one of the testing sites for Step 2 of the
study\
‚óè Contraindications or unwillingness to participate in fMRI in future
parts of the study.

`r suppnum<-suppnum+1`

The initial screening questionnaire was administered on the Gorilla
platform (www.gorilla.sc; Anwyl-Irvine, Massonni√©, Flitton, Kirkham &
Evershed, 2020), and is provided in Supplementary Material `r suppnum`.
Note that we did not exclude those who speak more than one language,
provided they met our stringent criteria for native-level competence in
English (see below). We also gathered information on
bilingualism/multilingualism in the initial demographic questionnaire,
so it would be possible to determine if this had any impact on results.
Participants were told that this test was part of a multistage study and
they might be invited back for in-person testing. Although there was no
obligation on them to do so, if they were in principle willing, they had
the opportunity to provide contact details.

Although this study did not include fMRI, we prioritised participants
who were likely to be eligible for fMRI in a future phase of the
research. The initial screening questionnaire checked whether
participants were in principle willing to return for an MRI scan, and
whether they were aware of any contraindications to being scanned.

If the participant passed the screening questionnaire, they were invited
to complete an online consent form prior to starting the online testing
session. Those who took part in the second session (in-person testing
with fTCD) completed an additional written consent form for that
session.

## 2.2 Procedure

There were two stages to the project. Participants who proceeded beyond
the initial screening were invited to complete the online testing (Step
1), and a subset of participants were invited back for the fTCD session
(Step 2). Participants received course credit or were paid in accordance
with guidelines at their local testing centre (at least ¬£8 per hour).\
<!-- From ZW/AP: I think this is too much detail, but 1071 people started online session
807 people passed screening questionnaire (264 did not)
644 people completed online session (163 failed to complete)
621 people were kept in the analysis (23 people did not pass data checks? Not sure about this)
-->

## 2.3 Step 1: Online Testing

After passing the screening questions and completing an online consent
form, participants continued with the online testing via the Gorilla
platform (www.gorilla.sc; Anwyl-Irvin et al., 2020). In this session
participants completed the following (described in more detail below):\
‚óè Demographics questionnaire\
‚óè Edinburgh Handedness Inventory (Oldfield, 1971)\
‚óè A test of ocular dominance and footedness\
‚óè Measures of language proficiency.\
‚óè Tests of language laterality\
- Rhyme Decision\
- Word Comprehension\
- Dichotic Listening

Many of these tests have been reported previously, and have been made
available for reuse: <https://gorilla.sc/openmaterials/104636>.

`r suppnum <- suppnum+1`  

### 2.3.1 Demographics Questionnaire\
The demographics questionnaire is shown in Supplementary Materials
`r suppnum`. Participants were asked to report age, gender, years in
education, and whether they were bilingual. They were also asked about
their own hand and foot preference, and left-handedness in first degree
relatives (parents and siblings - see question 7). The latter
information was used to compute a proportional familial sinistrality
index (Corey & Foundas, 2005).

`r suppnum<-suppnum+1` 

### 2.3.2 Edinburgh Handedness Inventory (EHI)
Participants completed the Edinburgh Handedness Inventory (Supplementary
Material `r suppnum`; Oldfield, 1971) in order to quantify handedness on
a continuum. For 10 activities, participants indicated their hand
preference on a 5-point scale (right hand strongly preferred, right hand
preferred, no preference, left hand preferred, left hand strongly
preferred). This questionnaire was selected for compatibility with prior
studies relating language laterality to hand preference.

### 2.3.3 Test of Ocular Dominance

A version of the Porta test (Porac & Coren, 1976) that is suitable for
online testing was administered to determine each participant's eye
dominance in central gaze (i.e., when looking straight ahead). This test
classifies participants as being either left or right eye dominant.

### 2.3.4 LexTALE

The Lexical Test for Advanced Learners of English (LexTALE; Lemh√∂fer &
Broersma, 2012) was used to assess level of English vocabulary
knowledge. Participants judge the lexical status of 60 letter strings
(word or non-word). Forty are real English words and 20 are non-words.
To correct for the unequal proportion of words and non-words, LexTALE
scores are calculated as ((number of words correct/40 x 100) + (number
of nonwords correct/20 x 100))/2). Following the norms provided by
Lemh√∂fer & Broersma, those scoring below 80 are not eligible for
inclusion in the online testing or fTCD. *N.B. In practice, we did not
implement this exclusion, for reasons stated below.*

### 2.3.5 Games With Words Test

The Games With Words test (Hartshorne, Tenenbaum, & Pinker, 2018) was
used to screen participants for adequate understanding of English
grammar. The first 8 items involve participants reading a sentence, such
as "The dog was chased by the cat", and deciding which of two pictures
presented below the text matches the sentence. The two pictures in this
example include a dog chasing a cat and a cat chasing a dog. Items 9-35
were four-alternative forced choice questions where participants select
which of four sentences sounds most natural: e.g., (1) "What age are
you?", (2) "How age are you?", (3) "How old are you?", and (d) "What old
are you?". To be included as having native English speaker proficiency,
participants need to make no more than 3 errors on this test: Hartshorne
et al. (2018) reported that most monolingual English-speakers performed
close to ceiling and very few made more than 3 errors. *N.B. In
practice, we did not implement this exclusion, for reasons stated
below.*

### 2.3.6 Rhyme Decision

A modified version of a Rhyme Decision visual half-field task reported
by Parker et al. (2020) was administered to determine brain
lateralisation for language generation. This involved participants
judging which of two parafoveal images rhymed with a foveally presented
word. The laterality index from the original task had test-retest
reliability of r = .63, and the overall lateralisation effect, though
significant, was small. We modified the task from the original with the
aim of improving its psychometric properties: first, we removed trials
in which neither of the pictures rhymed with the target word, as these
were potentially confusing. Second, we increased the distance between
the centrally presented word and parafoveal images, to ensure the image
is projected exclusively, at least initially, to the contralateral
hemisphere. Pilot testing with 30 participants obtained split-half
reliability of r = .74 with this version. *Note, however, lower
reliability was found with our main sample, as reported in Results*.

`r suppnum <- suppnum+1`  
#### 2.3.6.1 Materials
Written stimuli consisted of twenty-six monosyllabic written words (e.g.
bite). Stimulus pairs were created so that each written word was paired
with an image with a name that rhymed (e.g. kite). For each of the 26
word-image pairs there was a corresponding pair whose words did not
rhyme with the first pair (e.g. the corresponding pair for bite-kite was
more-door), see Supplementary Materials `r suppnum`.. On each trial,
stimuli were presented such that the written word was accompanied by one
rhyming image and one non-rhyming image (e.g. kite-bite-door). All
possible combinations were included such that each pairing constituted
four individual items. Thus, there were 52 unique stimuli, and all
images appeared both as rhymes and non-rhymes (see Parker et al., 2020,
for further detail).\
Word stimuli were presented in 28 pt black Courier New font on a white
background. The images were displaced at 7.9 degrees of visual angle
from the point of fixation. Gorilla's screen scaling tool was used to
maintain consistency of stimulus size across browsers and computers.

#### 2.3.6.2 Procedure

Participants completed a familiarisation procedure where they viewed
each image pair. The images were shown with their name presented in text
below the image to ensure that participants used the appropriate word
when making a rhyme decision. They then completed 208 trials of the
Rhyme Decision task (four blocks of 52 stimuli). Each trial began with a
central fixation cross which was visible for 800 ms after which a
foveally presented word appeared for 200 ms. Two bilateral images then
appeared for 150 ms. At stimulus offset, participants indicated whether
the centrally presented word rhymed with the image present to the left
or right visual field by pressing S for the left visual field and K for
the right visual field. Participants' responses triggered the next
trial. Accuracy and response times were recorded.

### 2.3.7 Word Comprehension task

A novel online Word Comprehension task was administered to determine
lateralisation of receptive language. This task involved indicating
which of two semantically related parafoveal images matched an orally
presented word stimulus. Pilot testing with 30 participants and one
block of stimuli obtained split-half reliability of r = .76. *Note,
however, lower reliability was found with our main sample, as reported
in Results.*

`r suppnum <- suppnum+1`  
#### 2.3.7.1 Materials  
A total of 108 experimental images were selected from the MultiPic
databank (Du√±abeitia et al., 2018). As Du√±abeitia et al. had
participants name stimuli, we were able to select images where at least
50% of English participants generated the intended name; M= 90.1%, SD=
13.06. The names of the images were high frequency according to Zipf
scores from the SUBTLEX-UK database (Van Heuven, Mandera, Keuleers, &
Brysbaert, 2014); M = 4.4, SD = 0.41, minimum = 3.8. Each image was
paired with another to form a semantically related pair, amounting to 54
pairs (see Supplementary Materials `r suppnum`. Latent semantic analysis
ratings were acquired using the LSA CU Boulder web-interface
(<http://lsa.colorado.edu/>). LSA scores ranged from 0.26 to 0.87; M =
0.46, SD = 0.87. We aimed to avoid errors arising from visual confusion
between pictures, and to this end, nine raters rated the pairs of images
for visual similarity on a 5 point scale (1: very similar to 5: not all
similar). Generally, image pairs were rated as being visually distinct;
M = 4.13, SD = 0.47. Audio files of the spoken name for each picture
were created using Google cloud text-to-speech
(<https://cloud.google.com/text-to-speech>), using a male, British
voice.

Each image pair was presented a total of four times, with each image
twice in either the left or right visual field. The name of each image
was presented twice: once when the image was in the right visual field,
and once when the image was in the left. `r fignum <- fignum+1`See
Figure `r fignum` for an example. The images were displaced at 7.9
degrees of visual angle from the point of fixation. Gorilla's screen
scaling tool was used to maintain consistency of stimulus size across
browsers and computers. In total, 208 trials were presented across four
blocks.

### Figure `r fignum`

![Schematic illustration of a trial on the Word Comprehension task.
First, participants are presented with a fixation cross for 600 ms. A
word, in this case 'drum', is presented aurally, along with two
semantically related parafoveal images that are visible for 150 ms.
Participants then indicate which of the two images matched the word. In
this case participants would indicate that the word matched the image on
the right. *This figure has been modified from the preregistered version
for clarity.*](figs/WordCompDemo.pdf)

#### 2.3.7.2 Procedure

Participants completed a familiarisation procedure (where each picture
was shown along with its spoken name) and a number of practice trials.
They then completed 208 experimental trials (four blocks of 52 stimuli).
Each trial began with a central fixation cross which was visible for 600
ms. The target word was then presented aurally along with two
semantically related parafoveal images that were visible for 150 ms.
Participants indicated whether the oral word matched the image in the
left or right visual field by pressing Q for the left visual field and P
for the right visual field. Participants' responses triggered the next
trial. Accuracy and response times were recorded.

### 2.3.8 Dichotic Listening Task

An online Dichotic Listening task was administered to assess the
lateralisation of speech perception (Hugdahl & Andersson, 1986). On each
trial, participants heard two consonant-vowel (CV) auditory stimuli
simultaneously to each ear. The stimuli have previously been
administered online via an app (Bless et al., 2013) and Gorilla (Parker
et al., 2020) and the task has good test-retest reliability (r = .78).

#### 2.3.8.1 Materials

Six stop-consonants (/b/, /d/, /g/, /p/, /t/, /k/) were combined with
the long vowel /a/ to create consonant-vowel stimuli. Stimuli are paired
in all possible combinations and played in each sound channel (e.g.
/pa/-/ga/). This resulted in 36 unique pairings (including pairs with
the same sound repeated).

#### 2.3.8.2 Procedure

To ensure adequate headphone use, participants were screened on two
measures. The first, described by Woods, Siegel, Traer, and McDermott
(2017), involves participants deciding which of three pure tones is the
quietest. One of the three tones is played 180 degrees out of phase, so
this task is difficult to perform through speakers but relatively easy
with headphones. The second task was a stereo check developed by Parker
et al. (2020). This task involves participants listening to a sound
played in a single channel and reporting whether the sound was played to
the left or the right ear via a button press. Each task had six trials.
We excluded participants who scored less than 4 correct on each.

Participants completed three blocks of the Dichotic Listening task. This
amounted to 96 trials when excluding homonyms. Our decision to use three
blocks was based on the previous observation that there is not much
improvement in reliability after 85 trials when using the Dichotic
Listening task (Parker et al., 2020). Each trial began with a fixation
cross, which was presented for 250 ms. Participants then heard the sound
pairs and reported the syllable that they heard. If they heard two
different syllables, participants were instructed to report the sound
that they heard the most clearly. Responses were made by clicking a
button which corresponded to one syllable. The response triggered the
start of the next trial. The side of selected response and response
times was recorded. Errors can occur on this task if the participant
selects a syllable that was not presented to either ear: these were
rare, but were recorded and were used to exclude those who performed
poorly.

### 2.3.9 General Procedure

The study implemented a within-subjects design where participants
completed all online tasks. When completing the battery, participants
were instructed to sit approximately 50 cm from the screen. Participants
completed four blocks of the Rhyme Decision (A) and Word Comprehension
(B) tasks (52 trials each). They completed three blocks of the Dichotic
Listening (C; 36 trials each). Blocks of different tasks were
interspersed in a quasi-random order (i.e., ABC, BAC, CAB). Other online
tasks relating to different studies were interspersed to avoid boredom
and maximise efficiency of data collection.

## 2.4 Step 2: Functional Transcranial Doppler Ultrasound (fTCD)

A subset of participants were given six tasks using fTCD that could be
administered in a single session of about 90-120 mins. These
participants were selected on the basis of handedness and willingness
and ability to travel for in-person testing to one of the *six test
centres (Oxford, University College London, Bangor, Lincoln, Lancaster,
University of Western Australia)*.\
The tasks were designed to cover a range of language functions in as
standard a format as possible. Four of the language tasks (tasks B, C, E
and F below) were based on tasks used by Woodhead et al. (2019). We
omitted the List Generation task used by Woodhead et al. (2019), as it
showed poor test-retest reliability. Instead, we used a shortened
version of the gold standard Word Generation task (Knecht et al., 2000).
As detailed below, the basic procedure for this task was the same as
used by Knecht et al., but with fewer trials and a shorter rest period
between trials. The sixth task was a new Word Decision task, selected to
act as a third indicator of receptive language.\
The six tasks are described below.

### 2.4.1 Task A: Word Generation

On each trial, the task was to silently generate words that begin with a
specified letter, and subsequently report them when a cue is given. The
task used 18 letters that commonly begin English words (S, C, P, D, T,
B, R, A, E, F, G, H, I, L, M, U, O, W).

### 2.4.2 Task B: Sentence Generation

This task was based on Mazoyer et al. (2014). Participants were shown a
line drawing in each trial and asked to produce a meaningful sentence to
describe it, following a prescribed simple structure, (e.g., "The boy in
the hat flew the kite"). The 18 stimuli were selected from those used by
Woodhead et al. (2018).

### 2.4.3 Task C: Phonological Decision

Following a familiarisation task, for each trial, participants decided
whether the names of pairs of pictures rhymed. The design of the
original task from Woodhead et al. (2019) was supplemented by adding a
number of black and white drawings from the MultiPic database
(Du√±abeitia et al., 2018), in order to create stimuli for three new
trials.

### 2.4.4 Task D: Word Decision

Participants were presented with pairs of pictured items on each trial,
and were asked to press a key to indicate which one matched a spoken
word. The pictures were from the same semantic category. The picture
pairs and audio stimuli for this task were the same as those used in the
online Word Comprehension task (see below for further details).

### 2.4.5 Task E: Sentence Decision

Two pictures were presented on each trial, and the task was to determine
which one matched a spoken sentence. Items were based on picture pairs
taken from the Test for Reception of Grammar - 2 (Bishop, 2003), using
distractors that differed in syntactic arrangement of words. As we used
slightly more trials than Woodhead et al. (2019), additional pictures
and sentences were devised in the same way. New spoken sentences were
created for all items using Google text to speech, with a male British
voice.

### 2.4.6 Task F: Syntactic Decision

On each trial, participants were presented with a sequence of words and
non-words, and asked to judge if they formed a plausible "Jabberwocky"
sentence with correct syntactic structure. Simultaneous spoken and
written presentation was used. The stimuli used by Woodhead et al.
(2019) were supplemented with additional Jabberwocky stimuli from
Fedorenko et al. (2010).

`r fignum <- fignum+1` Timings for the tasks are shown in Figure
`r fignum`. All stimulus materials for the tasks are available on OSF:
(<https://osf.io/g3qms/?view_only=a6c36957ffba4bc39232d9265ea13dd8>). We
previously used 15 trials of each task but increased this to 18 trials
for the current study, to ensure that there were sufficient stimuli for
reliable estimation of a laterality index, even if some trials were lost
due to recording problems. Previously we presented tasks with an
inter-stimulus interval (ISI) of 33 seconds (including 20 s of the task
and 10 s of rest). We increased this by extending the rest period to 15
s. Hence, the ISI was 38 seconds and each task lasted 11 minutes 24
seconds in total. The overall testing time (excluding set-up, practice
trials and breaks) was 68 minutes 24 seconds.

Task order was counterbalanced between participants to avoid order
effects using a replicated Latin square design using a customised script
in R Studio. Details of task A are described by Woodhead et al. (2018),
and Tasks B, C, E and F by Woodhead et al. (2019). Task D was designed
for this study and is described below.

### Figure `r fignum`

![Timings within a single trial for the six tasks used with
fTCD](figs/tasktimings.pdf)

Task D (Word Decision) used the same 54 picture pairs and audio stimuli
that were used for the online Word Comprehension task. The procedure for
this task in fTCD has been designed to match that of the Sentence
Decision task (task E). In each epoch, a pair of drawings was presented
for 3.33 seconds, one above the other, and the spoken word for one of
the drawings was played. Participants were required to respond by button
press to indicate which drawing matched the spoken word. Each pair of
drawings was presented twice, with a different drawing used as the
target word, creating 108 epochs in total. Epochs were presented in a
pseudorandomised order, with no repetitions of a drawing pair in
successive epochs. The same order was used for all participants. The
target location was pseudorandomised so that the target was presented at
the top / bottom of the screen (and therefore eliciting a left / right
button press) in 50% of all epochs to avoid a response bias. In each
fTCD trial, six epochs (drawing pairs) were presented, each lasting 3.33
seconds. The pseudorandom order was designed so that within an fTCD
trial, there were equal numbers of top or bottom targets. This ensured
that odd-even split-half reliability data would not be affected by
trial-to-trial variation in which hand is used. Participants were
instructed to respond as quickly and accurately as possible.

## 2.5 Computing Laterality Indices

### 2.5.1 Online Battery Tasks

The Edinburgh Handedness Inventory was scored in the standard way,
reflecting how often the left / right hands were used across all the
items in the inventory. This score was converted into an index as
described below. Indices greater than 0 were categorised as
right-handed, and indices less than 0 as left-handed. The Porta Test
classifies participants as being either left or right eye dominant.\
For the Rhyme Decision and Word Comprehension tasks, each participant's
RT for correct trials was used to calculate a laterality index that
corresponds to a z-score, known as a LIz score. This can be readily
derived from a t-test conducted on each participant's individual data,
where accurate log response times (after outlier exclusion) are the
dependent variable and visual field is the independent variable (see
Parker et al., 2020). This estimates sensitivity to stimuli presented in
either visual field and acts as a laterality index. The LIz score is
very highly correlated with the more traditional Laterality Index,
computed as (L-R)/(L+R), but it allows one to identify participants who
show a statistically significant RT advantage for one side, i.e., where
the LIz on a 2-sided test has p \< .05.\
For Dichotic Listening, a laterality index was calculated based on
trials in which participants correctly identified one of the two
consonant-vowel sounds. The count of correct responses that corresponded
to each side was used to generate an accuracy laterality index. The
index was calculated in the traditional fashion: 100 x (Left -- Right) /
(Left + Right). The laterality index allows us to relate our findings to
prior research that uses this index. In addition, we computed a
LIz-score for each participant, using the formula:\
z = (pL-.5)/sqrt((pL.pR)/n)\
where pR is the proportion of R responses, pL is the proportion of L
responses, and n is the total L and R responses. As with the Rhyme
Decision task, the z-score is highly correlated with the traditional
laterality index, but has the advantage that it can be used to test
whether an individual's lateral bias is unlikely to have arisen by
chance.\
*We made two small modifications to the pre-registered plan; first we
flipped the sign where necessary to ensure that left-hemisphere
superiority was reflected in a positive score on all measures. This has
no material effect on any computations, but gives better consistency
with other research. Second, for the laterality z-score on Dichotic
Listening, scores were censored at +/- 10, to avoid undue influence from
a handful of extreme scores (participants who responded overwhelmingly
to one ear).*

## 2.5.2 Functional Transcranial Doppler Ultrasound Tasks

Following Woodhead et al. (2019) we calculated a laterality index using
a customised R script (R Core Team, 2016). The cerebral blood flow
velocity (CBFV) data were first down-sampled from 100 to 25 Hz and then
segmented into epochs. Spiking or dropout data-points were identified as
being outside of the 0.0001--0.9999 quantiles of the CBFV data. If only
a single artefact data-point was identified within an epoch, it was
replaced with the mean for that epoch. If more than one data-point was
identified, the epoch was rejected. The CBFV was then normalised (by
dividing by the mean and multiplying by 100) such that the values for
CBFV become independent of the angle of insonation and the diameter of
the middle cerebral artery. Heart cycle integration was then used to
normalize the data relative to rhythmic modulations in CBFV. *The
pre-registration document stated that "Epochs are baseline corrected
using the interval from ‚àí10 to 0 s pre-stimulus time (where the onset of
the 'Clear' stimulus is used as the start of the trial, 0 s pre-stimulus
time)." Woodhead et al used a shorter interval of -5 to 2 seconds for
baseline correction to avoid activity from the prior trial influencing
the baseline. We had aimed to avoid that problem by having slightly
longer epochs, but inspection of blood flow plots (see Figure
`r fignum+4` below) showed this was not the case, and so we reverted to
the baseline of -5 to 2 s, consistent with the baseline period used by
Woodhead et al. For completeness, all key analyses have been re-run
using the original pre-registered baseline, to confirm this has minimal
effect on outcomes (see Supplementary file `r suppnum+3`).*\
Finally, artefacts were identified as values below 60% and above 140% of
the mean normalised CBFV: any epochs containing such artefacts were
rejected. The laterality index was then computed as the mean difference
between blood flow velocity in left and right channels over a period of
interest that is specified in advance for each task. For tasks without
an overt speech 'report' stimulus (tasks C-F), the period of interest
was from 6-23 s peri-stimulus time to cover the whole period where the
participant is performing the decision task; left-hemisphere blood flow
typically increases as each item in the trial is responded to. For tasks
with a 'report' stimulus (tasks A-B) the period of interest was from
6-17 s to avoid capturing activity related to the overt speech
production. The standard error of the laterality index for each
individual was computed from the laterality index obtained across
individual trials, and was used both to identify outliers (individuals
with unreliable laterality indices) and to categorise individuals in
terms of direction of laterality. When the 95% confidence interval of
the laterality index spanned zero, laterality was coded as bilateral;
otherwise, it was coded as left or right depending on the sign of the
difference.\
In the previous study by Woodhead et al. (2019) we excluded cases with
fewer than 12 acceptable trials and then used the Hoaglin-Iglewicz
(1987) criterion for outlier detection: this involves identifying cases
that have values well outside the interquartile range for the group.
This was used to identify individuals where the laterality index for a
given task had an unusually large standard error, indicative of high
trial-by-trial variation. However, we noted that some individuals had
very low standard errors, despite having fewer than 12 usable trials,
and so in the current study our preregistration specified a minimum
number of 10 trials (out of 18 trials administered), while retaining the
Hoaglin-Iglewicz method for removing data for a given subject and
condition when the standard error of the laterality index was high.

## 2.6 Sampling and Analysis Plan

The analysis starts with presentation of descriptive data, including
distributions of scores by handedness, and split-half reliability of
measures. The hypotheses are then tested, following the preregistration
from Table 1 of our original Stage 1 report. The preregistered text is
presented with each analysis. All analyses are conducted with alpha =
.02 and power .9. A Rmarkdown script to run analyses on simulated data
is available on Open Science Framework:
<https://osf.io/9dbrg/?view_only=357994fa8f6b49ee83964f5108d82ee2>.

We report how we determined our sample size, all data exclusions, all
inclusion/exclusion criteria, whether inclusion/exclusion criteria were
established prior to data analysis, all manipulations, and all measures
in the study.

```{r makeflowchart,echo=F,message=F,warning=F}

testtab <- table(combdat$Rhanded,combdat$ftcd)

rownames(testtab)<-c('Left-handed','Right-handed')


LH1 <- testtab[1,1]+testtab[1,2]+testtab[1,3]
RH1 <-testtab[2,1]+testtab[2,2]+testtab[2,3]


langtab<-table(combdat$Rhanded,combdat$langgroup)

LH1a <- langtab[1,1]
RH1a <-langtab[2,1]
LH1b <- langtab[1,2]
RH1b <- langtab[2,2]
LH1c <- langtab[1,3]
RH1c <- langtab[2,3]
LH1d <-langtab[1,4]
RH1d <-langtab[2,4]


thisdat<-combdat[combdat$ftcd>0,] #all with 1 or 9 for ftcd
w<-which(thisdat$ftcd==9)
thisdat$DopExclude[w]<-9 #need to exclude those with poor quality data at this next step
testtab2<-table(thisdat$Rhanded,thisdat$DopExclude)

LH2 <- sum(testtab2[1,]) #here we include those who had unusable data
RH2 <- sum(testtab2[2,])



dopdat<-combdat[combdat$ftcd==1,]

LH4 <- testtab2[1,1]
LH5 <- testtab2[1,3]
LH5a <- testtab2[1,2]
RH4 <- testtab2[2,1]
RH5 <- testtab2[2,3]
RH5a <- testtab2[2,2]

dopdat<-dopdat[dopdat$DopExclude==0,] #remove those with poor quality ata
langtab<-table(dopdat$Rhanded,dopdat$langgroup)

LH6a <- langtab[1,1]
RH6a <-langtab[2,1]
LH6b <- langtab[1,2]
RH6b <- langtab[2,2]
LH6c <- langtab[1,3]
RH6c <- langtab[2,3]
LH6d <-langtab[1,4]
RH6d <-langtab[2,4]

#A, B, C are the main boxes, X, Y Z etc are the labels on the left. By creating links between them coloured white, they are correctly positioned but we don't see the links
#Labels for boxes are defined below in section with square brackets

myflow<-grViz("digraph flowchart {
  node [fontname = arial, shape = plaintext]
          X[label='Screening\\nquestions']
          Y[label= 'Step 1\\nOnline testing']
          Z[label = 'Step2\\nfTCD for subset']
          U[label= 'fTCD quality']
          V[label = 'Language screen']
   node [fontname = arial, shape = rectangle, color = black]
  A [label = '@@1']
  B[label='@@2']
  E [label='@@3']
  F [label='@@4']
  G [label='@@5']
  H [label='@@6']
  I [label='@@7']

  A -> B 
  B-> E
  E ->{F,G}
  F -> {H,I}

  
      X -> Y [alpha=0,color='white']
      Y -> Z [alpha=0,color='white']
      Z -> U [alpha=0,color='white']
      U -> V [alpha=0,color='white']
              }
}
  
  [1]: paste0('Aged 16-50; Hearing and vision;','\\n','Native language; Neurological','\\n', 'history; Developmental disorders;','\\n', 'fMRI screening; Willingness to','\\n','attend Session 2')
  [2]: paste0('Demographic questionnaire','\\n','Language proficiency measures','\\n','Handedness/footedness/eyedness','\\n','3 lateralisation tasks','\\n','Planned: 300 L-handed + 150 R-handed', '\\n','Actual: ',LH1,' L-handed + ',RH1,' R-handed','\\n','(Retested: 29 L-handed + 24 R-handed)' )  
  [3]: paste0('At Oxford, Bangor, London','\\n',' Lincoln, Lancaster, or UWA','\\n','6 language tasks','\\n',LH2,' L-handed + ',RH2,' R-handed')
  [4]: paste0('Included','\\n', LH4,' L-handed + ',RH4,' R-handed','\\n','(Planned: 112 L-handed + 112 R-handed)')
  [5]: paste0('No signal: ','\\n', LH5,' L-handed + ',RH5,' R-handed','\\n','Poor quality:','\\n ',LH5a,' L-handed + ',RH5a,' R-handed')
  [6]: paste0('H: High proficiency','\\n','Native speaker:','\\n', LH6a,' L-handed + ',RH6a,' R-handed','\\n','Non-native speaker','\\n', 'passed screen:','\\n', LH6b,' L-handed + ',RH6b,' R-handed')
  [7]: paste0('M: Moderate proficiency','\\n','Non-native speaker','\\n', 'failed screen:','\\n', LH6c+LH6d,' L-handed + ',RH6c+RH6d,' R-handed')
")

makefile<-1
if(makefile==1){
  filename<-here("figs/COLAflow.eps")
  #myflow %>% export_svg %>% charToRaw %>% rsvg %>% jpeg::writeJPEG(filename)
  tmp <- export_svg(myflow)
  rsvg_eps(charToRaw(tmp), filename, width=3543) # Should be 9cm, 1000dpi
}


```

```{r graphicalabsbit,echo=F}
#Tried a minimised version of the flowchart for graphical abstract, but in the end omitted it.
mygraphabs<-grViz("digraph flowchart {
  node [fontname = arial, shape = plaintext]
          X[label= 'Step 1\\nOnline behavioural\\ntesting']
          Y[label = 'Step 2\\nFunctional transcranial\\nDoppler ultrasound']
          Z[label= 'After exclusions']
   node [fontname = arial, shape = rectangle, color = black]
  A [label = '@@1']
  B[label='@@2']
  C [label='@@3']


  A -> B 
  B-> C


  
      X -> Y [alpha=0,color='white']
      Y -> Z [alpha=0,color='white']

              }
}
  

  [1]: paste0('Demographic questionnaire','\\n','Handedness/footedness/eyedness','\\n','3 lateralisation tasks','\\n',LH1,' L-handed + ',RH1,' R-handed')  
  [2]: paste0('At Oxford, Bangor, London','\\n',' Lincoln, Lancaster, or W.Australia','\\n','6 language tasks','\\n',LH2,' L-handed + ',RH2,' R-handed')
  [3]: paste0(LH4,' L-handed + ',RH4,' R-handed')

")


  filename<-here("figs/graphabsflow.eps")
  tmp <- export_svg(mygraphabs)
  rsvg_eps(charToRaw(tmp), filename, width=3543) # Should be 9cm, 1000dpi


```
# 3. Results

## 3.1 Participants

`r fignum<-fignum+1` 
### Figure `r fignum`  

![Participant recruitment flowchart](figs/COLAflow.eps)

### 3.1.1 Departures From Pre-registration Plan

Our original flowchart had stated there were four online behavioural
laterality tasks, but, as stated in the Stage 1 report, one of these had
been dropped after pilot testing showed it had poor reliability, and
only three tasks were therefore used. The original flowchart from the
Stage 1 report was in error in mentioning 4 tasks, and this is corrected
in Figure `r fignum`.

Our plan had been to recruit 300 left-handers and 150 right-handers for
the online behavioural battery, and from these to select 112
left-handers and 112 right-handers for in-person testing. Because of the
Covid pandemic, the time periods when it was possible to test in person
were greatly restricted, and testing had to be carried out under strict
conditions to ensure safety (researchers wearing full personal
protective equipment in a ventilated space, and cleaning of equipment
between participants). Furthermore, researchers and participants could
become unavailable for testing at short notice because of a positive
Covid test or notification of a contact with an infected person, and
some participants were understandably reluctant to attend in-person
testing.

The disruption due to these factors meant we did not meet our target
numbers for in-person testing, despite over-recruiting for online
testing. We decided to relax the criteria for language competence of
participants; rather than excluding people, it seemed preferable to
include at Step 2 any participant who had completed Step 1 and was
willing to come for testing, and then check retrospectively to see
whether inclusion of these participants had any impact on the results.
This gives a larger sample, which gives more confidence that null
results are truly null.

To justify this approach, we considered whether either raw or absolute
laterality indices were correlated with either of the language screening
measures and showed they were not (See section 3.5 below and
Supplementary file `r suppnum<-suppnum+1` `r suppnum`
`r suppcorr<-suppnum`).

Figure `r fignum` presents the original preregistered recruitment
flowchart modified to show actual numbers recruited. In total we tested
`r sum(testtab[1,])` left-handers, `r sum(testtab[1,2:3])` of whom were
seen for fTCD assessment, and `r sum(testtab[2,])` right-handers,
`r sum(testtab[2,2:3])` of whom were seen for fTCD assessment.

Figure `r fignum` also shows the new rather than preregistered approach
to the language screen. Group H (high proficiency) corresponds to cases
who meet preregistered criteria: they are either native English
speakers, or non-native speakers who passed the preregistered criterion
for language competence (LexTALE of 80 or more, and no more than 3
errors on full Games with Words test). Group M (moderate proficiency)
failed the language screen. In the original flowchart, the language
screen was used to exclude those in group M prior to Step 2, whereas in
the final study, we included these individuals. In Supplementary file 3
we compare the current results with those obtained using the original,
stringent language cutoff (i.e., excluding Group M), and show that
differences are minimal.

### 3.1.2 Demographic Data

```{r makedemog.table,echo=F}
#We'll use the table1 package to create a nice-looking summary table for demographics; this excludes those excluded on language tests, but otherwise includes everyone given the online testing.

mycomb <- combdat #this is a hangover from previuos - we now exclude by langgroup early on in the script, so combdat has correct people excluded on Lextale or Grammar task

mycomb$Gender <- factor(mycomb$male,levels=c(0,1),labels=c("Female","Male"))
label(mycomb$age) <- "Age (yr)"
mycomb$Native <- factor(mycomb$nativeEnglish,levels=c(0,1),labels=c("No","Yes"))
label(mycomb$Native)<-"Native English speaker"
label(mycomb$EHI.LI)<-"Edinburgh Handedness LI"
mycomb$Bilingual <- factor(mycomb$bilingual,levels=c("No","Yes"),labels=c("No","Yes"))
mycomb$Handedness <- factor(mycomb$Rhanded,levels=c(0,1),labels=c("Left-Handed","Right-Handed"))
mycomb$ftcd <- factor(mycomb$ftcd,levels=c(0,1),labels=c("No FTCD data","With FTCD data"))

demog.table <- table1(~ Gender + age + Native+ Bilingual+EHI.LI|(factor(ftcd)+Handedness) , data=mycomb,overall=F)
ftab <- t1flex(demog.table) #convert from table1 to flextable format to allow formatting of width etc
ftab<- fit_to_width(ftab,8)
#ftab #This line prevents knitting to Word
tabnumber<-tabnumber+1

# Calculate delay between session 1 and session 2 (fTCD)
for (i in 1:length(myftcd$ID)){
  myftcd$date_S1[i] <- combdat$date[which(combdat$ID == myftcd$ID[i])]
}

myftcd$fTCD_Date <- as.Date(myftcd$fTCD_Date, '%d/%m/%Y')
myftcd$date_S1 <- substring(myftcd$date,1,10) #strip off first 10 character for yr/month/day
myftcd$date_S1 <- as.Date(myftcd$date_S1, '%d/%m/%Y')
myftcd$date_diff <- difftime(myftcd$fTCD_Date, myftcd$date_S1, units = "days")

ftcd_date_diff_stats <- fivenum(myftcd$date_diff)



```

Table `r tabnumber`: *Demographic characteristics of sample*

Table `r tabnumber` shows demographic data for the subset of individuals
tested on the online behavioural battery only, and the subset who also
completed the session with fTCD. To obtain the total number completing
the online session, the No FTCD data and With FTCD data columns should
be summed. It is evident from inspection that differences between the
two subgroups are not substantial. The median time difference between
the online battery and the fTCD session was `r ftcd_date_diff_stats[3]`
days (range: `r ftcd_date_diff_stats[1]` to `r ftcd_date_diff_stats[5]`
days).

### 3.1.3 Subsample for Test-Retest Study

```{r test-retest, echo=F}
#need to read in sess1 and sess3 for the test-retest data

sess1 <- read.csv(here('data/sess1.csv'))
sess3 <- read.csv(here('data/sess3.csv'))
sess3$EHIcat<-0 #modified to use EHI for categorising handedness, not writing hand
sess3$EHIcat[sess3$EHI.LI>0]<-1
mtab<-table(sess3$male,sess3$EHIcat)
sess1$date <- substring(sess1$date,1,10)
sess3$date <- substring(sess3$date,1,10)
# Calculate delay between session 1 and session 3 for retest participants
sess3$date_diff<-as.Date(sess3$date,'%d/%m/%Y')-as.Date(sess1$date,'%d/%m/%Y')

sess3$date_diff<-as.numeric(sess3$date_diff)

retest_date_diff_stats <- fivenum(sess3$date_diff)

```

We had preregistered that we would do a test-retest reliability check on
online behavioural tests for 50 participants. In practice, a subsample
of `r nrow(sess3)` participants was retested on part of the online
battery within `r ceiling(as.numeric(retest_date_diff_stats[5]) / 7)`
weeks of the first session (median = `r retest_date_diff_stats[3]` days,
range = `r retest_date_diff_stats[1]` to `r retest_date_diff_stats[5]`
days). There were `r mtab[1,1]` left-handed females, `r mtab[2,1]`
left-handed males, `r mtab[1,2]` right-handed females, and `r mtab[2,2]`
right-handed males. The retest session included the Rhyme Decision and
Word Comprehension tasks, which were new, but not Dichotic Listening,
for which we had adequate evidence of test-retest reliability from the
previous study by Parker et al (2021).

## 3.2 Step 1: Online Behavioural Testing

```{r behdataclean,echo=F}
#table(combdat$DLsame.corr,combdat$excludeDL)
#uncomment to view data - confirms 75% cutoff for exclusion
w<-which(combdat$DLsame.corr<.75)
nDLex <- length(w)
w<-which(combdat$RDT.pcorr<.75)
nRDex <- length(w)
w<-which(combdat$WC.pcorr<.75)
nWCex <- length(w)
```

### 3.2.1 Exclusions

We adopted the same procedures as Parker et al (2021) for excluding
participants. On Dichotic Listening, `r nDLex` participants were
excluded because their accuracy on 'same' trials (where both ears heard
the same syllable) was less than 75%. On Rhyme Decision and Word
Comprehension `r nRDex` and `r nWCex` participants respectively were
excluded because overall accuracy was less than 75%. No exclusions were
made on the basis of the size of ear difference, which was substantial
in some cases.

### 3.2.2 Derivation of Laterality Indices

```{r dichotic.plot,echo=F}


mycomb$DLsig <-0
w<-which(abs(mycomb$DL.zlat)>1.96)
mycomb$DLsig[w]<-1


DLdat <- mycomb[mycomb$excludeDL==0,]
DLsides <- ggplot(DLdat, aes(x=DL.L, y=DL.R, color=Handedness,shape=as.factor(DLsig))) +
  xlab("N correct L ear")+
   ylab("N correct R ear")+
  geom_point()+
  ggtitle("Dichotic Listening") +
  scale_shape_manual(name="Significantly lateralised",
                     labels=c("No","Yes"),
                     values=c(4,16))+
  scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
  geom_abline(intercept = 0, slope = 1)+
  annotate("text", x=50, y=100, label= "Above diagonal:\nR Ear Advantage",size=2.8) +
    annotate("text", x=71, y=44, label= "Below diagonal:\nL Ear Advantage",size=2.8) + 
  theme(text = element_text(size = 8)) 

ggsave(here("figs/AFig1_DLsides.eps"),width = 14, height = 9.5, units="cm", dpi=300)




```

```{r RDT,echo=F}
mycomb$RDTsig <-0
w<-which(abs(mycomb$RDT.zlat)>1.96)
mycomb$RDTsig[w]<-1


RDTdat <- mycomb[mycomb$excludeRDT==0,]
RDTsides <- ggplot(RDTdat, aes(x=RDT.Lmean, y=RDT.Rmean, color=Handedness,shape=as.factor(RDTsig))) +
  xlab("Log mean correct RT (ms) L VHF")+
   ylab("Log mean correct RT (ms) R VHF")+
  geom_point()+
  ggtitle("Rhyme Decision") +
  scale_shape_manual(name="Significantly lateralised",
                     labels=c("No","Yes"),
                     values=c(4,16))+
  scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
  geom_abline(intercept = 0, slope = 1)+
    annotate("text", x=6.6, y=7.25, label= "Above diagonal:\nLVF Advantage",size=2.8) +
    annotate("text", x=7.1, y=6.5, label= "Below diagonal:\nRVF Advantage",size=2.8) + 
  theme(text = element_text(size = 8)) 

ggsave(here("figs/AFig2_RDTsides.eps"),width = 14, height = 9.5, units="cm", dpi=300)


```

```{r WC,echo=F}
mycomb$WCsig <-0
w<-which(abs(mycomb$WC.zlat)>1.96)
mycomb$WCsig[w]<-1


WCdat <- mycomb[mycomb$excludeWC==0,]
WCsides <- ggplot(WCdat, aes(x=WC.Lmean, y=WC.Rmean, color=Handedness,shape=as.factor(WCsig))) +
  xlab("Log mean correct RT (ms) L VHF")+
   ylab("Log mean correct RT (ms) R VHF")+
  geom_point()+
  ggtitle("Word Comprehension") +
  scale_shape_manual(name="Significantly lateralised",
                     labels=c("No","Yes"),
                     values=c(4,16))+
  scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
  geom_abline(intercept = 0, slope = 1)+
      annotate("text", x=6.6, y=7, label= "Above diagonal:\nLVF Advantage",size=2.8) + 
    annotate("text", x=6.9, y=6.5, label= "Below diagonal:\nRVF Advantage",size=2.8) + 
  theme(text = element_text(size = 8)) 

ggsave(here("figs/AFig3_WCsides.eps"),width = 14 , height = 9.5, units="cm", dpi=300)

#plot with all 3 L vs. R scatterplots together in a row.
allplot <- ggarrange(DLsides, RDTsides, WCsides, ncol = 1, nrow = 3,common.legend=TRUE)


ggsave(here("figs/allLRbeh.eps"),width = 9, height = 25, units="cm", dpi=300)
fignum<-fignum+1

```

```{r addsupp,echo=F}
suppnum <- suppnum+1
suppdich<-suppnum
```

Visualisations of the raw data from which laterality indices were
computed can be found in Supplementary file `r suppdich`. This
Supplement also shows a scatterplot showing how the LIz score relates to
the conventional laterality index for Dichotic Listening. The principal
difference is that the LIz follows the normal distribution, with a
sigmoid shape at the extremes (truncated in the Figure S`r suppdich`.2
because of the censored scale). For subsequent analyses, we use LIz, as
this allows us to compare different tasks on a common scale.

```{r DLdensities,echo=F,include=F}


plot1 <- ggplot(DLdat, aes(x = DL.LI, y = DL.zlat)) + 
  geom_point(shape = 4,  size = 1)+
  ggtitle("Dichotic Listening") 

  

dens1 <- ggplot(DLdat, aes(x = DL.LI, fill = Handedness)) + 
  geom_density(alpha = 0.4) + 
  geom_vline(xintercept = 0, linetype="dotted")+
  theme_void() + 
  theme(legend.position = "none")

dens2 <- ggplot(DLdat, aes(x = DL.zlat, fill = Handedness)) + 
  geom_density(alpha = 0.4) + 
  geom_vline(xintercept = 0, linetype="dotted")+
  theme_void() + 
  theme(legend.position = "none") + 
  coord_flip()

dens1 + plot_spacer() + plot1 + dens2 + 
  plot_layout(ncol = 2, nrow = 2, widths = c(4, 1), heights = c(1, 4))

DLdens <- ggsave(here("figs/AFig4_DLdens.eps"),width = 14, height = 12, units="cm", dpi=300, device=cairo_ps)


suppfignum<- suppfignum+1
```

### 3.2.3 Distribution of Laterality Indices

Before testing specific predictions about interrelationships between
measures, we conducted preliminary analysis on LIz values for all three
online tasks, to test for normality, to check for significant
lateralisation in left- and right-handers, to compare laterality between
handedness groups, and to compute split-half and test-retest reliability
for laterality indices. For these, and subsequent analyses of
handedness, a simple binary split into left- and right-handed was made
according to whether the laterality index was above (R-handed) or below
(L-handed) zero on the Edinburgh Handedness Inventory.

```{r DL-ttests,echo=F, message=F, warning=F}
#Make a table to show characteristics of different tests

onlinesummary <- data.frame(matrix(NA,nrow=12,ncol=4))
colnames(onlinesummary)<-c('Statistic','Dichotic','Rhyme','Comprehension')
onlinesummary[,1]<-c('N','Mean (SD)','Skew','Kurtosis','Shapiro-Wilk normality','Mean (SD): L-hander','Mean (SD): R-hander','one-group t: L-hander','one-group t: R-hander','R-hander vs. L-hander t','Split-half rho  [95% CI]','Test-retest rho [95% CI] (N = 53)')
```

```{r fillonlinesummary,echo=F, message=F, warning=F}



mytask<-'DL'
writecolnum <- 2 #column of online summary to write to for this task
onlinesummary <- populate(mycomb,mytask,onlinesummary,writecolnum,'beh')

mytask<-'RDT'
writecolnum <- 3 #column of online summary to write to for this task
onlinesummary <- populate(mycomb,mytask,onlinesummary,writecolnum,'beh')
cor3<-spearman.ci(sess1$RDT.zlat, sess3$RDT.zlat, nrep = 10000, conf.level = 0.95)
onlinesummary[12,3]<-paste0(round(cor3$estimate,2),' [',round(cor3$conf.int[1],2),', ',round(cor3$conf.int[2],2),']')

mytask<-'WC'
writecolnum <- 4 #column of online summary to write to for this task
onlinesummary <- populate(mycomb,mytask,onlinesummary,writecolnum,'beh')
cor4<-spearman.ci(sess1$WC.zlat, sess3$WC.zlat, nrep = 10000, conf.level = 0.95)
onlinesummary[12,4]<-paste0(round(cor4$estimate,2),' [',round(cor4$conf.int[1],2),', ',round(cor4$conf.int[2],2),']')


ft<-flextable(onlinesummary)%>% 
  flextable::compose(i=11, j='Statistic', value = as_paragraph('Split-half r', as_sub('s'), ' [95% CI]'), part = "body") %>% 
  flextable::compose(i=12, j='Statistic', value = as_paragraph('Test-retest r', as_sub('s'), ' [95% CI] (N = 53)')) %>% 
  flextable::compose(i=11, j='Dichotic', value = as_paragraph('r', as_sub('s'),' = ', onlinesummary[11,2]), part = 'body') %>% 
  flextable::compose(i=11, j='Rhyme', value = as_paragraph('r', as_sub('s'),' = ', onlinesummary[11,3]), part = 'body') %>% 
  flextable::compose(i=11, j='Comprehension', value = as_paragraph('r', as_sub('s'),' = ', onlinesummary[11,4]), part = 'body') %>% 
  flextable::compose(i=12, j='Rhyme', value = as_paragraph('r', as_sub('s'),' = ', onlinesummary[12,3]), part = 'body') %>% 
  flextable::compose(i=12, j='Comprehension', value = as_paragraph('r', as_sub('s'),' = ', onlinesummary[12,4]), part = 'body') %>% 
  fontsize(size=10) %>% autofit

ft
tabnumber <- tabnumber+1
tabonlinesummary<-tabnumber

ftbig <- ft %>% flextable::fontsize(size=16)
ftbig<-autofit(ftbig)

# I don't think these lines are needed here, but I might be wrong...
#tabnumber <- tabnumber+1
#tabonlinesummary<-tabnumber
```

Table `r tabnumber`: *Descriptive statistics for three online laterality
measures (LIz)*

```{r behpirates, echo=FALSE, warning=FALSE,message=F}
#NB Formatting of this figure needs to be tweaked to achieve good resolution and legibility, but we can do that when we know what format figures need to be created in. 


bLIdat <- combdat %>% 
  dplyr::select(ID, Rhanded, DL.zlat,RDT.zlat,WC.zlat)
colnames(bLIdat) <- c('ID','Handed','Dichotic','Rhyme','Comprehension')
bLIdat$Handed <- as.factor(bLIdat$Handed)
levels(bLIdat$Handed)<-c("L","R")
longdata.b <- pivot_longer(data = bLIdat, cols = c(3:5), names_to = 'Task', values_to = 'LIz')
longdata.b$Task<-as.factor(longdata.b$Task)
longdata.b$Task <- factor(longdata.b$Task, levels = c('Dichotic', 'Rhyme', 'Comprehension'))

mypath<-mydir

setEPS()
cairo_ps(here('figs/beh_pirates.eps'), fallback_resolution=300, width=19/2.54, height=15/2.54) # convert width and height to inches
pirateplot(data = longdata.b, LIz ~ Handed * Task,  cex.lab=1, cex.axis=1, cex.names=1, ylab='R hem bias <--- LIz ---> L hem bias')

abline(h=0)



#fignum<-fignum+1 #check if this is throwing out figure numbering?

percentlat <- data.frame(matrix(NA,nrow=3,ncol=4))
colnames(percentlat)<-c('task','Rlat','nolat','Llat')
percentlat$task<-colnames(bLIdat)[3:5]
pN <- nrow(bLIdat)
for (c in 3:5){
  thisrow<-c-2
  percentlat[thisrow,2]<-100*length(which(bLIdat[,c]<(-1.96)))/pN
  percentlat[thisrow,4]<-100*length(which(bLIdat[,c]>1.96))/pN
  percentlat[thisrow,3]<-100-percentlat[thisrow,2]-percentlat[thisrow,4]
}
percentlat[,2:4]<-numformat(percentlat[,2:4],1)
```

### Figure `r fignum`

![Pirate plot distributions of LIz scores on online
tasks](figs/beh_pirates.eps)

Figure `r fignum` shows distributions of scores for left- and
right-handers as pirate plots, a form of beanplot with grey dots showing
individual datapoints, the horizontal bar showing the mean, and the
rectangle around the bar corresponding to the 95% Bayesian Highest
Density Interval (Phillips, 2017). Table `r tabnumber`, distributions of
LIz on the three tasks were non-normal, and the three tasks showed very
different patterns of laterality. As expected from previous studies, on
Dichotic Listening there was a clear right ear advantage in both left-
and right-handers. Nevertheless, on the criterion of having an absolute
LIz score of 1.96 or more, only `r percentlat[1,4]`% of participants
were reliably left-lateralised, with `r percentlat[1,3]`% unlateralised,
and `r percentlat[1,2]`% reliably right-lateralised. In addition, there
was a small but statistically reliable difference between handedness
groups, with stronger laterality in the right-handers. We did not assess
test-retest reliability for this task, as we had done this in our
previous study and found it to be high (Parker et al., 2021). Here we
confirm excellent split-half reliability for this task.

The Rhyme Decision task was far less reliable, with split-half
reliability [95% CI] of `r onlinesummary[11,3]` and test-retest
reliability of `r onlinesummary[12,3]`. These figures indicate that
laterality bias from zero on this test is well above chance, but there
is a great deal of random variation. In addition, although the task
showed statistically reliable laterality in both left- and right-handers
in this large sample, the effect size was small, and most individuals
were not significantly lateralised on the criterion of having an
absolute LIz score of 1.96 or more: `r percentlat[2,4]`% of participants
were reliably left-lateralised, with `r percentlat[2,3]`% unlateralised,
and `r percentlat[2,2]`% reliably right-lateralised. Furthermore, there
was no effect of handedness on laterality on this task.

The Word Comprehension task did rather better than the other tasks in
terms of reliability, with split-half reliability of
`r onlinesummary[11,4]` though test-retest reliability was lower at
`r onlinesummary[12,4]`. The striking observation about this task was
that it showed a laterality bias in the opposite direction to what is
usually seen in language tasks. Responses were faster when the target
picture that matched the auditorily presented word occurred in the left
visual half-field, which projects directly to the right hemisphere. On
the criterion of having an absolute LIz score of 1.96 or more:
`r percentlat[3,4]`% of participants were reliably left-lateralised,
with `r percentlat[3,3]`% unlateralised, and `r percentlat[3,2]`%
reliably right-lateralised. Furthermore, there was a significant effect
of handedness, with the laterality index being more negative in
left-handers than in right-handers. We consider the implications of
these findings in the Discussion below.

### 3.2.4 Testing Prediction 1: Fit of two-factor model to behavioural data

Prediction 1 stated: ***The pattern of correlation between laterality
indices from online measures will reflect the extent to which they
involve implicit speech production, rather than whether they involve
spoken or written language. Thus we anticipate dissociation between the
rhyme judgement task and the other two measures (Dichotic Listening and
Word Comprehension task), which is not accountable for in terms of low
reliability of measures.***

*Departure from pregistration*: *We had planned to do a formal
comparison of model fit using AIC weights, but we realised our data were
inadequate for this because our Model A was, in formal terms,
just-identified: it simply estimated three pairwise correlations from
the data, and always gave perfect fit, regardless of the size or
direction of correlations. We considered alternative approaches to the
analysis, but decided to just report the correlations at this stage, as
the pattern of results was distinctive, and we had already planned to
incorporate the online behavioural measures into the SEM analysis that
includes the fTCD measures (see section 3.4 below).*

```{r behavcorrs,echo=F,warning=F,include=F}


bivdat <- filter(mycomb,excludeRDT==0,excludeDL==0,excludeWC==0)
#Assign Group at random
set.seed(50) #make reproducible
bivdat$Group<-1+rbinom(nrow(bivdat),1,.5)
#Check handedness distribution
handgrouptab <- table(bivdat$Handed,bivdat$Group) #This is just to confirm roughly equal distribution of L and R handers in the 2 random groups.


#tempx and tempy are reassigned to variables of interest before calling generic function that will base plot on these two variables
bivdat$tempx <- bivdat$DL.zlat
bivdat$tempy <- bivdat$RDT.zlat
name1 <-"Dichotic Listening LIz"
name2 <- "Rhyme decision LIz"
DL_RD_plot <- bivplot2(bivdat,name1,name2) #this is our specially created function
#I've commented out saving the individual plots, just because I've used ggarange to make a composite plot with all 3 pairings
#ggsave(paste0(mydir,"/03-graphic-outputs/DL-RDT.png"),width = 6, height = 4)

bivdat$tempy <- bivdat$WC.zlat
name2 <- "Word Comprehension LIz"
DL_WC_plot <- bivplot2(bivdat,name1,name2)
#ggsave(paste0(mydir,"/03-graphic-outputs/DL-WC.png"),width = 6, height = 4)

bivdat$tempx <- bivdat$RDT.zlat
name1 <- "Rhyme Decision LIz"
RD_WC_plot <- bivplot2(bivdat,name1,name2)
#ggsave(paste0(mydir,"/03-graphic-outputs/RD-WC.png"),width = 6, height = 4)

#For now am making a plot with all 3 scatterplots together in a row. Easy to change layout if needed.
allplot <- ggarrange(DL_RD_plot, DL_WC_plot, RD_WC_plot, ncol = 1, nrow = 3,common.legend=TRUE)

ggsave(here('figs/allbiv.eps'), width = 9, height = 25, units="cm", dpi=300)
fignum<-fignum+1
```

### Figure `r fignum`

![Bivariate distributions of LIs on behavioural tasks](figs/allbiv.eps)

Figure `r fignum` shows scatterplots of the bivariate relationships
between the three variables. Spearman correlations are shown with 95%
confidence intervals estimated using the *spearman.ci()* function from
the RVAideMemoire package (version 0.9-79; Herv√©, 2021) with 1,000
iterations. It is evident from inspection that we can reject model C, in
which all three LIs are independent, and model B1, where only Dichotic
Listening and Rhyme Decision are correlated. The strongest correlation
is between the two visual tasks, Rhyme Decision and Word Comprehension,
as predicted by model B2.

Note that correlations will be influenced by test reliability. Indeed,
the correlation between Rhyme Decision and Word Comprehension is close
in magnitude to the split-half reliability of the two measures. An
estimate of the association between these measures after adjusting for
the split-half reliabilities can be obtained using the Spearman-Brown
correction for attenuation, r.xy(corrected) = r.xy(observed)/sqrt(r.xx
\* r.yy), which gives a value of `r round(.44/sqrt(.436*.664),3)`.

**Evaluation of Prediction 1: Separable dimensions for receptive and
production online laterality tasks.** This prediction was not confirmed.
The baseline hypothesis of a single laterality factor was also rejected.

## 3.3 Step 2: Functional Transcranial Doppler Ultrasound (fTCD)

We excluded `r n_excluded` participants who met our criteria for
outliers on two or more fTCD tasks. For the remaining `r nrow(ddat)`
participants, the numbers with fewer than 10 usable trials on the six
tasks (A = Word Generation, B = Sentence Generation, C = Phonological
Decision, D = Word Decision, E = Sentence Decision and F = Syntactic
Decision) were `r n_excludeLI[1]`, `r n_excludeLI[2]`,
`r n_excludeLI[3]`, `r n_excludeLI[4]`, `r n_excludeLI[5]` and
`r n_excludeLI[6]` respectively.

<!-- Plot is created by fctd_task_plots.R-->

```{r maketimecourse,echo=F,message=F,warning=F}
## This chunk was updated on 10 July 2022 because the POI was wrongly depicted. The dotted vertical lines denoting both baseline and POI had disappeared when the combined plots were converted to .eps. These are now reinstated, and the white on black labels made smaller to ensure they do not extend beyond the bounds of baseline and POI intervals. 
## Based on ftcd_task_plots.R script to create fTCD timecourse plots for each of the 6 language tasks. ZW Feb 2022

## The script uses data produced during the fTCD preprocessing. 
## For each participant, there are 6 .csv files, one for each task,
## containing the averaged, preprocessed fTCD signal for the task.

## The script reads in all of the data files and averages over all participants,
## to produce one grand mean plot for each task.


tasks <- c('WG','SG','PD','WC','SC','SD')
longtasks<-c('Word Generation','Sentence Generation','Phonological Decision','Word Decision','Sentence Decision','Syntactic Decision')
ntasks <- length(tasks)

basestart=-5 # baseline start
baseend=2 # baseline end
poistart=6 # period of interest start
poiend=23 # period of interest end

## Read in data
ftcddata <- read.csv(here("data/ftcd_data.csv"))
w<-which(ftcddata$No_signal==1)#all cases with no signal
ftcddata <- ftcddata[-w,] 

ftcddata$Nexclude <- ftcddata$A_exclude + ftcddata$B_exclude + ftcddata$C_exclude + ftcddata$D_exclude + ftcddata$E_exclude + ftcddata$F_exclude
ftcddata <- ftcddata %>% filter(Nexclude <= 1)

#NB; some are NA on Nexclude - I checked and these are both cases with just one missing

# Load in the first one to find out how big it is
tmpfile <- read.csv(here('ftcd_task_means/3151603_WG.csv'))

# Get the x-axis (time) information from tmpfile
mytime <- tmpfile$time

## Get data for left-handers (h=0) and right-handers (h=1)
for (t in 1:ntasks){
  taskmean <- matrix(data=NA, nrow=dim(tmpfile)[1], ncol=6)
  colnames(taskmean) <- c('LeftHand_L', 'LeftHand_R', 'RightHand_L', 
  'RightHand_R','LeftHand_L-R','RightHand_L-R')
  for (h in 0:1){
    mysubj <- which(ftcddata$Hand_R == h) #rows of subs with this handedness
    nsubj <- length(mysubj)
     # Create matrices for the data we're going to read in (Ldata and Rdata)
    Ldata <- Rdata <-diffdata<-matrix(data=NA, nrow=dim(tmpfile)[1], ncol=nsubj)
    
    for (s in 1:nsubj){
      # Read in the data
      mysubname <- ftcddata$Gorilla_ID[mysubj[s]]
      myfile <- paste0('ftcd_task_means/',mysubname,'_',tasks[t],'.csv')
      if (file.exists(here(myfile))){
        mydat <- read.csv(here(myfile))
        Ldata[ , s] <- mydat$Lmean
        Rdata[ , s] <- mydat$Rmean
        diffdata[,s]<-100+mydat$meanDiff #NB we add 100 to the difference so we can put it on the same plot
      }
    } # end subject loop
    
    # Average over participants (rows)
    Lmean <- rowMeans(Ldata, na.rm=TRUE)
    Rmean <- rowMeans(Rdata, na.rm=TRUE)
    diff <- rowMeans(diffdata,na.rm=TRUE)
    
    taskmean[ , (h*2+1)] <- Lmean
    taskmean[ , (h*2+2)] <- Rmean
    taskmean[,(h+5)] <- diff #diffs in cols 5 and 6
    
  } # end handedness loop
  
  # Create plot for this task
  # Make data longer
  taskmean <- as.data.frame(taskmean)
  taskmean$Time <- mytime
  
  # Crop to -10 to 30 seconds
  start_index <- which(mytime==-10)
  end_index <- which(mytime==24)
  xtime <- mytime[start_index:end_index]
  taskmean <- taskmean[start_index:end_index, ]
  ntimes<-nrow(taskmean)
  taskmean[(ntimes+1),1:6]<-108 #plots won't scale to same yaxis if 2nd axis is added, so this is a trick to make sure they do - I've added a spurious point with maximum value we want but it is outside the range of time values that display
  
  taskmean_long <- pivot_longer(data=taskmean, cols=c(1:6), 
                                names_to = 'Condition', values_to='Velocity')
  
  if (t == 1 | t == 2) {poiend <- 17}
  if (t > 2) {poiend <- 23}
  
  #chaos caused by R's ordering of factors, 
  # so we need to change some names before factorising.
  
  uniques<-unique(taskmean_long$Condition)
  #These are actually in the right order! Presumably because of order in original file.
  #But they are character strings that get alphabetised if factored.
  #So!
  for (i in 1:6){
    w<-which(taskmean_long$Condition==uniques[i])
    n<-c(1,2,3,4,5,6)
    taskmean_long$Condition[w]<-paste0(n[i],uniques[i]) #stick a number on the front
  }

  #Total nightmare getting correct allocation of condition to linetype and colour!
  taskmean_long$Handed_Side <- as.factor(taskmean_long$Condition)
  levels(taskmean_long$Handed_Side)<- c('L hander/L side', 'L hander/R side', 
                                        'R hander/L side','R hander/R side',
                                        'L hander/L-R','R hander/L-R')
#Make a little file that allocates linetype/color to condition, but NB whether this works depends also on code below, as order of factors v important
  color_lty_cross = expand.grid(
    ltypes = c(1,2),
    colors = c('red','blue','black'),
    stringsAsFactors = F)
  color_lty_cross$condition<-levels(taskmean_long$Handed_Side)
#Change the order, as preferable to have solid for all R and dotted for all L
  color_lty_cross$ltypes <- c(2,2,1,1,2,1)
  color_lty_cross$colors <- c("red","blue","red","blue","black","black")
  
  thisplot<-ggplot(taskmean_long, aes(x=Time, y=Velocity, group=Handed_Side)) + 
    geom_line(aes(linetype=Handed_Side,colour=Handed_Side,)) +
    scale_color_manual(values = color_lty_cross$colors) +
    scale_linetype_manual(values = color_lty_cross$ltypes[1:6]) +
    theme_bw()+
    xlim(-5,25)+
    scale_y_continuous(name = "Blood flow velocity", sec.axis = sec_axis(~.-100, name = "L-R"),breaks = seq(94, 108, by=2))+
    scale_x_continuous(breaks = seq(-5, 25, by=5),name = "Time (s)")+
    geom_hline(yintercept = 100, linetype="solid", alpha = 0.8) +
    geom_vline(xintercept = 0, linetype="solid", alpha = 0.8) +
    annotate(geom="label", x=(basestart+baseend)/2, y=95, label="Baseline",hjust=0.5,size=2.5,fill='black',colour='white')+
    annotate(geom="label", x=(poistart+poiend)/2, y=95, label="Period of Interest",hjust=0.5,size=2.5,fill='black',colour='white')+
    geom_vline(xintercept = basestart, linetype = "dotted", alpha = 1) + # Start of Baseline
    geom_vline(xintercept = baseend, linetype = "dotted", alpha = 1) + # End of Baseline
    geom_vline(xintercept = poistart, linetype = "dotted", alpha = 1) + # Start of POI
    geom_vline(xintercept = poiend, linetype = "dotted", alpha = 1) +  # End of POI
    ggtitle(longtasks[t])
  thisplot

  ggsave(paste0('figs/ftcd_grandmeanx_', tasks[t],'.eps'),
         width = 14,
         height = 9, units="cm", dpi=300)
  d6<-thisplot
  if(t==1){d1<-thisplot}
  if(t==2){d2<-thisplot}
  if(t==3){d3<-thisplot}
  if(t==4){d4<-thisplot}
  if(t==5){d5<-thisplot}
  
}
alltimes <- ggarrange(d1,d2,d3,d4,d5,d6, ncol = 2, nrow = 3,common.legend=TRUE)


ggsave(here("figs/alltimecourse.eps"),width = 19, height = 19, units="cm", dpi=300)

  # The POI ends earlier for WG and SG tasks, due to REPORT phase
```

### 3.3.1 Cerebral Blood Flow Velocity

`r fignum <- fignum+1`  
### Figure `r fignum`
![Timecourse of left and right hemisphere blood flow velocity (left
axis) and L-R difference (right axis) for six tasks in left- and
right-handers)](figs/alltimecourse.eps)

Figure `r fignum` shows the mean time course of blood flow velocity on
left and right channels for left- and right-handers. In addition, the
difference between left and right channels is plotted in black, after
adding 100 to the values so that they can be shown on the same plot
(with scale on right side axis). The laterality index is computed as the
mean difference score (shown in black) over the period of interest. For
Word Generation and Sentence Generation tasks, the LI is computed during
a period corresponding to silent generation; the waveform peaks again
after this period, corresponding to the activity from the subsequent
spoken response. For the other tasks, a series of items is presented in
each trial and no spoken response is required. The periodic fluctuations
in the response correspond to the individual items that are responded
to.

Inspection of this figure indicates that we see a strong left hemisphere
bias in both handedness groups for Word Generation, Sentence Generation
and Phonological Decision, whereas the other tasks do not show this
pattern.

```{r accuracyftcd,echo=F,warning=F}
#What is above chance performance for 2-choice task with 54 items (SD?
#1-pbinom(33, size=54, prob=0.5) = .03
#So less than 61% correct is chance level
#What is above chance performance for 2-choice task with 108 items (SD?
#1-pbinom(63, size=108, prob=0.5)=.03, this is 58%
#So less than 58% correct is chance level for PD, WC and SC

mycols<-c("WG_nWords","SG_nWords","PD_Acc","WC_Acc","SC_Acc","SD_Acc")
acccols<-which(colnames(combdat)%in% mycols)
cutoffs<-c(2,6,58,58,58,61)
poordata<-vector()
for(i in 1:6){
  combdat[,acccols[i]]<-as.numeric(combdat[,acccols[i]])
 # hist(combdat[,acccols[i]])
  w<-which(combdat[,acccols[i]]<cutoffs[i])
  poordata<-c(poordata,w)
  
}
poordata<-unique(poordata)
poordatalist<-combdat[poordata,c(1,acccols,ncol(combdat)-1,ncol(combdat))]
write.csv(poordatalist,here('data/poordatacases.csv'),row.names=F)
datalist<-combdat[,c(1,acccols,ncol(combdat)-1,ncol(combdat))]
acctab<-describe(datalist[,2:7])

#After checking, we do have 5 cases where Syntactic Decision is below 60% correct. 3 of them in language group 1, and two in language group 3. None is currently excluded.
```

### 3.3.2 Behavioural Performance on FTCD tasks

The mean number of words produced per trial was
`r round(acctab$mean[1],2)`, (SD = `r round(acctab$sd[1],2)`) for Word
Generation and `r round(acctab$mean[2],2)`, (SD =
`r round(acctab$sd[2],2)`) for Sentence Generation. For the four
decision tasks, accuracy was recorded as mean percentage correct:
`r round(acctab$mean[3],2)` (SD = `r round(acctab$sd[3],2)`) for
Phonological Decision, `r round(acctab$mean[4],2)` (SD =
`r round(acctab$sd[4],2)`) for Word Decision,
`r round(acctab$mean[5],2)` (SD = `r round(acctab$sd[5],2)`) for
Sentence Decision, and `r round(acctab$mean[6],2)` (SD =
`r round(acctab$sd[6],2)`) for Syntactic Decision.

### 3.3.3 Laterality Indices

```{r LIpirates, echo=FALSE, warning=FALSE,message=F}
#NB Formatting of this figure needs to be tweaked to achieve good resolution and legibility, but we can do that when we know what format figures need to be created in. 
#was REORDERED IN TERMS OF BIAS TO LEFT HEMISPHERE, but now in original order

#Make task names that will print on 2 lines for compactness
tasknames2 <- c("Word\ngeneration","Sentence\ngeneration","Phonological\ndecision","Word\ndecision","Sentence\ndecision","Syntactic\ndecision")
#Now make text locations for these on pirate plot: we'll place A-C below plot and D-F above it
horizpts<-rep(1,6)
for (i in 1:6){
  horizpts[i] <- 1+(i-1)*3
}
vertpts <- rep(-5.5,6)
vertpts[4:6]<-6

LIdata <- ddat %>% 
  dplyr::select(ID, Rhanded,A_mean_LI,  B_mean_LI, C_mean_LI,D_mean_LI, E_mean_LI, F_mean_LI, )
colnames(LIdata) <- c('ID','Handed','1A','2BV','3C','4D','5E','6F')
LIdata$Handed <- as.factor(LIdata$Handed)
levels(LIdata$Handed)<-c("L","R")
longdata.d <- pivot_longer(data = LIdata, cols = c(3:8), names_to = 'Task', values_to = 'LI')
longdata.d$Task <- as.factor(longdata.d$Task)
levels(longdata.d$Task)<-c('A','B','C','D','E','F')

 
setEPS()
cairo_ps(here('figs/ftcd_pirates.eps'), fallback_resolution=300, width=19/2.54, height=15/2.54) # convert width and height to inches
pirateplot(data = longdata.d, LI ~ Handed * Task, ylim=c(-6,8))

abline(h=0)

for (i in 1:6){
  text(horizpts[i], vertpts[i], tasknames2[i], #add label for task
     cex = .8)
}

#pdf version for graphical abstract
pdf(file='figs/piratepdf.pdf',width=7,height=4)
pirateplot(data = longdata.d, LI ~ Handed * Task, ylim=c(-6,8))

abline(h=0)

for (i in 1:6){
  text(horizpts[i], vertpts[i], tasknames2[i], #add label for task
     cex = .8)
}
dev.off()


fignum<-fignum+1
piratenumber <- fignum
```

### Figure `r fignum`

![Distributions of fTCD LIs on six tasks for 104 left-handers and 91
right-handers.](figs/ftcd_pirates.eps)

```{r doppler-ttests,echo=F}
#Make a table to show characteristics of different tests


ftcdsummary <- data.frame(matrix(NA,nrow=11,ncol=7))
colnames(ftcdsummary)<-c('Statistic',LETTERS[1:6])
ftcdsummary[,1]<-c('N','Mean (SD)','Skew','Kurtosis','Shapiro-Wilk normality','Mean (SD) L-hander','Mean (SD) R-hander','one-group t L-hander','one-group t R-hander','R-hander vs. L-hander t','Split-half rho [95% CI]')
```

The pirate plot in Figure `r fignum` shows LI values for the six tasks
(A = Word Generation, B = Sentence Generation, C = Phonological
Decision, D = Word Decision, E = Sentence Decision and F = Syntactic
Decision) for left- and right-handed participants.

```{r fill-ftcdsummary, echo=FALSE, warning=FALSE,message=F}
#We use the same function 'populate' to populate the data frame as we had for behavioural tasks - the 'ftcd' term at end of function call ensures correct columns are found
for (t in 1:6){
mytask<-LETTERS[t]
writecolnum <- 1+t #column of online summary to write to for this task

ftcdsummary <- populate(ddat,mytask,ftcdsummary,writecolnum,'ftcd')
#ftcdsummary <- ftcdsummary[1:(nrow(ftcdsummary)-1),]
}
ft<-flextable(ftcdsummary) %>% 
  flextable::compose(i=11, j='Statistic', value = as_paragraph('Split-half r', as_sub('s'), ' [95% CI]'), part = "body") %>% 
  flextable::compose(i=11, j='A', value = as_paragraph('r', as_sub('s'),' = ', ftcdsummary[11,2]), part = 'body') %>% 
  flextable::compose(i=11, j='B', value = as_paragraph('r', as_sub('s'),' = ', ftcdsummary[11,3]), part = 'body') %>% 
  flextable::compose(i=11, j='C', value = as_paragraph('r', as_sub('s'),' = ', ftcdsummary[11,4]), part = 'body') %>% 
    flextable::compose(i=11, j='D', value = as_paragraph('r', as_sub('s'),' = ', ftcdsummary[11,5]), part = 'body') %>% 
  flextable::compose(i=11, j='E', value = as_paragraph('r', as_sub('s'),' = ', ftcdsummary[11,6]), part = 'body') %>% 
  flextable::compose(i=11, j='F', value = as_paragraph('r', as_sub('s'),' = ', ftcdsummary[11,7]), part = 'body') %>% 

  fontsize(size=10) %>% fit_to_width(8,inc=1L)

ft
tabnumber<-tabnumber+1
ftcdtabnumber<-tabnumber #for later reference
```

Table `r tabnumber`: *Descriptive statistics for six fTCD laterality
indices*

Table `r tabnumber` shows basic statistics for the fTCD laterality
indices, in the same format as for the online tasks. Right-handers
showed significant left-lateralisation on Word Generation, Sentence
Generation, Phonological Decision, and Sentence Decision, but were not
lateralised for Word Decision or Syntactic Decision. Left-handers were
significantly left-lateralised for Word Generation, Sentence Generation
and Phonological Decision, were not lateralised for Sentence Decision or
Syntactic Decision, and were significantly right-lateralised for Word
Decision. The direct comparison between left- and right-handers showed
significantly greater left-lateralisation in right-handers on all tasks
except Word Decision and Syntactic Decision.

All tasks had split-half reliability coefficients of .72 or above,
except for Word Decision, where the coefficient was only .52.

Shapiro Wilk tests revealed significant non-normality for Sentence
Generation, Phonological Decision, Word Decision and Sentence Decision,
though values of skewness and kurtosis were generally not extreme.

As noted above, there were a few participants with missing data on a
single measure. Before running the SEM analysis, the *mice* package (Van
Buuren & Groothuis-Oudshoorn, 2011) was run with default settings in R
to impute these missing values.

```{r imputemissing,echo=F, include=F}
nunames<- c('A_P1','B_P2','C_P3','D_R1','E_R2','F_R3') #cols for SEM; short names useful here. These cols with have imputed values
tasknames <- c('Word Generation','Sentence Generation','Phonological Decision','Word Decision','Sentence Decision','Syntactic Decision')
ddat$Handed<-as.factor(ddat$Rhanded)
levels(ddat$Handed)<-c("Left","Right")

thisdat <- ddat[,c('A_mean_LI','B_mean_LI','C_mean_LI','D_mean_LI','E_mean_LI','F_mean_LI')]

#Interpolate missing values using mice package
thisdat.i <- mice(thisdat, m=1, maxit = 50, method = 'pmm', seed = 500,print=FALSE)
ddati <-cbind(ddat[,c('ID','male','Handed','langgroup')],complete(thisdat.i,1))
colnames(ddati)<-c('ID','male','Handed','langgroup',nunames)

```

### 3.3.4 Testing Prediction 2: Fit of Two-factor Model to FTCD Data

Our second preregistered prediction was: ***The data will fit a model
where "language generation" tasks cluster together on one factor, and
"receptive language" tasks on a second factor.*** It was further
predicted that factors will be correlated, but the fit of a 2-factor
model will be superior to a single-factor model where all LIs load on a
common factor.

The analysis conducted by Woodhead et al (2019, 2020) used an
exploratory bifactor model in which each task could load on each of two
factors. Because there were two measures for each task (from test and
retest sessions), this exploratory approach was adequately powered. In
the current study, two tasks differed to Woodhead et al's studies (List
Generation and Semantic Decision were removed, and Word Generation and
Word Decision were added), and we only had one measurement occasion for
each of the six measures. Accordingly, we used confirmatory factor
analysis, using a prespecified two-factor model that constrains which
indicators can load on two factors. This was compared to a unitary
model, in which all tasks load on a single factor.

`r fignum <- fignum+1`Figure `r fignum` shows the pattern of
correlations between LIs for the different tasks as a heatmap, with values for left-handers above the diagonal and those for right-handers below. Pearson correlations are shown here, as these relate more directly to the analysis of covariances that is the basis of Structural Equation Modeling. The
two-factor model predicts that correlations will form two clusters, with
positive correlations within the first three tests, and within the last
three tests, but weaker or absent correlations across these two clusters
of measures. In both handedness groups, the heatmap shows moderate correlations within both
clusters of measures, and generally lower correlations across clusters,
but there are some exceptions. Notably, there is a moderate correlation
between Phonological Decision and Sentence Decision, which was not
predicted by the two-factor model.

```{r modelfit,echo=F,include=F}

#nb we will use the variables from ddati
set.seed(50)

ddati$randgroup<-1+rbinom(nrow(ddati),1,.5) #create random group 1 or 2 for later split

#Add correlation matrix

LIcols <- c("A_P1","B_P2","C_P3","D_R1","E_R2","F_R3")

temp<-ddati #make copy with full names for variables and use this for heatmap
w<-which(colnames(temp)%in%LIcols)
colnames(temp)[w]<-tasknames
#myheatmap <- makeheatmap(temp,tasknames)

#ggsave(here("figs/ftcd-heatmap.eps"),width = 14, height = 14, units="cm", dpi=300)

suppnum<-suppnum+1
#NB we have substituted a heatmap showing L and R hander correlations with reliabilities on diagonal - this is created below and added manually after knitting
```

### Figure `r fignum`

![Heatmap showing correlations between laterality indices from six fTCD
tasks,, with values for L-handers above the diagonal, R-handers below the diagonal, and split-half reliability on the diagonal (cells with grey frame).](figs/ftcd-heatmap.eps)

#### 3.3.4.1 Structural Equation Modeling

Because Structural equation modeling (SEM) is not widely used in
laterality research, we provide here a brief explanation, to aid
interpretation of the subsequent analysis.

Structural equation modeling (Kline, 2011) is a method that allows a
formal test of adequacy of competing models for explaining patterns of
association between variables. The underlying assumption of this
approach is that observed variables can be treated as indicators of
underlying, unobserved latent variables.

Associations between latent factors and observed variables are shown in
a path diagram, with latent factors in circles, and observed variables
in boxes. Single-headed arrows indicate causal paths, and double-headed
arrows indicate variances. Although means can be incorporated in SEM
(and we shall be doing this in our analysis), the main use of SEM is to
analyse patterns of covariances. Path diagrams have a precise
mathematical interpretation, and can be converted into linear equations
that specify the covariances between observed variables. Thus it is
possible to obtain a measure of goodness of fit for observed data in
relation to a model by comparing how far the observed covariances agree
with those predicted by the model. We can already see by inspecting the
heatmap of Figure `r fignum` that a single-factor model is unlikely to
provide a good fit to the observed data, because it would not predict
the clustering of correlations that is evident.

SEM does not arrive at a single algebraic estimation of model fit, but
rather uses a maximum likelihood approach, whereby values for the paths
from the factors to the observed variables are first assigned starting
values, and the expected covariances between variables are computed with
these values, and then compared to observed covariances. This process is
iterated many times with different path estimates, with an algorithm
adjusting paths on each run to reduce the mismatch between observed and
expected (i.e., model-implied) covariances.

Path diagrams are shown below in Figure `r fignum+1` (single factor
model), and Figure `r fignum<-fignum+1` `r fignum+1` (two-factor model).
***The two-factor model is equivalent to Figure 4 from the preregistered
document***. These include one path to each factor shown as a dotted
line. This is a fixed parameter, set to 1, which is necessary to scale
the estimates. Computationally, it makes no difference to the solution
which path is fixed: for each factor this can be either one of the paths
from an observed variable, or the variance of the factor. All the other
paths are free to vary, and the estimation process will consider
different values, to converge on a solution that gives the best fit.
Some paths may have little impact on the solution, and may be dropped
without any deterioration of fit.

There is no single method for evaluating the fit of a model to observed
data (Schermelleh-Engel et al., 2003). A $\chi$^2^ test can give an
estimate of the extent of departure of observed values from expectation:
a good model is one where $\chi$^2^ is small and has a high associated
p-value, indicating that any difference between expectation and
observation is likely to just reflect sampling error. It is usually
possible to improve the fit by including additional paths or factors in
a model until good fit is achieved, but this does not mean that the
model is better: the goal is rather to obtain a parsimonious and
theoretically meaningful model that does not include arbitrary
parameters that are specified solely to fit the data. Note, however,
that values of $\chi$^2^ are dependent on sample size, and with small
samples, a small $\chi$^2^ value may suggest good fit, when differences
between observed and model-implied covariances are large; in effect
small samples may lack power to detect departures from model
predictions, whereas large samples risk finding significant departures
from perfect fit on the basis of trivial mismatch.

For this reason, a range of different measures of model fit has been
devised. The first is the Root Mean Square Error of Approximation
(RMSEA), a measure of approximate fit in the population that is largely
independent of sample size. RMSEA is a measure of "badness of fit",
where a value of zero indicates good fit. The Standardized Root Mean
Square Residual (SRMR) considers the average size of fitted residuals
after the model is fitted, and also ranges from 0 to 1. For both RMSEA
and SRMR, values below .05 are generally regarded as indicating good
model fit (Schermelleh-Engel et al., 2003).

Other indices have been developed that penalise models with a large
number of parameters. The Comparative Fit Index (CFI) measures relative
improvement of fit of a model relative to a model that assumes
independence of all variables. CFI values of .95 or more are
conventionally regarded as indicating acceptable fit. The Tucker-Lewis
Index (TLI) similarly compares chi square of the observed model with an
independence model, with values of .97 or more indicating good model fit
(Schermelleh-Engel et al., 2003).

Models are 'nested' when a simple model can be derived from a more
complex model by fixing at least one free parameter in the complex
model. In Figures `r fignum` and `r fignum+1`, the one-factor model is
equivalent to the two-factor model if the covariance between F1 and F2
is fixed to one. In such cases, model fit can be compared by subtraction
of the $\chi$^2^ and degrees of freedom for the two models; the
difference in $\chi$^2^ is then evaluated; if it is nonsignificant, this
indicates that the simpler model gives as good a fit as a complex model
with more parameters, in which case the simpler model is preferred.

Following recommendations by Schermelleh-Engel et al. (2003) we report
here values for $\chi$^2^, CFI, TLI, RMSEA and SRMR. Detailed outputs
for all SEM analyses are available in Supplementary document
`r suppnum`.

```{r initialisebigsummary,echo=F,include=F}
#Initialise a table to show factors loadings and some other stuff (CFA, rmsea) for each model in a column, so we can compare them
initbigsummary <- function(){
bigsummary <- data.frame(matrix(NA,nrow=23,ncol=4))
colnames(bigsummary)<-c('Estimate','Model.1F','Model.2F','Model.2Fn')
bigsummary[,1]<-c('N participants','Standardized paths [SE]','A -> Fac1','B -> Fac1','C -> Fac1','D -> Fac1','E -> Fac1','F -> Fac1','A -> Fac2','B -> Fac2','C -> Fac2','D -> Fac2','E -> Fac2 ','F -> Fac2','Fac1~~Fac2','Fit indices','CFI','TLI','SRMR','RMSEA [95% CI]','chisq','robust chisq','DF')
return(bigsummary)
}
```

```{r doSemPaths,echo=F,warning=F,message=F}
#Path diagram for SEM saved to file in .eps format
doSemPath <- function(myfit,mytitle,mypathfigname,mynodes){
  #NB order of mynodes is important! default is for them to be ordered in the same order as they occur in the model specification - mynodes gives the long names
setEPS()
postscript(mypathfigname)

semPaths(myfit, "std",weighted = FALSE, shapeMan = "rectangle", sizeMan = 16, 
    sizeMan2 = 5,rotation=4,edge.color='black',asize=2,edge.label.cex=1.1,nodeLabels=mynodes,whatLabels="std",bg="white",width=9,height=6) #draws a path diagram

title(mytitle, line=3)
dev.off()
}
```

### Figure `r fignum`

![One-factor model, showing standardized path coefficients obtained in
the current analysis](figs/pathfigF1.eps)

`r fignum<-fignum+1`

### Figure `r fignum`

![Two-factor model, showing standardized path coefficients obtained in
the current analysis](figs/pathfigF2.eps)

```{r factormodels,echo=F,include=F, warning=F,message=F}
#In lavaan, we first define the factor model between quotes
#This step doesn't do anything - just sets up the model to be run later

#This is definition of single factor model we will use here. A_P1 will be index variable with path of 1; this is default in lavaan - the first mentioned will be the index variable unless specified as NA
model.1F <- 'f1 =~  A_P1 + B_P2  + C_P3 +D_R1 + E_R2 + F_R3' 


#2 factor production/reception model; we fix path from A and also variance of F2
model.2F <- '
f1 =~  A_P1 + B_P2+C_P3
f2 =~  NA*D_R1 + E_R2 +F_R3 #2 factor model: 
f2~~1*f2
#covariance unspecified, which means there is no constraint on covariance

'


fit1 <- cfa(model.1F, estimator="WLSMV",data=ddati,test="robust") #runs the model and saves results in fit1
sfit1 <- makeSEMtab(fit1) #saves the results from the model in a neat format

bigsummary<- initbigsummary()
bigsummary<- addmodel(bigsummary,fit1,NA,'Model.1F',writecol=2) #summary from this model written to col 2 of bigsummary. For single factor model we don't specify a comparison model, hence NA.

#lavResiduals(fit1) #if we want to understand reasons for poor fit, we can look at residuals - shows size of covariances that aren't explained by model.

#Creates a structural diagram for single factor model: nb does NOT include path estimates (these can be shown if we put 'par' rather than 'diagram', but it gets messy, and the parameters are shown instead in a table)
#lots of details of this here
#https://www.rdocumentation.org/packages/semPlot/versions/1.1.2/topics/semPaths
pathfigname1<-here("figs/pathfigF1.eps")
mynodes<-c('  Word \nGeneration','  Sentence \nGeneration','Phonological\nDecision','   Word   \n  Decision','   Sentence  \n  Decision','   Syntactic  \n  Decision','F1') #spaces added to make all same size

doSemPath(fit1,'',pathfigname1,mynodes) #call function specified earlier to draw path diagram. 2nd term can be used for a title, but now made blank
  
fit2 <- cfa(model.2F, estimator="WLSMV",data=ddati)
sfit2 <- makeSEMtab(fit2)
bigsummary<- addmodel(bigsummary,fit2,fit1,'Model.2F',writecol=3) #summary from this model written to col 3 of bigsummary

#lavResiduals(fit2)
pathfigname2<-here("figs/pathfigF2.eps")
#needed to reorder mynodes2 for correct specification
mynodes2<-c("  Word \nGeneration" , "   Sentence \nGeneration", "Phonological\nDecision", " Word   \n  Decision"  ,   "   Sentence  \n  Decision", "   Syntactic  \n  Decision",  "F1","F2")
doSemPath(fit2,'',pathfigname2,mynodes2) #call function to draw path diagram


#anova(fit1,fit2) #compares model fit -if significant, means 2nd model is better fit than 1st. This information is now included in bigsummary.

#I checked whether the one-factor model would fit better if we specified a 2-group solution with handedness groups; it did not
#measurementInvariance(model=model.1F,estimator="WLSMV",data=ddati,group="Handed")
#CFI very low.

#Also true with the 2F model: fit is better but still well below acceptable on CFI and RMSEA/ Also gives negative variance estimates.
#measurementInvariance(model=model.2F,estimator="WLSMV",data=ddati,group="Handed")

```

```{r boltfigstogether,echo=F}
boltfigs<-0 #previously had 2 models in single figure, but difficult to control size/quality, so now keep separate

if(boltfigs==1){
#read in the figures just created
imageX<-image_read(paste0(pathfigname1),density=300)
imageY<-image_read(paste0(pathfigname2),density=300)
itemname<- paste0(alldir,'/bothF1F2.eps')
itemnamej<-paste0(alldir,'/bothF1F2.eps')

#cropX <- image_crop(imageX, "800x+60")#crop out image of 800 pixels width starting at 60 from left
#cropY <- image_crop(imageY, "800x0+60")
#imgboth <- c(cropX,cropY)

imgboth <- c(imageX, imageY)
  imgboth <- image_append(imgboth, stack=TRUE)
  imgboth<-image_trim(imgboth)
  imgboth <- image_crop(imgboth)
 
  #tname <- "~/Dropbox/COLA_RR_Analysis/03-graphic-outputs/bothF1F2.tiff"
  #jname <- "~/Dropbox/COLA_RR_Analysis/03-graphic-outputs/bothF1F2.eps"
  #image_write(imgboth, path=tname,format = "tiff")
  #image_write(imgboth, path=jname, format = "jpg")
  
  image_write(imgboth, path="bothF1F2.eps", format = "eps")
}

```

```{r newmodel,echo=F,include=F}


#make oddeven file so can try same model as Woodhead et al
wantcols <- c("ID","male","Rhanded","langgroup","A_mean_odd","A_mean_even","B_mean_odd","B_mean_even","C_mean_odd","C_mean_even","D_mean_odd","D_mean_even","E_mean_odd","E_mean_even","F_mean_odd","F_mean_even","DopExclude")
ddat_OE <- combdat[combdat$ftcd==1,wantcols]
ddat_OE<-ddat_OE[ddat_OE$DopExclude==0,]

#impute missing values using mice package
nunames<-wantcols[5:16]
thisdat.j <- mice(ddat_OE[,nunames], m=1, maxit = 50, method = 'pmm', seed = 500,print=FALSE)
ddat_OE <-cbind(ddat_OE[,1:4],complete(thisdat.j,1))

colnames(ddat_OE)<-c("ID","male","Rhanded","langgroup","A_o","A_e","B_o","B_e","C_o","C_e","D_o","D_e","E_o","E_e","F_o","F_e")
set.seed(50)

#We explore for best model using just half the data (randgroup = 1)

ddat_OE$randgroup<-1+rbinom(nrow(ddat_OE),1,.5) #create random group 1 or 2 for later split
mygroup <-1
mydat<- ddat_OE[ddat_OE$randgroup==mygroup,]

write.csv(ddat_OE,here('data/ddat_OE.csv'),row.names=F) #save this for use in Supplementary tables

#Model that is same as Woodhead et al, i.e. path B is fixed
model.2FZW <- '
f1 =~  NA*A_o+equal("f1=~A_o")*A_e+ #same path value for A_o and A_e
       1*B_o+equal("f1=~B_o")*B_e+  #same path value for B_o and B_e
       c*C_o+c*C_e+
       d*D_o+d*D_e+
       e*E_o+e*E_e+
       f*F_o+f*F_e
f2 =~  NA*D_o+equal("f2=~D_o")*D_e+ #same path value for D_o and D_e
       e2*E_o+e2*E_e+
       f2*F_o+f2*F_e+
       a2*A_o+a2*A_e +
       c2*C_o+c2*C_e  #only B is omitted from f2

f2~~1*f2
A_o~~av*A_o #equate variances for odds and evens
A_e~~av*A_e
B_o~~bv*B_o
B_e~~bv*B_e
C_o~~cv*C_o
C_e~~cv*C_e
D_o~~dv*D_o
D_e~~dv*D_e
E_o~~ev*E_o
E_e~~ev*E_e
F_o~~fv*F_o
F_e~~fv*F_e
'

fit.2FZW.1 <- cfa(model.2FZW,estimator="WLSMV", data=mydat)
wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2FZW.1,wantfits)
#This gives matrix not positive definite

#Paths suggest dropping nonsig paths from f1 to D and F 
# and A from F2 (B is already missing from f2 as was excluded from model)
model.2FZWa <- '
f1 =~  NA*A_o+equal("f1=~A_o")*A_e+ #same path value for A_o and A_e
       1*B_o+equal("f1=~B_o")*B_e+  #same path value for B_o and B_e
       c*C_o+c*C_e+
       e*E_o+e*E_e
f2 =~  NA*D_o+equal("f2=~D_o")*D_e+ #same path value for D_o and D_e
       e2*E_o+e2*E_e+
       f2*F_o+f2*F_e+
       c2*C_o+c2*C_e  #B is already omitted from f2

f2~~1*f2
A_o~~av*A_o #equate variances for odds and evens
A_e~~av*A_e
B_o~~bv*B_o
B_e~~bv*B_e
C_o~~cv*C_o
C_e~~cv*C_e
D_o~~dv*D_o
D_e~~dv*D_e
E_o~~ev*E_o
E_e~~ev*E_e
F_o~~fv*F_o
F_e~~fv*F_e
'




fit.2FZWa.1 <- cfa(model.2FZWa,estimator="WLSMV", data=mydat)

wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2FZWa.1,wantfits)
#Converges nicely


#Now test model with hold-out group 2
mygroup <-2
mydat<- ddat_OE[ddat_OE$randgroup==mygroup,]
fit.2FZWa.2 <- cfa(model.2FZWa,estimator="WLSMV", data=mydat)

wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2FZWa.2,wantfits)
```

```{r new2factor-with-all,echo=F,include=F}


mydat<- ddati #back to full LI scores and full group; fixed to B again
model.2Fn <- '
f1 =~  NA*A_P1 +1*B_P2+C_P3+ E_R2
f2 =~  NA*C_P3+D_R1+ E_R2 +F_R3  #2 factor model:

f2~~1*f2
'

fit.2F<-cfa(model.2F, estimator="WLSMV",data=mydat,test="robust")
fit.2Fn <- cfa(model.2Fn, estimator="WLSMV",data=mydat,test="robust")
tab.2Fn <- makeSEMtab(fit.2Fn)
fitmeasures(fit.2Fn,wantfits)

bigsummary<- addmodel(bigsummary,fit.2Fn ,fit.2F,'Model.2Fn',writecol=4) #summary from this model written to col 4 of bigsummary 
anova(fit.2Fn,fit.2F)

pathfigname2n<-here("figs/pathfigF2_revised.eps")
mynodes3<-mynodes2<-c(" Word \nGeneration" , "   Sentence \nGeneration", "Phonological\nDecision", "   Sentence  \n  Decision", " Word   \n  Decision"  , "   Syntactic  \n  Decision",    "F1","F2")
doSemPath(fit.2Fn,'',pathfigname2n,mynodes3) #call function to draw path diagram


#look at residuals for diagnostics
res<-resid(fit.2Fn, type = "cor") #large abs values (> .1) indicate relationships not well accounted for


#Can't get 2nd col to reformat - need to convert to character - then OK
#bigsummary[,2]<-as.character(bigsummary[,2])
flextable(bigsummary)
tabnumber<-tabnumber+1
fignum<-fignum+1


```

Table `r tabnumber`: *Fit statistics for 1-factor (Model.1F), 2-factor
(Model.2F) and modified 2-factor (Model.2Fn) models (N = 209)*

```{r formatbigsummary,echo=F}
#bigsummary[,2:4]<-round(bigsummary[,2:4],2)
for (c in 2:4){
w<-which(bigsummary[,c]=='NA [NA]')
bigsummary[w,c]<-'-'
}
ftb<-flextable(bigsummary[17:nrow(bigsummary),])

ftb<-autofit(ftb)
ftb

```

We used the lavaan() package (Rosseel, 2021) to perform the
preregistered model comparison. To take into account non-normality of
some variables, the WLSMV estimator was specified; this uses weighted
least squares with robust standard errors and a mean- and variance
adjusted test statistic, and makes no distributional assumptions about
the observed variables. When this estimator is used, robust $\chi$^2^
values should be used to evaluate model fit, though the unadjusted
$\chi$^2^ values are used for model comparison. Table `r tabnumber`
summarises the main output of the model-fitting. The fit of both the
one-factor and the two-factor model is poor.

Therefore, as planned, we divided the sample into two random subsamples,
1 and 2. The first subsample was used in an exploratory analysis, based
on that used by Woodhead et al (2021), and the second for
cross-validation. Fuller results from the analysis are given in
Supplementary material. Because we had data from a single session, for
both exploratory and cross-validation analyses, we used the LIs from the
odd and even trials to give two indicators per task. We started with a
model with two factors, where all 12 measures (2 measures from 6 tasks)
were allowed to load on both factors, except for Sentence Generation.
This was an indicator variable with a loading of 1 on Factor 1, and no
loading on Factor 2. To ensure model identification, the variance of
Factor 1 was free to vary, and variance of Factor 2 was set to 1.
Although this model converged with good fit, there were warnings
indicating problems with unfeasibly small eigenvalues. However, when
non-significant paths were dropped from the model (from Word Decision
and Syntactic Decision to Factor 1, and from Word Generation to Factor
2), there was good model convergence with plausible parameters and
excellent fit (CFI = 1 and RMSEA = 0). This same model was then
evaluated with the hold-out sample, and again the fit was good. We
therefore took this model forward to the next stage of analysis, first
checking the fit with the original full sample and with the original LIs
based on all trials. The path diagram is shown in Figure `r fignum` and
summary output is shown in Table `r tabnumber`; the fit was a
significant improvement on the fit of the original 2-factor model.

### Figure `r fignum`

![Revised two-factor model, showing standardized path coefficients
obtained in the final analysis.](figs/pathfigF2_revised.eps)

**Evaluation of Prediction 2: Separable factors for receptive and
production fTCD laterality measures**\
This prediction was not confirmed. The baseline hypothesis of a single
laterality factor was also rejected. A modification of the two-factor
model that allowed additional paths from Phonological Decision to Factor
2, and from Sentence Decision to Factor 1 gave a good fit.

```{r makefacscatter,echo=F,include=F}
myscatter <- function(myx,myy,xlabel,ylabel,thisdat){
thisscatter<-ggplot(thisdat, aes(x = myx, y = myy,color=Handed)) + 
  geom_point(shape = 4,  size = 1)+
 scale_color_manual(name="Handedness",
                       labels=c("Left","Right"),
                       values=c("blue","red"))+
    xlab(xlabel)+
    ylab(ylabel)+
  geom_hline(yintercept=0, linetype="dashed")+
    geom_vline(xintercept=0, linetype="dashed")
 #NB scale of factors is NOT centred on LI of zero!
  return(thisscatter)
}

```

```{r plotfacs, echo=F,include=F}
#Creates a plot and saves it; also adds factor scores to ddati
ddati <- makefactorplot(cfa(model.2Fn, estimator="WLSMV",data=ddati),'fit.2Fn',ddati)


```

### 3.3.5 Testing Prediction 3: Model Equivalence for Left- and Right-Handers

The third prediction was: ***better model fit will be obtained when
different parameters are estimated for left- vs. right-handers, compared
with when all parameters are equated for the two handedness groups.***

```{=html}
<!---Level of measurement equivalency are assessed through model fit of a series of nested multiple group models.  
Substantial decrease in goodness of fit indicates non-invariance
Xu: It is a good practice to look at several model fit indices rather than relying on a single one
‚Ä¢ Œîœá2
‚Ä¢ ŒîRMSEA
‚Ä¢ ŒîCFI
‚Ä¢ ŒîTLI
‚Ä¢ ŒîBIC
‚Ä¢ ŒîAIC  


Step 1: Configural invariance
‚ÄÄ Same factor structure in each group
‚ÄÄ First, fit model separately in each group
‚ÄÄ Second, fit model in multiple group but let all parameters vary freely in each group
‚ÄÄ No latent mean difference is estimated
‚ÄÄ 
‚ÄÄ 
Step 2: Weak/metric invariance
‚ÄÄ Constrain factor loadings equal across groups
‚ÄÄ This shows that the construct has the same meaning across groups
‚ÄÄ No latent mean difference is estimated
‚ÄÄ 
Step 3: Strong/scalar invariance
‚ÄÄ Constrain item intercepts equal across groups
‚ÄÄ Constrain factor loadings
‚ÄÄ This is important for assessing mean difference of the latent variable across groups
‚ÄÄ Latent mean difference is estimated
‚ÄÄ 
Step 4: Strict invariance
‚ÄÄ Constrain item residual variances to be equal across groups
‚ÄÄ Constrain item factor loadings and intercepts equal across groups. 
‚ÄÄ Strict invariance is important for group comparisons based on the sum of observed item scores, because observed variance is a combination of true score variance and residual variance
‚ÄÄ Latent mean difference is estimated --->
```
‚ÄÄ

```{r measurementinvariance,echo=F,include=F}
#https://towardsdatascience.com/measurement-invariance-definition-and-example-in-r-15b4efcab351


#library(semTools) fits increasingly restrictive models in one command

#same model in different syntax to make it easier to interpret

model.2Fn <- '
f1 =~  B_P2+A_P1 +C_P3+ E_R2
f2 =~  NA*F_R3 + C_P3+D_R1+ E_R2   #2 factor model:

f2~~1*f2
'

#measurementInvariance(model=model.2Fn,estimator="WLSMV",data=ddati,group="Handed")
#This gives message to say command is deprecated.
#It gives substantial change to model fit for the final fit.means model.


#NB makeSEMtab2 is a function defined at top of script - just makes it easier to compare parameters across groups

fitM0 <- cfa(model.2Fn,estimator="WLSMV", data = ddati,meanstructure=T)
#tabM0 <- makeSEMtab(fitM0)  #

# configural invariance
fitM1 <- cfa(model.2Fn, estimator="WLSMV",data = ddati,meanstructure=T, group = "Handed")

tabM1 <- makeSEMtab2(fitM1,c('_L','_R')) #in tab1, everything varies for the 2 groups

# weak invariance
fitM2 <- cfa(model.2Fn, ddati, estimator="WLSMV",meanstructure=T,group = "Handed",
            group.equal = "loadings")
tabM2 <- makeSEMtab2(fitM2,c('_L','_R'))
#in tab2, the factor loadings are same for the 2 groups, but everything else can vary


# strong invariance
fitM3 <- cfa(model.2Fn, ddati, estimator="WLSMV",meanstructure=T,group = "Handed",
            group.equal = c("intercepts", "loadings"))
tabM3 <- makeSEMtab2(fitM3,c('_L','_R'))
#THis does have same intercepts and loadings, but the correlation between f1/f2 and the variances of each factor can differ, as well as the residuals.

#strong invariance with covariance between factors too
fitM3a <- cfa(model.2Fn, ddati,estimator="WLSMV",meanstructure=T, group = "Handed",
            group.equal = c("intercepts","loadings","lv.covariances"))
tabM3a <- makeSEMtab2(fitM3a,c('_L','_R'))
# This is not included in usual hierarchy of models but seems relevant for our hypothesis, which assumes the two factors have equivalent correlation in L and R handers?. but including this at step 2 gives error re standardized measures

fitM4 <- cfa(model.2Fn, ddati, estimator="WLSMV",meanstructure=T,group = "Handed",
 group.equal = c("loadings","intercepts","means","lv.covariances"))

tabM4 <- makeSEMtab2(fitM4,c('_L','_R'))



# model comparison tests
#mylavtest<-lavTestLRT(fitM1, fitM2, fitM3,fitM3a, fitM4)
mylavtest<-lavTestLRT(fitM1, fitM2, fitM3,fitM3a,fitM4)

save(tabM1,tabM2,tabM3,tabM3a,tabM4,file='SEMtab2groups.RData')
#save tabs which can then be included in supplementary material
```

```{r showlavtest,echo=F,warning=F,message=F}

mylavtest<-add_rownames(mylavtest)
mylavtest<-mylavtest[,-c(3,4)]
myvec<-1:5
mylavtest<-cbind(myvec,mylavtest)
colnames(mylavtest)[1:2]<-c('Model','Group constraints')
mylavtest[,2]<-c('None','Equal loadings','2 + Equal intercepts','3 + Equal factor covariance','4 + Equal factor means')
mylavtest[,4:5]<-round(mylavtest[,4:5],2)

mylavtest[2:5,7]<-pformat2(mylavtest[2:5,7]) #irritatingly, this doesnt give <.001 as it should, because it insists that values are numeric
colnames(mylavtest)[7] <- 'p'

ft<-flextable(mylavtest)
ft<-autofit(ft)
ft
#see https://users.ugent.be/~yrosseel/lavaan/multiplegroup6Dec2012.pdf
tabnumber<-tabnumber+1

```

Table `r tabnumber`: *Nested tests of model equivalence for left- and
right-handers*

The approach we adopted is a standard one used when structural equation
modeling (SEM) is applied to evaluation of measurement models in other
domains, where it is described as a test of measurement invariance.
Essentially, the data from left- and right-handers are analysed together
in a series of nested models; these pose increasingly stringent
constraints on which parameters of the model are allowed to vary for the
two handedness groups. Detailed outputs of the nested models are given
in Supplementary document `r suppnum`.

Initially, a model is fit in which all the paths, covariances, and
intercepts are free to differ between left- and right-handers. This
model is tested against a model of 'metric invariance', which sets the
loadings from each observed variable to the factors to be the same for
the two groups. If the fit of the model does not worsen, we can assume
the basic model structure is equivalent for the two groups. This test of
equivalence was passed (see Table `r tabnumber`).

```{r checkf1f2cov,echo=F}
w<-which(tabM3$Estimates=='f1~~f2')
myests <- tabM3[w,2:5]

```

At the next step, (scalar invariance), the item intercepts are set to be
the same across groups. Once again, the model fit did not worsen (see
Table `r tabnumber`).

Previously, Woodhead et al. (2021) had found weaker covariance between
factors in left-handers than in right-handers. To test whether this was
the case for the current dataset, we added a further constraint, which
was that the covariance between factors should be the same for the two
groups. The covariance between factors was numerically lower in
left-handers (`r myests[1]`, SE = `r myests[2]`) than right-handers
(`r myests[3]`, SE = `r myests[4]`), but the difference was not large,
and model fit was not impaired when the covariance was forced to be
identical in the two handedness groups.

In a final step (strict invariance), we constrain item residual
variances as well as factor loadings and intercepts to be equal across
groups. Here we obtained a substantial worsening of model fit,
indicating that the mean difference between groups on the latent factors
is not the same.

In sum, results from the measurement invariance test showed that,
contrary to our prediction, the same underlying structural model can be
assumed to apply for both left- and right-handers, with the differences
between handedness groups being explained solely in terms of differences
in factor means, rather than in the pattern of covariances between the
six LIs.

```{r factorcompare,echo=F}
cohd1 <- effsize::cohen.d(ddati$Factor1[ddati$Handed=="Right"],ddati$Factor1[ddati$Handed=="Left"])
cohd2 <- effsize::cohen.d(ddati$Factor2[ddati$Handed=="Right"],ddati$Factor2[ddati$Handed=="Left"])
fact1 <- t.test(ddati$Factor1 ~ ddati$Handed)
fact2 <- t.test(ddati$Factor2 ~ ddati$Handed)

es1 <- round(cohd1$estimate,2)
es2 <- round(cohd2$estimate,2)
ci11 <-  round(cohd1$conf.int[1],2)
ci12 <-  round(cohd1$conf.int[2],2)
ci21 <-  round(cohd2$conf.int[1],2)
ci22 <-  round(cohd2$conf.int[2],2)

t1 <- round(fact1$statistic,2)
t2 <- round(fact2$statistic,2)

df1<-round(fact1$parameter,1)
df2<-round(fact2$parameter,1)

pf1<-pformat(fact1$p.value)
pf2<-pformat(fact2$p.value)

#useful to also have sds for factor scores by handedness
agfac1 <- aggregate(ddati$Factor1,by=list(ddati$Handed),FUN=sd)
agfac2 <- aggregate(ddati$Factor2,by=list(ddati$Handed),FUN=sd)
```

In a further exploratory analysis, we compared left- and right-handers
on mean factor scores. Values for Factor 1 were: Left-handers mean =
`r numformat(fact1$estimate[1],2)`, SD = `r numformat(agfac1$x[1],2)`;
Right-handers, mean = `r numformat(fact1$estimate[2],2)`, SD =
`r numformat(agfac1$x[2],2)`. For Factor 1, the effect size for
handedness (Cohen's d) was `r es1`, and a t-test gave t (`r df1`) =
`r t1`, `r pf1`. For Factor 2, Left-handers mean =
`r numformat(fact2$estimate[1],2)`, SD = `r numformat(agfac2$x[1],2)`;
Right-handers, mean = `r numformat(fact2$estimate[2],2)`, SD =
`r numformat(agfac2$x[2],2)`. The effect size for handedness (Cohen's d)
was `r es2`, and a t-test gave t (`r df2`) = `r t2`, `r pf2`.

**Evaluation of Prediction 3: Different model parameters for left- and
right-handers** Our interpretation of this analysis was rather complex,
given the range of possible results. Specifically, in the
preregistration we stated: ***"The simplest result would be to confirm
structural invariance, i.e., no difference in model parameters for left-
vs. right-handers. This is unlikely, given our prior results, but it
would indicate that handedness is unrelated to language laterality
profile. On the basis of prior results we anticipate the best-fitting
model will require different factor means for left- and right-handers.
If so, we will ask whether specifying different means is sufficient to
explain group differences -- this would indicate we can conceptualise
the effect of handedness in terms of a population mean shift. Our
simulated data suggests we may also need to specify different residuals
for the two groups, reflecting greater variance in left-handers; if
confirmed, this would indicate that a mean shift is insufficient to
explain handedness effects, and suggest the underlying laterality
distribution may contain a mixture of left- and right-biased
individuals.*** The obtained result clearly indicated that there were
different factor means for left- and right-handers, and this was
sufficient to account for group differences. The handedness difference
was striking for Factor 1, but more marginal for Factor 2. Thus the most
parsimonious account of the data was that handedness differences could
be explained solely in terms of a shift away from left-hemisphere bias
in left-handers for a laterality factor that had loadings from language
generation tasks.

### 3.3.6 Testing Prediction 4: Categorical Analysis of Laterality Indices

Prediction 4 was: *On categorical analysis, individuals who depart from
left-brained laterality on one or more tasks will be more likely to be
left-handed than those who are consistently left-lateralised.*

The analysis so far has treated laterality as a continuum, but this
continuum does have a zero-point, and negative scores indicate
right-lateralisation and positive scores left-lateralisation. There are
theoretical reasons to suppose that brain function might be influenced
more by consistency in direction of lateralisation, than by degree.
Thus, regardless of how strong or weak a laterality index is, brain
functioning might be more efficient if all language functions are
predominantly mediated by the same hemisphere.

As stated in our preregistration: ***we first adopt the simple approach
of dichotomising laterality at a cutoff of zero for each task, and then
perform a*** $\chi$^2^ analysis to test for association with handedness.
For 6 measures, we adopt a Bonferroni-corrected alpha level of .02/6 =
.003.

```{r categorical-assignment, echo=F}
mycols<-paste0(LETTERS[1:6],'_mean_LI')
for (i in 1:6){
  c<-which(colnames(ddat)==mycols[i])
  myvector<-rep(NA, nrow(ddat))
  w<-which(ddat[,c]>0.000000001)
  myvector[w]<-1
  w<-which(ddat[,c]<0)
  myvector[w]<-0
  ddat[,(1+ncol(ddat))]<-myvector
  colnames(ddat)[ncol(ddat)] <- paste0(LETTERS[i],'_catlatL')
}
```

```{r category-analysis,echo=F}
chidf <- data.frame(matrix(NA,nrow=6,ncol=6))
colnames(chidf) <- c('Task','L hander %','R hander %','R-L hander %','chisq','p')
nucols <- paste0(LETTERS[1:6],'_catlatL')
  myL <- filter(ddat,Handed=='Left')
  myR <- filter(ddat,Handed=='Right')
for (i in 1:6){
    c<-which(colnames(ddat)==nucols[i])
 
  myt<-table(ddat$Handed,ddat[,c])
  chitab<-chisq.test(myt)
  chidf[i,1]<-LETTERS[i]
  chidf[i,2]<-round(100*sum(myL[,c],na.rm=T)/nrow(myL),1)
  chidf[i,3]<-round(100*sum(myR[,c],na.rm=T)/nrow(myR),1)
  chidf[i,5]<-round(chitab$statistic,2)
  chidf[i,6]<-pformat2(chitab$p.value)

}
  chidf[,4] <- (chidf[ ,3] - chidf [ , 2])
  chidf$Task<-c('Word Generation','Sentence Generation','Phonological Decision','Word Decision','Sentence Decision','Syntactic Decision')
  ft<-flextable(chidf)
  autofit(ft)
  
 mydiff<-chidf[,4]
  tabnumber<-tabnumber+1
  

  
```

Table `r tabnumber`: *Proportions showing percentages of left
lateralisation on fTCD tasks, with* $\chi$^2^ test for L vs R hander
difference

Results are shown in Table `r tabnumber`. The trend is similar for all
six tasks, with the proportion who are left-lateralised averaging at
`r round(mean(mydiff), 0)`% lower in left-handers than in right-handers,
regardless of the mean LI for the task. The difference ranged from
`r round(min(mydiff), 0)`% for Syntactic Decision to
`r round(max(mydiff), 0)`% for Sentence Decision, but did not meet our
prespecified significance criterion for any measure.

```{r catreliab-assignment, echo=F}
#In response to reviewer suggestion re reliability of categories, use odd and evens
for (j in 1:2){
mycols<-paste0(LETTERS[1:6],'_mean_odd')
if(j==2){mycols<-paste0(LETTERS[1:6],'_mean_even')}
for (i in 1:6){
  c<-which(colnames(ddat)==mycols[i])
  myvector<-rep(NA, nrow(ddat))
  w<-which(ddat[,c]>0.000000001)
  myvector[w]<-1
  w<-which(ddat[,c]<0)
  myvector[w]<-0
  ddat[,(1+ncol(ddat))]<-myvector
  colnames(ddat)[ncol(ddat)] <- paste0(LETTERS[i],'_catlat_odd')
  if(j==2){
    colnames(ddat)[ncol(ddat)] <- paste0(LETTERS[i],'_catlat_even')}
 }
}
ncol<-length(colnames(ddat))
ddat[,(ncol+1):(ncol+6)]<-NA
colnames(ddat)[(ncol+1):(ncol+6)]<- paste0(LETTERS[1:6],'_catlatOE')
for(i in 1:6){
  c1<-paste0(LETTERS[1:6],'_catlat_odd')
  c2<-paste0(LETTERS[1:6],'_catlat_even')
  w1<-which(colnames(ddat)==c1[i])
  w2<-which(colnames(ddat)==c2[i])
  ddat[,(ncol+i)]<- 10*ddat[,w1]+ddat[,w2]
  
}

#make little table to show inconsistent cases
inconsistents <-data.frame(matrix(NA,nrow=6,ncol=4))
colnames(inconsistents)<-c('Task','LL','RR','inconsistent')
w<-which(colnames(ddat)=='A_catlatOE')
for (i in 1:6){
  temptab<-table(ddat[,(w+i-1)])
  temptab[3]<-temptab[2]+temptab[3]

  ptab<-prop.table(temptab[c(1,3,4)])
  inconsistents[i,c(3,4,2)]<-round(100*ptab,1)
}
 inconsistents[,1]<-c('Word Generation','Sentence Generation','Phonological Decision','Word Decision','Sentence Decision','Syntactic Decision')
```

Our preregistered analysis plan stated: ***If we find no significant
difference between handedness groups, then we can conclude that any true
difference in the percentage of left-lateralised individuals is 17% or
less, i.e., much lower than the estimates of left-sided language
lateralisation for left- and right-handers from lesion or Wada studies
(67% and 95% respectively, a difference of 28%) We anticipate that all
measures will differentiate left- from right-handers with our sample
size of 224; any measure that does not do so would be regarded as an
exception to the general rule that handedness is weakly predictive of
language lateralisation.*** Clearly, this prediction is not supported by
the observed data (albeit with a smaller sample size).

We had preregistered a subsidiary analysis as follows: ***After testing
associations for individual measures, we will categorise individuals as
either consistently left-lateralised on all tests, or right-lateralised
on one or more tests, and conduct a*** $\chi$^2^ test contrasting the
proportion of left- and right-handers on this composite measure.

The proportions of left- and right-handers who are left-lateralised on
between 0 and 6 tasks, using the same cutoff of zero, is shown in Table
`r tabnumber+1`.

Table `r tabnumber+1`: *Proportions of left- and right-handers with
between 0 and 6 tasks left-lateralised on fTCD.*

```{r ntestL,echo=F}
ddat$Nleft <- ddat$A_catlatL+ ddat$B_catlatL+ddat$C_catlatL+ddat$D_catlatL+ddat$E_catlatL+ddat$F_catlatL

tabN <- table(ddat$Nleft,ddat$Handed)
tabNp<-round(prop.table(tabN,2),3)
ntestL <- as.data.frame(cbind(0:6,tabNp[,1],tabNp[,2])) #N tests that are L lateralised
colnames(ntestL)<-c('N tasks L lateralised','L-handers','R-handers')

ft<-autofit(flextable(ntestL))
ft
tabnumber<-tabnumber+1

chitab<-matrix(c(sum(tabN[1:6,1]),sum(tabN[1:6,2]),tabN[7,1],tabN[7,2]),nrow=2)
mychiN<-chisq.test(chitab)

#KW suggested figure - I think table is better.
# mytab<-as.table(t(ntestL[,2:3]))
# bp<-barplot(t(mytab))


#Some exploratory analyses folllow
#repeating just for factor 1
ddat$NleftABCE <- ddat$A_catlatL+ ddat$B_catlatL+ddat$C_catlatL+ddat$E_catlatL

tabN4 <- table(ddat$NleftABCE,ddat$Handed)

ntestL_F1 <- round(prop.table(tabN4,2),3)
chitab4<-matrix(c(sum(tabN4[1:4,1]),sum(tabN4[1:4,2]),tabN4[5,1],tabN4[5,2]),nrow=2)
mychi4<-chisq.test(chitab4)


# How many right/left handers are left lateralised for SG?
SG_left_Lprop <- sum(ddat$B_catlatL[which(ddat$Rhanded==0)])/length(ddat$B_catlatL[which(ddat$Rhanded==0)])
SG_right_Lprop <- sum(ddat$B_catlatL[which(ddat$Rhanded==1)])/length(ddat$B_catlatL[which(ddat$Rhanded==1)])


```

It is evident from Table `r tabnumber` that a minority of individuals is
consistently left-lateralised on all six tasks, regardless of
handedness. The trend is for more right-handers to show this pattern
than left-handers, but this difference is not significant on $\chi$^2^
test, $\chi$^2^ = `r round(mychiN$statistic,2)`,
`r pformat(mychiN$p.value)`. However, two of the tasks included in this
analysis, Word Decision and Syntactic Decision, were not
left-lateralised at the population level, and it could be argued they
would just add noise to the analysis, which was intended to identify
those who departed from the typical pattern of left-lateralisation. We
therefore added an exploratory analysis, in which we excluded these two
tasks. When only Word Generation, Sentence Generation, Phonological
Decision and Syntactic Comprehension were considered,
`r round(100*tabN4[5,1]/sum(tabN4[1:5,1]),1)`% of left-handers and
`r round(100*tabN4[5,2]/sum(tabN4[1:5,2]),1)`% of right-handers were
consistently left-lateralised.

It is noteworthy that this more categorical analysis finds rates of
"atypical", i.e., non-left, lateralisation on language tasks that are
task-dependent, and are lower than typically observed when methods such
as Wada test or fMRI are used. This is the case even for the most
lateralised task, Sentence Generation, where
`r round(SG_right_Lprop*100, 0)`% of right-handers vs.
`r round(SG_left_Lprop*100, 0)`% of left-handers were left-lateralised.

This raises the question of how reliable the categorisation of
laterality is with fTCD. In response to a reviewer suggestion, we
recategorised participants using just the odd or even trials on these
tasks, making it possible to identify cases where lateralisation as left
or right was inconsistent. The percentages with inconsistent laterality,
when a binary divide was placed at zero, were as follows: Word
Generation, `r inconsistents[1,4]`%; Sentence Generation,
`r inconsistents[2,4]`%; Phonological Decision, `r inconsistents[3,4]`%;
Word Decision, `r inconsistents[4,4]`%; Sentence Decision,
`r inconsistents[5,4]`%; Syntactic Decision, `r inconsistents[6,4]`%.

**Evaluation of Prediction 4: On categorical analysis, individuals who
depart from left-brained laterality on one or more tasks will be more
likely to be left-handed than those who are consistently
left-lateralised.** Our preregistered analysis did not support this
prediction, but this negative result should be interpreted cautiously.

It would be premature, given the observed data, to assume that there was
no handedness effect on consistent left-lateralisation. First, the
categorical analysis is less sensitive than the analysis of continuous
laterality indices shown in Table `r ftcdtabnumber`, which clearly
indicate reduced lateralisation for left-handers on a subset of tasks.
Furthermore, as shown in our prior analyses, the different laterality
indices are not independent, and hence Bonferroni correction, which
assumes independence of measures, is over-conservative. We drew a binary
divide at zero, but this means that many cases close to zero will not be
clearly lateralised. An alternative approach would have been to make a
three-way division between lateralised left, not lateralised, and
lateralised right. The main reason for not doing that was that the
numbers in the clearly lateralised groups would be relatively small,
giving low power.

Nevertheless, although we cannot conclude there is no effect of
handedness, it is evident that on the fTCD measures, the differences
between left- and right-handers are modest, and smaller than reported in
the literature on Wada testing. The most striking finding is that, when
we use just a categorical left- vs. right-hemisphere coding, a large
proportion of people are not consistent in their direction of
lateralisation across measures, regardless of handedness.

## 3.4 Relationship Between Behavioural and fTCD Laterality Indices

### 3.4.1 Testing Prediction 5: LIs will be similar for comparable behavioural and blood-flow measures

Our fifth prediction was: ***the laterality profile obtained with the
online language battery will be significantly associated with the
profile seen with the direct measurement of cerebral blood flow using
fTCD, with laterality on Dichotic Listening and Word Comprehension
relating more strongly to receptive language tasks, and Rhyme Decision
to language generation tasks.***

A preliminary inspection of correlations between online and fTCD
laterality indices (Figure `r fignum+1`) showed very little relationship
between the two, even for the two measures, Word Comprehension and Rhyme
Decision, that have analogues in fTCD (Word Decision and Phonological
Decision respectively).

```{r preparedata,echo=F}
ncol<-ncol(ddati)
for (i in 1:nrow(ddati)){
  mysub <- ddati$ID[i]
  w<-which(combdat$ID==mysub)
  ddati$DL_z[i] <- combdat$DL.zlat.ex[w]
   ddati$RDT_z[i] <- combdat$RDT.zlat.ex[w]
   ddati$WC_z[i] <- combdat$WC.zlat.ex[w]
}

#impute missing values using mice package
nunames<-c('DL_z','RDT_z','WC_z')
thisdat.j <- mice(ddati[,nunames], m=1, maxit = 50, method = 'pmm', seed = 500,print=FALSE)
ddati <-cbind(ddati[,1:ncol],complete(thisdat.j,1))

#We save the data frame, which will be used for the big Supplementary output file. 
if(useorigbaseline==1){
  write.csv(ddati,here('data/ddati_origbaseline.csv'),row.names=F)
}

#here we save the version with original baseline that will be used in supplement.
if(useorigbaseline==0){
  write.csv(ddati,here('data/ddati.csv'),row.names=F)
}
```

```{r bigheatmap,echo=F}

#First make little dataframe with split half reliabilities based on those with complete data on all 9 tasks - using Pearson this time for compatibility with SEM

pearson_splits<-data.frame(matrix(NA,ncol=2,nrow=9))
colnames(pearson_splits)<-c('Task','r')
forpearson<-filter(combdat,DopExclude==0,ftcd==1)
temptasks<-c('DL','RDT','WC','A','B','C','D','E','F')
for (i in 1:length(temptasks)){
  oddname<-paste0(temptasks[i],'.odd.zlat')
  evenname<-paste0(temptasks[i],'.even.zlat')
  if(i>3){
    oddname<-paste0(temptasks[i],'_mean_odd')
    evenname<-paste0(temptasks[i],'_mean_even')
  }
  oddcol<-which(colnames(combdat)==oddname)
  evencol<-which(colnames(combdat)==evenname)
  r<-cor(forpearson[,oddcol],forpearson[,evencol],use='complete.obs')
  pearson_splits[i,2]<-r
}

 #need to ensure online LIs from excluded are not included
mycolsb<-c("DL_z","RDT_z","WC_z",  "A_P1", "B_P2","C_P3", "D_R1","E_R2", "F_R3")
behnames<-c("Dichotic","Rhyme Decision","Word Comprehension")
temp<-ddati #make copy with full names for variables and use this for heatmap
w<-vector()
for (i in 1:length(mycolsb)){ #use loop to ensure you get variables in the right order
wx<-which(colnames(temp) == mycolsb[i])
w<-c(w,wx)
}
colnames(temp)[w]<-c(behnames,tasknames)
pearson_splits[,1]<-c(behnames,tasknames)

#THis is used in graphical abstract, includes reliability on diag
graphicalheatmap <- makeLRheatmap(temp,c(behnames,tasknames),pearson_splits,shortnames=T)
ggsave(here("figs/bigLRheatmap.eps"),width = 14, height = 14, units="cm", dpi=300)

fignum<-fignum+1

#This heatmap now substituted for earlier figure with ftcd
ftcdLRheatmap <- makeLRheatmap(temp,tasknames,pearson_splits[4:9,],shortnames=F)
ggsave(here("figs/ftcdLRheatmap.eps"),width = 14, height = 14, units="cm", dpi=300)

#This is original big heatmap with upper triangle only, now with reliab on diag as well
bigheatmap <- makeheatmap(temp,c(behnames,tasknames),pearson_splits)
ggsave(here("figs/bigheatmap.eps"),width = 14, height = 14, units="cm", dpi=300)

```

### Figure `r fignum`

![Heatmap with correlations between  behavioural and fTCD laterality indices for both handedness groups combined. The diagonal (cells with grey frames) shows split-half reliability based on participants who completed both sets of measures.](figs/bigheatmap.eps)

We had preregistered two data checks: 1) Online measures that have
split-half reliability below .6 will be excluded from further analysis.
2) Online measures of Word Comprehension and Rhyme Decision will only be
taken forward to the next stage of analysis if they have a correlation
of at least .11 with the counterpart measure from fTCD (Word Decision
and Phonological Decision respectively).

The online Word Comprehension measure failed the second check: the
correlation with the fTCD Word Decision laterality index was close to
zero. The correlation between the online Rhyme Decision and fTCD
Phonological Decision was
`r round(cor(ddati$RDT_z,ddati$C_P3,use='complete.obs'),3)`, meeting our
criterion, but split-half reliability was only `r onlinesummary[11,3]`.
Accordingly, we proceeded with the next step of analysis only with
Dichotic Listening (which had good reliability, but no counterpart in
the fTCD battery).

We had predicted that Dichotic Listening, as a receptive task, should
load on the same factor as the Word Decision, Sentence Decision and
Syntactic Decision, but it is evident from the heatmap that, insofar as
it correlates with the fTCD tasks, the strongest association is with
Sentence Generation, a production task.

```{r newmodelx,echo=F,include=F}

#Base model: dichotic included but with path set to zero

model.2Fnbase <- '
f1 =~  NA*A_P1 +1*B_P2+C_P3+ E_R2
f2 =~  NA*C_P3+D_R1+ E_R2 +F_R3 +0*DL_z  #2 factor model:

f2~~1*f2
'

fit.2Fnbase <- cfa(model.2Fnbase,estimator="WLSMV", data=ddati)
summary(fit.2Fnbase)
wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2Fn,wantfits)

#Dichotic loads on factor 2
model.2Fnx <- '
f1 =~  NA*A_P1 +1*B_P2+C_P3+ E_R2
f2 =~  NA*C_P3+D_R1+ E_R2 +F_R3 +DL_z  #2 factor model, dichotic on Fac2:

f2~~1*f2
'
fit.2Fnx <- cfa(model.2Fnx,estimator="WLSMV", data=ddati)
summary(fit.2Fnx)
wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2Fnx,wantfits)



#Dichotic loads on factor 1
model.2Fny <- '
f1 =~  NA*A_P1 +1*B_P2+C_P3+ E_R2+DL_z 
f2 =~  NA*C_P3+D_R1+ E_R2 +F_R3  #2 factor model, dichotic on Fac1:

f2~~1*f2
'
fit.2Fny <- cfa(model.2Fny,estimator="WLSMV", data=ddati)
summary(fit.2Fny)
wantfits <- c('CFI','rmsea','chisq','df')
fitmeasures(fit.2Fny,wantfits)
fitcompare<-anova(fit.2Fnbase,fit.2Fnx,fit.2Fny)

tab.2Fnbase <- makeSEMtab(fit.2Fnbase)
tab.2Fnx <- makeSEMtab(fit.2Fnx)
tab.2Fny <- makeSEMtab(fit.2Fny)

#measurementInvariance(model=model.2Fny,estimator="WLSMV",data=ddati,meanstructure=T,group="Handed")



pathfigname<-here("figs/pathfigF2_bigA")
semPaths(fit.2Fnx, "diagram", weighted = FALSE,  shapeMan = "rectangle", sizeMan = 8, 
    sizeMan2 = 5,filetype='eps',filename=pathfigname,width=3,height=2) #draws a path diagram

pathfigname<-here("figs/pathfigF2_bigB")
semPaths(fit.2Fny, "diagram", weighted = FALSE,  shapeMan = "rectangle", sizeMan = 8, 
    sizeMan2 = 5,filetype='eps',filename=pathfigname,width=3,height=2) #draws a path diagram


#Looking at residuals : dichotic has high variability and big residuals - may be worth trying to scale it - or possibly just censor at a less extreme value
```

```{r models-with-dichotic,echo=F}
modelnames<-c('No dichotic path','Dichotic <- Factor 1','Dichotic <- Factor 2')
dichmodels<-cbind(modelnames,fitcompare[c(3,1,2),])
dichmodels <- dichmodels[,c(1,2,5)]
dichmodels[,3]<-round(dichmodels[,3],2)
colnames(dichmodels)[1:2]<-c('Path to dichotic','DF')
fd<-flextable(dichmodels)
fd<-autofit(fd)
fd
tabnumber<-tabnumber+1


```

Table `r tabnumber`: *Fit statistics for models including Dichotic
Listening*

In practice, a model including a free path from Dichotic Listening to
Factor 2 gave a better fit than a model with the path fixed to zero
(Table `r tabnumber`): $\chi$^2^ difference =
`r round(dichmodels$Chisq[1]-dichmodels$Chisq[2],2)`, DF = 1, p \< .001.
This is not a strong test of our prediction, because it will be passed
if there is even a weak correlation between dichotic LI and fTCD
laterality indices. Guided by the data, we ran an alternative model (not
preregistered) with Dichotic Listening loading on Factor 1. This also
gave excellent fit, with lower $\chi$^2^ than the preregistered model
(for full results see Supplementary Material `r suppnum`).

**Evaluation of Prediction 5: Equivalence of behavioural and fTCD
laterality indices**\
In our preregistration analysis plan, we stated: ***If the online
measures do not correspond to the parallel measures from fTCD this could
mean that these measures are insufficiently reliable to index laterality
in individuals -- this would be evidenced by poor split-half
reliability, and would indicate the need to either abandon this approach
or to seek better measures. If the measures are reliable, but 95%
confidence intervals for path estimates of online measures span zero, we
can conclude that the online measures are tapping different aspects of
laterality than the fTCD measures. If measures are reliable and good fit
is obtained for a two-factor model that incorporates the laterality
indices from online measures, this would support the use of online tests
as proxy measures for underlying lateralised brain activation***

Overall, the associations between behavioural and fTCD laterality
indices were low enough to give little confidence in the specific
pattern of associations. Although the split-half reliability of the
online measures was not high, it was not so low as to explain the lack
of association with fTCD measures. The main conclusion is that our
online behavioural measures of laterality based on speed of responding
to lateralised stimuli have little in common with measures of relative
blood flow to the two hemispheres while performing the same tasks, and
cannot be used as proxy measures.

## 3.5 Additional Analysis of Relationship between Behavioural Performance and Laterality Indices

It seemed possible that the laterality indices might be influenced by
either the language status of participants, and/or behavioural scores on
the fTCD tasks (number of items generated or percentage correct).
Supplementary file `r suppcorr` shows relevant analyses indicating that
this is not the case.

# 4. Discussion

In this study, we measured individual differences in language laterality
using two approaches: behavioural biases on online measures, and
task-related blood flow to left and right hemispheres using fTCD. Our
focus was on the extent to which laterality measures were associated,
and whether the pattern of association differed for left- and
right-handers.

We had preregistered five specific predictions, none of which was
confirmed. Nevertheless, the study has taken forward our understanding
of language laterality, by allowing us to dismiss certain hypotheses,
suggesting new avenues for research, and evaluating comparability of
different ways of measuring cerebral lateralisation.

To simplify the interpretation of this complex dataset, we focus first
on two overarching questions addressed by the study. The first question
concerns correlations between laterality measures: in brief, is there
evidence for a single language laterality dimension on which people
vary? The second question concerns handedness: does the answer to our
first question differ in groups of left- and right-handers? In addition
we consider specific issues arising in this dataset, namely the finding
of right hemisphere lateralisation on Word Decision, and the lack of
agreement between behavioural and fTCD laterality indices. Finally, we
consider how the particular factor structure seen in the fTCD analysis
might be explained.

## 4.1 No Support For a Single Laterality Dimension

Previous attempts to consider the dimensionality of language
lateralisation have been obscured by two issues. First, many studies
have been conducted with measures whose reliability was not established.
If two laterality indices are not correlated, it could just be because
they are unreliable, and so it has been easy to dismiss lack of
correlation between laterality measures as uninformative. Second,
researchers have tended to focus only on measures that show
left-lateralisation at the population level, treating unlateralised
language measures as uninteresting.

Considering first the online behavioural data, the most noteworthy
observation was that the correlations between laterality indices from
the three tasks were generally weak. This could not be attributed solely
to poor reliability: although reliabilities of the two new tasks, Rhyme
Decision and Word Comprehension, were not impressive (ranging from
.54-.55 for test-retest), they were higher than the intercorrelations
between measures. Our data was not suitable for a more formal model
comparison, but inspection of the pattern of correlations between
measures made it clear that our proposed two-factor model, with Dichotic
Listening and Word Comprehension being positively correlated and
unrelated to Rhyme Decision could not be supported. Indeed, the
strongest correlation was found between Word Comprehension and Rhyme
Decision, consistent with the idea that task demands (speeded responding
to picture stimuli) might be a greater determinant of strength of
lateralisation than whether receptive or expressive language was
involved.

For the fTCD data, results were generally in good agreement with
Woodhead et al. (2021), with good reliability of LIs on most tasks, even
though some tasks were not left-lateralised. As in our previous study
(Woodhead et al., 2019), the LI on Syntactic Decision task had good
reliability, and a distinctive pattern of association with other LIs,
despite being unlateralised. This observation shows that lack of
lateralisation at the population level does not mean that all
individuals use both hemispheres equally for the task: it seems rather
that the population contains a mixture of people, some of whom
consistently prefer the left hemisphere, others the right, and others
more equally balanced. The LI from the Word Decision task was the least
reliable in the battery, yet again showed quite distinctive patterns of
selective association with other tasks.

With fTCD we were able to subject the single factor model to a stronger
test, because we had sufficient tasks for Structural Equation Modeling.
Consistent with Woodhead et al (2021), we could reject a single factor
model; this gave a poor fit to the data, as it could not account for the
fact that the correlations between LIs tended to form clusters. We
tested a preregistered, alternative two-factor model that involved a
division between language generation and language reception. This
accounted for significantly more variance than the single-factor model,
but still left a great deal unexplained, and overall the fit was poor.
Accordingly, following our preregistration, we divided the sample into
two sub samples to explore different models and found one that gave good
fit, which was then replicated in the second half of the sample. This
again had a two factor structure, but had two of the tasks, Phonological
Decision and Syntactic Comprehension, loading on both the factors.

In a final step of analysis, we considered adding the laterality indices
from online tasks to the model. The correlations between LIs from online
tasks and fTCD were generally weak, and these measures did not help
differentiate models. A model that included Dichotic Listening gave
better fit when a non-zero path was included, than when it was set to
zero, but the best fit was seen for a model where Dichotic Listening
loaded on Factor 1 (with language generation tasks), rather than for our
prespecified model where Dichotic Listening was regarded as an indicator
of Factor 2 (with receptive tasks).

## 4.2 Left- Versus Right-Handers

For both behavioural and fTCD LIs, with just one exception, there was a
consistent trend for stronger left-hemisphere bias in right-handers than
in left-handers. This reached significance on all measures except fTCD
Word Decision. The exception was online Rhyme Judgement, which was not
left-lateralised and where means for left- and right-handers were very
similar.

The SEM analysis allowed us to go beyond simple comparison of means to
test whether the association between LIs showed a similar pattern in the
two handedness groups. In our previous fTCD study using four of the same
measures (Woodhead et al., 2021), we had concluded that there was more
dissociation between factors in left-handers, but this finding was not
replicated here. There are two possible explanations for this
discrepancy. First, the sample size in both the current and the previous
study was small for this kind of analysis, raising the possibility that
the initial finding was a false positive, or the current finding a false
negative. Even with the current sample size, power to detect model
invariance is not high. Second, the language tasks used with fTCD
differed across studies. In the Woodhead et al. study, we included a
List Generation task, that was intended to be a relatively pure measure
of phonological output, using overlearned sequences, as well as a
Semantic Decision task, which involved judging whether two pictures were
semantically related. The List Generation task was dropped because of
low reliability, and the Word Decision task was substituted for Semantic
Decision to give a purer measure of comprehension at the single word
level. We cannot rule out the possibility that these tests would show
different patterns of association in left- and right-handers. Note,
however, that the data did not support an additional possibility, namely
that left-handers are more variable than right-handers. The comparison
of factor scores in the two handedness groups showed differences in
means, but only small differences in standard deviations.

Overall, we can conclude that the current data suggest that there is no
reason to postulate different models for left- vs. right-handers. The
substantial differences between these groups could be entirely accounted
for in terms of differences in factor means, and was driven primarily by
differences on Factor 1, which related to tasks involving language generation.  

These quantitative differences between handedness groups did not,
however, translate into pronounced differences in proportions who were
atypically lateralised on individual tasks, and most people had a
mixture of left- and right-lateralisation across the whole fTCD battery.

## 4.3 Right Hemisphere Lateralisation for Comprehension of Single Words

The finding of a slight bias to right hemisphere lateralisation for the
behavioural Word Comprehension task and Word Decision on fTCD. Woodhead
et al. (2021), using fTCD, showed left lateralisation in right-handers
for a semantic decision task that involved judging if two (unnamed)
pictured items were semantically related. A meta-analysis of fMRI
studies by Vigneau et al. (2011) found that right-hemisphere involvement
in lexical-semantic tasks was extremely limited. However, tasks used in
fMRI typically involve thinking explicitly about meaning; e.g. Binder et
al. (1996) devised a classic semantic decision task that is highly
left-lateralised, which involves listening to animal names and deciding
if they met specific semantic criteria (e.g. "native to the United
States"). Another consideration is that written, rather than auditory,
presentation is commonly used in fMRI studies to avoid interference from
scanner noise. In contrast, our current task required only a direct
matching of a spoken word to one of a pair of semantically-related
pictures. This task was the easiest in the battery, with near-ceiling
performance by all participants, and it could be that it was
insufficiently challenging to engage lateralised language systems. It
might be tempting to suppose that right-hemisphere bias was induced by
the left-right scanning of pictured items, but if that were the case, we
should also have seen this bias in Rhyme Decision (which was weakly left-lateralised), and we should not have found a rightward bias in the fTCD Word Decision task, where pictures are presented vertically.

The relatively low reliability of the task, in both behavioural and fTCD
formats, indicates the need for caution in interpretation. We might be
tempted to dismiss the result, except for the fact that the bias was
found both in the online behavioural version of the task (see Table
`r tabonlinesummary` and in the analogous Word Decision fTCD task (see
Table `r ftcdtabnumber`). Furthermore, on the behavioural task,
left-handers showed a stronger effect than right-handers, in line with
their reduced left-hemisphere bias on other tasks. This consistent
picture, however, is challenged by the fact that the correlation between
LIs on the online and fTCD versions of the task was close to zero.

Overall, the pattern of results is puzzling, and we will need more
reliable measures of single word comprehension to determine whether it
is meaningful. It does raise the possibility that where semantic
decision tasks are lateralised, this may relate to task demands that
involve explicit reasoning about word meanings.In addition, we need to
be aware that other, nonlinguistic, factors, such as presentation rate
may also affect laterality indices (Payne, Gutierrez-Sigut, Subik, Woll,
& MacSweeney, 2015).

## 4.4 Differences Between Behavioural and fTCD Measures of Laterality

The lack of agreement between online and fTCD laterality indices is
disappointing for those hoping to use behavioural measures as proxies
for more direct brain measures of laterality. It is, of course, possible
that stronger associations might be seen with better measures:
behavioural measures obtained under laboratory conditions are likely to
be more reliable than those obtained online; the tasks we developed for
fTCD might have limitations in terms of variation in strategies used by
participants, or in the impact of nonlinguistic task demands that could
also engage lateralised systems. Nevertheless, even for Dichotic
Listening, a task that has a long track record as a behavioural
laterality index, and Word Generation, the gold standard laterality
measure in fTCD, the correlation between LIs was weak. This is despite
the fact that both tasks are consistently lateralised with good
reliability. Furthermore, validity of the online Dichotic Listening task
is supported by the fact that it showed a small but reliable difference
in laterality for left- and right-handers, similar to that previously
reported by Karlsson et al. (2019), who used in-person rather than
online testing.

Several researchers have proposed that when optimal methods are used,
dichotic listening and/or visual half-field methods can be useful
indicators of language laterality (Van der Haegen et al., 2013;
Westerhausen, 2019), and it would seem that, when the goal is to
categorise individuals as left- or right-lateralised for language,
strong lateralisation on such behavioural measures is usually a good
indicator of laterality on brain measures. Nevertheless, many people do
not show such strong lateralisation, and when studies have considered
quantitative associations between laterality indices from dichotic
listening and fMRI, correlations have been unimpressive; e.g. Bethmann
et al. (2007) reported a correlation of .38, finding that many
individuals with a left-ear advantage on dichotic listening were
left-lateralised for language on a synonym decision task. Other studies
have found higher correlations than those reported in the current study,
but the level of agreement is moderate at best, and small samples mean
that there will be large confidence intervals around these estimates.
Data extracted from a scatterplot by Van der Haegen et al. (2013) gave a
Spearman correlation between dichotic listening and fMRI (word
generation) of .46 (N = 62), in a sample that excluded cases of
ambiguous laterality on fMRI (those with absolute value of LI below .6).
Hund-Georgiadis et al. (2002) tested 17 left-handers and 17
right-handers and found Spearman correlation of 0.56 between laterality
indices from dichotic listening and semantic encoding on fMRI (computed
from data extracted from scatterplot). Gerrits, De Clercq, Verhelst, &
Vingerhoets (2020) found that a laterality index from a visual
half-field naming task gave a Spearman correlation of .54, (95% CI .31,
.71) with laterality on word generation from fMRI in a sample of 63
left-handers, 38 of whom were selected as candidates for
right-hemisphere language because they had a difference of at least 20
ms in response time in favour of the left vs right visual field. On
categorical assignment based on fMRI, 58% of the candidate cases had
right-hemisphere language confirmed. This indicates that on the one
hand, behavioural tasks have potential to screen for cases of atypical
lateralisation, but on the other hand, agreement between behavioural
laterality and fMRI laterality is imperfect, even when the behavioural
task involves speech production.

It is well worth pursuing the goal of standardizing behavioural
laterality measures and working towards optimising their reliability, as
exemplified by work by Westerhausen and Samuelsen (2020). We suspect,
however, that the lack of agreement between different kinds of
laterality measure might not be resolved simply by improving
reliability. If, as we propose, language laterality is not unitary, then
the agreement between different measures will depend on the relative
contribution of different lateralised systems. In this regard, it is
worth noting that fMRI studies of dichotic listening indicate activation
that extends well beyond the temporal lobes to include bilateral
activity in frontal lobes, which seems in part dependent on the
inhibitory demands of the task (J√§ncke & Shah, 2002; Westerhausen et
al., 2014).

## 4.5 Interpreting the Two-Factor Structure

Although our data did not support the simple two-factor structure of
laterality that we predicted, we were able to obtain a good fit for a
model that included additional paths. This two-factor account is
reminiscent of the dual stream model of Hickok and Poeppel (2007), who
postulated a dorsal stream from superior temporal to premotor cortices
via the arcuate fasciculus, and a ventral stream from temporal cortex to
anterior inferior frontal gyrus. The former is implicated in integrating
auditory speech with articulator motor actions, and is lateralised,
whereas the latter is not lateralised, and is involved in access to
conceptual memory and mapping of sound to meaning. However, as noted in
our previous study, the exclusive loading of Sentence Generation on
factor 1, and the loading of Sentence Decision on both factors is not
entirely consistent with the dual stream account. We also previously
found List Generation, which would be expected to involve the dorsal
stream, was not lateralised.

We conclude by considering what commonalities there are between tasks
that characterise each factor.

```{r taskanalysis,echo=F}

tasks <- read.csv(here("data/task_details.csv"))
ftasks <- flextable(tasks[,2:7])

ftasks<-fit_to_width(ftasks,8,inc=1L)
ftasks <- flextable::fontsize(ftasks,size=9)
ftasks
tabnumber <- tabnumber+1
```

Table `r tabnumber`: *Characteristics of tasks*

Table `r tabnumber` summarises the characteristics of the six fTCD
tasks, grouped according to the factors they load on. The online
Dichotic Listening task is also shown. The task battery had been
designed to include three tasks that involved language generation, and
three that involved receptive language. Note that, in contrast to most
tasks used in fMRI studies, spoken language was used to present stimuli
for the receptive tasks. For Syntactic Decision this was supplemented
with written words, as the task was otherwise too difficult. As
predicted, the language generation tasks loaded on Factor 1 and the
receptive tasks on Factor 2, but in addition Phonological Decision
loaded on Factor 2, and Sentence Decision on Factor 1. Phonological
Decision, unlike the other tasks loading on Factor 2, did not involve
auditory input, but did have in common the 2-choice response format of
other Factor 2 tasks.

Sentence Decision also behaved unexpectedly, in that it had significant
loadings on Factor 1, despite being designed as a purely receptive task.
Unlike the other two receptive tasks, Sentence Decision requires the
listener to use syntactic information to assign semantic roles and build
meaning representations, and hence more linguistic computation than the
receptive tasks that used single word stimuli (Word Decision) or
meaningless material (Syntactic Decision).

As seen in Figure `r piratenumber`, the four tasks that load on Factor 1
are all significantly lateralised, at least in right-handers. In
contrast, those loading on Factor 2 include two tasks that are not
significantly lateralised.

In interpreting this finding, we should first rule out two trivial
explanations for the factor structure uncovered in SEM. First, this
structure cannot be regarded as an artefact of including tasks differing
in degree of laterality. This is because factor structure in SEM is
computed solely on the basis of covariances between measures, and means
do not affect it. Thus, we could add a constant to the LIs for tasks D
and E to make them lateralised, and the factor solution would remain the
same.

Second, it is unlikely that the pattern of results is simply due to
contamination of the laterality index by non-linguistic activations.
This is because with fTCD we do a direct subtraction of blood flow
velocity in left and right hemispheres. Activation due, for instance, to
visual processing, would influence the laterality index only if such
activation were also lateralised.

Third, we can rule out an explanation that treats the non-lateralised
tasks as not relevant for studying individual differences in laterality.
Such an explanation would be justified if tasks such as Word Decision,
Sentence Decision and Syntactic Decision were simply unreliable. Low
reliability would be expected if these tasks were not lateralised in
individuals, because people used both hemispheres jointly, or switched
from one to the other at random. The new task, Word Decision, was the
least reliable in the battery, but nevertheless, the split-half
reliability was moderate. The other two receptive tasks had good
test-retest reliability in our previous study (Woodhead et al, 2021) and
good split-half reliability in the current study. Thus, even though
there is weak or absent lateralisation at the population level on these
tasks, the degree and direction of lateralisation is reasonably
consistent within individuals. And indeed, if that were not the case, we
would not expect the tasks to show moderate intercorrelations with one
another.

To account for the observed pattern of results, we postulate two
language centres, one lateralised at the population level (centre L),
and the other centred on zero (centre Z). An individual's observed fTCD
laterality on a task will depend on the extent to which these two
centres are implicated in task performance, with Word Generation and
Sentence Generation being largely dominated by centre L, Syntactic
Decision and Word Decision by centre Z, and Phonological Decision and
Sentence Decision implicating both centres.

To some extent, this is less of an explanation than a redescription of
the data, but it does yield novel predictions that can be tested using
fMRI, which gives information on localisation of activation within a
hemisphere. The prediction would be that there would be more overlap in
brain regions activated by tasks that load on the same factor than for
those loading on different factors, and furthermore, activation would be
lateralised only for brain regions supporting Factor 1 tasks. The
BIL&GIN fMRI study (Mazoyer et al., 2016) included data from tasks
analogous to those used here and could be used to test these
predictions.

## 5. Conclusions

Language laterality in individuals is not a single dimension, but varies
depending on task demands. A simple two-factor model that distinguished
tasks involving language generation and comprehension was superior to a
single-factor model in accounting for individual variation, but tasks
did not neatly map onto the two factors. Previously, we had suggested
that a different factor structure would be seen in left- and
right-handers, but that was not the case for the current dataset:
handedness determined the mean level of laterality on a language
generation factor, but not the covariance between factors. Behavioural
laterality measures, assessed online, were only weakly related to
laterality measured using fTCD. The full dataset provides unique
possibilities for assessing associations between different laterality
measures and is openly available for exploration by other researchers
without restriction.

## 6. References

Anwyl-Irvine, A. L., Massonni√©, J., Flitton, A., Kirkham, N., &
Evershed, J. K. (2020). Gorilla in our midst: An online behavioral
experiment builder. Behavior Research Methods, 52(1), 388--407.
<https://doi.org/10.3758/s13428-019-01237-x>\
Balota, D. A., Yap, M. J., Hutchison, K. A., Cortese, M. J., Kessler,
B., Loftis, B., ... & Treiman, R. (2007). The English lexicon project.
Behavior Research Methods, 39(3), 445-459.\
Bethmann, A., Tempelmann, C., De Bleser, R., Scheich, H., & Brechmann,
A. (2007). Determining language laterality by fMRI and dichotic
listening. Brain Research, 1133, 145--157.
<https://doi.org/10.1016/j.brainres.2006.11.057>\
Binder, J. R., Swanson, S. J., Hammeke, T. A., Morris, G. L., Mueller,
W. M., Fischer, M., Benbadis, S., Frost, J. A., Rao, S. M., & Haughton,
V. M. (1996). Determination of language dominance using functional MRI:
A comparison with the Wada test. Neurology, 46(4), 978--984.
<https://doi.org/10.1212/wnl.46.4.978> Bishop, D. V. M. (1980).
Measuring familial sinistrality. Cortex, 16, 311-313.\
Bishop, D. V. M. (1990). On the futility of using familial sinistrality
to subclassify handedness groups. Cortex, 26, 153-155.\
Bishop, D. V. M. (2003). The Test for Reception of Grammar, version 2
(TROG-2). London: Pearson.\
Bishop, D. V. M. (2013). Cerebral asymmetry and language development:
Cause, correlate, or consequence? Science, 340(6138).
<doi:10.1126/science.1230531>\
Bless, J. J., Westerhausen, R., Arciuli, J., Kompus, K., Gudmundsen, M.,
& Hugdahl, K. (2013). "Right on all occasions?"-- On the feasibility of
laterality research using a smartphone dichotic listening application.
Frontiers in Psychology, 4, 42.\
Bradshaw, A. R., Bishop, D. V., & Woodhead, Z. V. (2017). Methodological
considerations in assessment of language lateralisation with fMRI: a
systematic review. PeerJ, 5, e3557.
<https://doi.org/10.7717/peerj.3557>\
Bryden, M. P. (1975). Speech lateralisation in families: A preliminary
study using dichotic listening. Brain and Language, 2, 201-211.\
Bryden, M.P. (1982). Laterality: Functional Asymmetry in the Intact
Brain. New York: Academic Press.\
Carey, D. P., & Johnstone, L. T. (2014). Quantifying cerebral
asymmetries for language in dextrals and adextrals with random-effects
meta analysis. Frontiers in Psychology, 5, 1128.
<doi:10.3389/fpsyg.2014.01128>\
Champely, S. (2018). pwr: Basic Functions for Power Analysis. R package
version 1.2-2. Retrieved from <https://CRAN.R-project.org/package=pwr>\
Corey, D., & Foundas, A. (2010). Measuring familial sinistrality:
Problems with dichotomous classification. Laterality, 10(4), 321-335.
<doi:10.1080/13576500442000111>\
Du√±abeitia, J. A., Crepaldi, D., Meyer, A. S., New, B., Pliatsikas, C.,
Smolka, E., & Brysbaert, M. (2018). MultiPic: A standardized set of 750
drawings with norms for six European languages. Quarterly Journal of
Experimental Psychology, 71(4), 808--816.
<https://doi.org/10.1080/17470218.2017.1310261>\
Fedorenko, E., Hsieh, P.-J., Nieto-Casta√±√≥n, A., Whitfield-Gabrieli, S.,
& Kanwisher, N. (2010). New method for fMRI investigations of language:
Defining ROIs functionally in individual subjects. Journal of
Neurophysiology, 104(2), 1177--1194.
<https://doi.org/10.1152/jn.00032.2010>\
Fesl, G., Bruhns, P., Rau, S., Wiesmann, M., Ilmberger, J., Kegel, G., .
. . 2010, P. M. (2010). Sensitivity and reliability of language
laterality assessment with a free reversed association task-a fMRI
study. European Radiology, 20(3), 683-695.
<doi:10.1007/s00330-009-1602-4>\
Fletcher, T. D. (2010). psychometric: Applied Psychometric Theory. R
package version 2.2. Retrieved from
<https://CRAN.R-project.org/package=psychometric>\
Gerrits, R., De Clercq, P., Verhelst, H., & Vingerhoets, G. (2020).
Evaluating the performance of the visual half field paradigm as a
screening tool to detect right hemispheric language dominance.
Laterality, 25(6), 722--739.
<https://doi.org/10.1080/1357650X.2020.1854279>\
Gerrits, R., Verhelst, H., & Vingerhoets, G. (2020). Mirrored brain
organization: Statistical anomaly or reversal of hemispheric functional
segregation bias? Proceedings of the National Academy of Sciences, 117,
14057-14065. <doi:10.1073/pnas.2002981117>\
Hartshorne, J. K., Tenenbaum, J. B, & Pinker, S. (2018). A critical
period for second language acquisition: Evidence from 2/3 million
English speakers. Cognition, 177, 263-277.
<doi:https://doi.org/10.1016/j.cognition.2018.04.007>\
Herv√©, M. (2021). RVAideMemoire: Testing and Plotting Procedures for
Biostatistics. R package version 0.9-79.\
Hickok, G., & Poeppel, D. (2007). The cortical organization of speech
processing. Nature Reviews. Neuroscience, 8(5), 393--402.
<https://doi.org/10.1038/nrn2113>\
Hoaglin, D. C., & Iglewicz, B. (1987). Fine-tuning some resistant rules
for outlier labelling. Journal of the American Statistical Association,
82(400), 1147-1149.\
Hugdahl, K., & Andersson, L. (1986). The "forced-attention paradigm" in
dichotic listening to CV-syllables: a comparison between adults and
children. Cortex, 22(3), 417-432.\
Hund-Georgiadis, M., Lex, U., Friederici, A. D., & von Cramon, D. Y.
(2002). Non-invasive regime for language lateralization in right- and
left-handers by means of functional MRI and dichotic listening.
Experimental Brain Research, 145(2), 166--176.
<https://doi.org/10.1007/s00221-002-1090-0>\
Hunter, Z. R., & Brysbaert, M. (2008). Visual half-field experiments are
a good measure of cerebral language dominance if used properly: Evidence
from fMRI. Neuropsychologia, 46(1), 316-325.\
J√§ncke, L., & Shah, N. J. (2002). Does dichotic listening probe temporal
lobe functions? Neurology, 58(5), 736--743.
<https://doi.org/10.1212/wnl.58.5.736>\
Johnstone, L. T., Karlsson, E. M., & Carey, D. P. (2020). The validity
and reliability of quantifying hemispheric specialisation using fMRI:
Evidence from left and right handers on three different cerebral
asymmetries. Neuropsychologia, 138, 107331.
<doi:10.1016/j.neuropsychologia.2020.107331>\
Karlsson, E. M., Johnstone, L. T., & Carey, D. P. (2019). The depth and
breadth of multiple perceptual asymmetries in right handers and
non-right handers. Laterality, 24(6), 707-739.
<https://doi.org/10.1080/1357650X.2019.1652308>\
Kline, R. B. (2011). Principals and Practice of Structural Equation
Modeling, 3rd Edition. Guilford Press.\
Knecht, S., Deppe, M., Ebner, A., Henningsen, H., Huber, T., Jokeit, H.,
& Ringelstein, E. B. (1998). Noninvasive determination of language
lateralization by functional transcranial Doppler sonography A
comparison with the Wada test. Stroke, 29(1), 82-86.
<http://dx.doi.org/> 10.1161/01.STR.29.1.82\
Knecht, S., Deppe, M., Dr√§ger, B., Bobe, L., Lohmann, H., Ringelstein,
E.-B., & Henningsen, H. (2000). Language lateralization in healthy
right-handers. Brain, 123(1), 74--81.
<https://doi.org/10.1093/brain/123.1.74>\
Lee, D., Swanson, S. J., Sabsevitz, D. S., Hammeke, T. A., Scott
Winstanley, F., Possing, E. T., & Binder, J. R. (2008). Functional MRI
and Wada studies in patients with interhemispheric dissociation of
language functions. Epilepsy & Behavior: E&B, 13(2), 350--356.
<https://doi.org/10.1016/j.yebeh.2008.04.010>\
Lemh√∂fer, K., & Broersma, M. (2012). Introducing LexTALE: A quick and
valid Lexical Test for Advanced Learners of English. Behavior Research
Methods, 44(2), 325--343. <https://doi.org/10.3758/s13428-011-0146-0>\
Mazoyer, B., Mellet, E., Perchey, G., Zago, L., Crivello, F., Jobard,
G., Delcroix, N., Vigneau, M., Leroux, G., Petit, L., Joliot, M., &
Tzourio-Mazoyer, N. (2016). BIL&GIN: A neuroimaging, cognitive,
behavioral, and genetic database for the study of human brain
lateralization. NeuroImage, 124, 1225--1231.
<https://doi.org/10.1016/j.neuroimage.2015.02.071>\
Mazoyer, B., Zago, L., Jobard, G., Crivello, F., Joloit, M., Perchey,
G., . . . Tzourio-Mazoyer, N. (2014). Gaussian Mixture Modeling of
hemispheric lateralization for language in a large sample of healthy
individuals balanced for handedness. PLOS One, 9, e101165.
<doi:10.1371/journal.pone.0101165>\
McKeever, W. F., & Vandeventer, A. D. (1977). Visual and auditory
language processing asymmetries: influences of handedness, familial
sinistrality, and sex. Cortex, 13, 225-241.\
Moshagen, M. (2020). semPower: Power Analyses for SEM. R package version
1.0.1. Retrieved from <https://CRAN.R-project.org/package=semPower>.\
Moshagen, M., & Erdfelder, E. (2016). A new strategy for testing
structural equation models. Structural Equation Modeling, 23, 54-60.
<doi:10.1080/10705511.2014.950896>\
Ocklenburg, S., Str√∂ckens, F., Bless, J. J., Hugdahl, K., Westerhausen,
R., & Manns, M. (2016). Investigating heritability of laterality and
cognitive control in speech perception. Brain and Cognition, 109,
34--39. <https://doi.org/10.1016/j.bandc.2016.09.003>\
Oldfield, R. C. (1971). The assessment and analysis of handedness: the
Edinburgh inventory. Neuropsychologia, 9, 97-113.\
Orsini, D. L., Satz, P., Soper, H. V., & Light, R. K. (1985). The role
of familial sinistrality in cerebral organization. Neuropsychologia,
23(2), 223--232.
[https://doi.org/10.1016/0028-3932(85)90106-x](https://doi.org/10.1016/0028-3932(85)90106-x){.uri}\
Parker, A. J., Woodhead, Z. V. J., Thompson, P. A., & Bishop, D. V. M.
(2021). Assessing the reliability of an online behavioural laterality
battery: A pre-registered study. Laterality, 26(4), 359--397.
<https://doi.org/10>.\
Payne, H., Gutierrez-Sigut, E., Subik, J., Woll, B., & MacSweeney, M.
(2015). Stimulus rate increases lateralisation in linguistic and
non-linguistic tasks measured by functional transcranial Doppler
sonography. Neuropsychologia, 72, 59--69.
<https://doi.org/10.1016/j.neuropsychologia.2015.04.019>
.1080/1357650X.2020.1859526.\
Phillips, N. (2017). yarrr: A Companion to the e-Book "YaRrr!: The
Pirate's Guide to R". R package version 0.1.5.
<https://CRAN.R-project.org/package=yarrr>.\
Porac, C., & Coren, S. (1976). The dominant eye. Psychological Bulletin,
83(5), 880--897. <https://doi.org/10.1037/0033-2909.83.5.880>\
Pornprasertmanit, S., Miller, P., Schoemann, A., & Jorgensen, T. D.
(2020). simsem: SIMulated Structural Equation Modeling. R package
version 0.5-15. <https://CRAN.R-project.org/package=simsem>.\
R Core Team. (2016). R: A language and environment for statistical
computing. Vienna, Austria: R Foundation for Statistical Computing.
Retrieved from <https://www.R-project.org/>\
Ramsey, N. F., Sommer, I. E. C., Rutten, G. J., & Kahn, R. S. (2001).
Combined analysis of language tasks in fMRI improves assessment of
hemispheric dominance for language functions in individual subjects.
Neuroimage, 13(4), 719-733. <doi:10.1006/nimg.2000.0722>\
Rasmussen, T., & Milner, B. (1975). Clinical and surgical studies of the
cerebral speech areas in man. In K. ZuÃàlch, O. Creutzfeldt, & G.
Galbraith (Eds.), Cerebral Localisation (pp. 238-257). New York:
Springer Verlag.\
Rasmussen, T., & Milner, B. (1977). The role of early left‚Äêbrain injury
in determining lateralization of cerebral speech functions. Annals of
the New York Academy of Sciences, 299(1), 355-369.
<doi:10.1111/j.1749-6632.1977.tb41921.x>\
Rosseel, Y. (2012). lavaan: An R Package for Structural Equation
Modeling. Journal of Statistical Software, 48, 1-36.\
Schermelleh-Engel, K., Moosbrugger, H., & M√ºller, H. (2003). Evaluating
the fit of Structural Equation Models: Tests of significance and
goodness-of-fit measures. Methods of Psychological Research, 8(2),
23--74.\
Seghier, M. L. (2008). Laterality index in functional MRI:
methodological issues. Magnetic Resonance Imaging, 26(5), 594-601;
<https://doi.org/10.1016/j.mri.2007.10.010>\
S√∏rensen, √ò., & Westerhausen, R. (2020). From observed laterality to
latent hemispheric differences: Revisiting the inference problem.
Laterality, 25(5), 560-582. <doi:10.1080/1357650X.2020.1769124>\
Taylor, J.E., Beith, A. & Sereno, S.C. LexOPS: An R package and user
interface for the controlled generation of word stimuli. Behavior
Research Methods (2020). <https://doi.org/10.3758/s13428-020-01389-1>\
Van Buuren, S., & Groothuis-Oudshoorn, K. (2011). mice: Multivariate
Imputation by Chained Equations in R. Journal of Statistical Software,
45(3), 1--67.\
Van der Haegen, L., & Brysbaert, M. (2018). The relationship between
behavioral language laterality, face laterality and language performance
in left-handers. PLoS One, 13(12), 1--22.
<https://doi.org/10.1371/journal.pone.0208696>\
Van der Haegen, L., Westerhausen, R., Hugdahl, K., & Brysbaert, M.
(2013). Speech dominance is a better predictor of functional brain
asymmetry than handedness: A combined fMRI word generation and
behavioral dichotic listening study. Neuropsychologia, 51(1), 91--97.
<https://doi.org/10.1016/j.neuropsychologia.2012.11.002>\
Van Heuven, W. J. B., Mandera, P., Keuleers, E., & Brysbaert, M. (2014).
SUBTLEX-UK: A new and improved word frequency database for British
English. Quarterly Journal of Experimental Psychology, 67, 1176-1190.
<doi:10.1080/17470218.2013.850521>\
Vigneau, M., Beaucousin, V., Herv√©, P.-Y., Jobard, G., Petit, L.,
Crivello, F., Mellet, E., Zago, L., Mazoyer, B., & Tzourio-Mazoyer, N.
(2011). What is right-hemisphere contribution to phonological,
lexico-semantic, and sentence processing? Insights from a meta-analysis.
Neuroimage, 54(1), 577--593.
<https://doi.org/10.1016/j.neuroimage.2010.07.036>\
Vingerhoets, G. (2019). Phenotypes in hemispheric functional
segregation? Perspectives and challenges. Physics of Life Reviews, 30,
1-18. <doi:10.1016/j.plrev.2019.06.002>\
Voyer, D. (1998). On the reliability and validity of noninvasive
laterality measures. Brain and Cognition, 36, 209-236.\
Wagenmakers, E. J., & Farrell, S. (2004). AIC model selection using
Akaike weights. Psychonomic Bulletin and Review, 11(1), 192-196.\
Wang, J.,& Wang, X. (2012). Structural Equation Modeling: Applications
Using Mplus. Chichester, West Sussex, U.K: Wiley/Higher Education
Press.\
Westerhausen, R. (2019). A primer on dichotic listening as a paradigm
for the assessment of hemispheric asymmetry. Laterality: Asymmetries of
Body, Brain and Cognition, 24(6), 740--771.
<https://doi.org/10.1080/1357650X.2019.1598426>\
Westerhausen, R., Kompus, K., & Hugdahl, K. (2014). Mapping hemispheric
symmetries, relative asymmetries, and absolute asymmetries underlying
the auditory laterality effect. NeuroImage, 84, 962--970.
<https://doi.org/10.1016/j.neuroimage.2013.09.074>\
Westerhausen, R., & Samuelsen, F. (2020). An optimal dichotic-listening
paradigm for the assessment of hemispheric dominance for speech
processing. PloS One, 15(6), e0234665.
<https://doi.org/10.1371/journal.pone.0234665>\
Wilke, M., & Lidzba, K. (2007). LI-tool: A new toolbox to assess
lateralization in functional MR-data. Journal of Neuroscience Methods,
163(1), 128-136. <doi:10.1016/j.jneumeth.2007.01.026>\
Wolf, E. J., Harrington, K. M., Clark, S. L., & Miller, M. W. (2013).
Sample size requirements for Structural Equation Models: An evaluation
of power, bias, and solution propriety. Educational and Psychological
Measurement, 73(6), 913-934.\
Woodhead, Z. V. J., Bradshaw, A. R., Wilson, A. C., Thompson, P. A., &
Bishop, D. V. M. (2019). Testing the unitary theory of language
lateralization using functional transcranial Doppler sonography in
adults. Royal Society Open Science, 6(3), 181801.
<https://doi.org/10.1098/rsos.181801>\
Woodhead, Z. V. J., Rutherford, H., & Bishop, D. V. M. (2018).
Measurement of language laterality using functional transcranial Doppler
ultrasound: a comparison of different tasks [version 2; peer review: 2
approved]. Wellcome Open Research, 3, 104.
<doi:https://doi.org/10.12688/wellcomeopenres.14720.2>\
Woodhead, Z. V. J., Thompson, P. A., Karlsson, E. M., & Bishop, D. V. M.
(2021). An updated investigation of the multidimensional structure of
language lateralization in left- and right-handed adults: A test--retest
functional transcranial Doppler sonography study with six language
tasks. Royal Society Open Science, 8(2), 200696.
<https://doi.org/10.1098/rsos.200696>

## Acknowledgements

This study is supported by an Advanced Grant from the European Research
Council [694189]. We thank Melissa Gibbs, Hussein Mehmet, Nahian Tasnim
Nur and Louis Pitsikas for assistance with recruitment and data
collection.

## Declaration of interest

The authors declare no competing interests.

## Author contributions

Adam J. Parker: Conceptualisation, Methodology, Validation, Formal
analysis, Investigation, Data Curation, Writing - Review and Editing,
Visualisation, Supervision; Zoe V. J. Woodhead: Conceptualisation,
Methodology, Validation, Formal analysis, Investigation, Data Curation,
Writing - Review and Editing, Visualisation, Supervision; David P.
Carey: Conceptualisation, Methodology, Writing - Review and Editing,
Supervision; Margriet A. Groen: Conceptualisation, Writing - Review and
Editing, Supervision; Eva Gutierrez-Sigut: Methodology, Writing - Review
and Editing; Jessica Hodgson: Methodology, Writing - Review and Editing,
Supervision; John Hudson: Supervision; Emma M. Karlsson:
Conceptualisation, Methodology, Writing - Review and Editing; Mair√©ad
MacSweeney: Conceptualisation, Methodology, Writing - Review and
Editing, Supervision; Heather Payne: Methodology, Writing - Review and
Editing; Nuala Simpson: Project Administration; Paul A. Thompson: Formal
analysis, Data Curation; Writing - Review and Editing; Kate E. Watkins:
Conceptualisation, Writing - Review and Editing, Ciara Egan:
Methodology, Investigation; Jack H. Grant: Methodology, Investigation;
Sophie Harte: Methodology, Investigation; Brad T. Hudson: Methodology,
Investigation; Maria Sablik: Investigation; Nicholas A. Badcock:
Investigation, Writing -- Review and Editing; Dorothy V. M. Bishop:
Conceptualisation, Methodology, Formal analysis, Writing - original
draft, Supervision, Funding Acquisition.

## Data availability

Raw data, analysis scripts and materials are all available on Open
Science Framework: <https://osf.io/g9tqh/>.

```{r sessioninfo, echo=F}
writeLines(capture.output(sessionInfo()), "COLA_RR_analysis_sessionInfo.txt")
save.image() #retain all variables from this session so can be used with supplement files
#saved info is in .Rdata
```

# Supplementary file `r suppcorr`

## Justification for ignoring pre-registered exclusionary criteria based on language tests.

In our pre-registration, we stated that we would include participants
who had English as a second language, provided they met proficiency
criteria on two language tests: LexTALE and Games with Words. As
explained above, given difficulties recruiting participants, we decided
to relax this criterion and consider retrospectively whether performance
on the two language screeners affected results. An additional
consideration was that we discovered that the inclusion criterion we had
adopted for Games with Words (no more than 3 errors) was overly
stringent, and based on a misinterpretation of Hartshorne et al. (2018).
See Bishop (2022) for more details.

Table S`r suppcorr`.1 below confirms there was no relationship between
language screen scores and laterality. See also Supplementary file 3 for
SEM analyses conducted using the full sample vs. the restricted sample
(excluding those who failed the language screen).

Table S`r suppcorr`.1 shows correlational analysis of Laterality Indices
and (a) measures of task performance; (b) Games with Words score; (c)
LexTALE scores. THe first table shows correlations with the raw
laterality indices, and the second table shows correlations with
absolute laterality indices (strength of laterality regardless of
direction). A total of 54 correlations is considered, giving
Bonferroni-corrected alpha level of .0009. None of the correlations
reached this level.

```{r errs-vs-LI,echo=F,warning=F}
LIs <- c('DL.zlat.ex','RDT.zlat.ex','WC.zlat.ex','A.LI.ex','B.LI.ex','C.LI.ex','D.LI.ex','E.LI.ex','F.LI.ex')
perf<-c('DLsame.corr','RDT.pcorr','WC.pcorr','WG_nWords','SG_nWords','PD_Acc','WC_Acc','SC_Acc','SD_Acc')
LInames<-c("Dichotic Listening","Rhyme Decision","Word Comprehension","Word Generation","Sentence Generation","Phonological Decision","Word Decision","Sentence Decision","Syntactic Decision")
perfnames<-c("% correct Same","% correct","% correct","N words","N words","% correct","% correct","% correct","% correct")
ntest<-length(LIs)
pairframe <- data.frame(matrix(NA,nrow=ntest,ncol=9))
colnames(pairframe)<-c('LI','Performance','N','rho: Performance','p-value','rho:Games with Words','p-value_','rho: Lextale','p-value__')
pairframe[,1]<-LInames
pairframe[,2]<-perfnames

for (r in 1:2){ #First round does absolute LI , 2nd round original LI
for (n in 1:ntest){
  xvalue<-combdat[,LIs[n]]
   if(r==1){xvalue<-abs(xvalue)}
  mycor <- cor.test(xvalue,combdat[,perf[n]],method='spearman')
  mycorx <-cor.test(xvalue,combdat[,perf[n]])#pearson just so we get df for N!
    mycorg <- cor.test(xvalue,combdat$gramerrs,method='spearman')
    mycorl <-cor.test(xvalue,combdat$lexTALE,method='spearman')
  pairframe[n,3]<-mycorx$parameter+2
  pairframe[n,4]<-round(mycor$estimate,2)
   pairframe[n,5]<-pformat2(mycor$p.value)
   pairframe[n,6]<- -round(mycorg$estimate,2)
   pairframe[n,7]<-pformat2(mycorg$p.value)
     pairframe[n,8]<- -round(mycorl$estimate,2)
   pairframe[n,9]<-pformat2(mycorl$p.value)
}
  if(r==1){pairframe.abs<-pairframe}
}
#flextable dislikes duplicate colnames for p-value



fpf1 <- flextable(pairframe) %>%
  set_caption("Correlations with raw LIs: Performance measures and language competence measures") %>% 
  flextable::compose(part = "header", j='rho: Performance', value = as_paragraph('r', as_sub('s'),': Performance')) %>% 
  flextable::compose(part = 'header', j='rho: Lextale', value = as_paragraph('r', as_sub('s'),': LexTALE')) %>% fontsize(size=10) %>% autofit

fpf1


fpf2 <- flextable(pairframe.abs) %>% 
  set_caption("Correlations (rho) with absolute LIs: Performance measures and language competence measures") %>% 
  flextable::compose(part = "header", j='rho: Performance', value = as_paragraph('r', as_sub('s'),': Performance')) %>% 
  flextable::compose(part = 'header', j='rho: Lextale', value = as_paragraph('r', as_sub('s'),': LexTALE')) %>% fontsize(size=10) %>% autofit  

fpf2
```

### Supplementary File `r suppdich`

Figure S`r suppdich`.1 shows the raw data from which laterality indices
were computed, for the three behavioural tasks, colour-coded by
handedness. The point type indicates whether the absolute individual
z-lat score was greater than 1.96, indicating reliable lateralisation
for that individual.

![Responses to stimuli on left and right: online behavioural laterality
tasks\_](figs/allLRbeh.eps)

Note that on the Dichotic Listening task there is inevitably a negative
correlation between these totals, because on each trial the response is
either left or right: hence, the more responses are 'left' the fewer are
'right', and vice versa. The black line shows the point of equality,
where the number correct is the same for left and right. For this task,
the points are distinguished for those where the proportion on one side
is significantly different from .5 (filled circles), versus those who
are equally likely to respond to L or R (crosses, which are bunched
around the black line). It is evident from this plot that a high
proportion of participants are significantly lateralised, and that
overall the sample shows a right ear advantage, i.e., there are more
points above the black line than below it.

In Rhyme Decision, the task was to judge which of two pictured items had
a name that rhymed with a centrally-presented written word. The pictures
were presented in the left or right periphery. This was an easy task, as
the words used were common, and the participants were given a practice
session where they were introduced to all the pictures and their names.
We could therefore use accuracy as an index of engagement with the task,
and we excluded `r length(which(combdat$excludeRDT==1))` participants
with accuracy of less than 75% correct.

The dependent variable of interest was Response Time (RT), which was
computed for correct responses in each half-field, after excluding
outliers using a participant-specific algorithm. This involved taking
the complete set of correct RTs for a participant, and applying the
Hoaglin-Iglewicz (1987) criterion of outlier detection with cutoff set
to 1.65 to remove unusually long RTs. In addition, any RTs less than 200
ms were considered anticipation errors and excluded. A LIz was then
computed by computing a t-test for each participant, to compare the mean
correct RT to left- and right-sided stimuli. Absolute values of LIz
greater than 1.96 can be regarded as evidence that an individual is
significantly faster to one side than the other. Because RT is scaled so
that high values correspond to poor performance, the LIz was computed so
that it was positive when left-sided RT was greater than right-sided RT.
Thus, those with a left-hemisphere advantage should cluster below the
black line.

As with Rhyme Decision, Word Comprehension involved responding to
laterally presented visual stimuli. The stimuli are pictures of
semantically-related words, and the task is simply to respond to the
picture whose name is spoken. Again, this is an easy task, where the
focus is on RT of correct responses. Participants who made more than 75%
errors on the task (N = `r length((which(combdat$excludeWC==1)))`) were
excluded from analysis. The same method as for Rhyme Decision was used
to remove outlier RTs participant by participant before computing a LIz
based on comparison of mean RT to left and right sides, where a positive
value indicated faster RTs to stimuli presented in the right visual
field.

Figure S`r suppdich`.2\
![Density plots and scatterplot showing relationship between
conventional laterality index and LIz for dichotic
listening.](figs/Afig4_DLdens.eps)
